{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9233c70e-f85a-4a2f-b71a-41176a3a46c9",
   "metadata": {},
   "source": [
    "# H5 Data Analysis\n",
    "\n",
    "Adapted from https://github.com/MLforHealth/MIMIC_Extract/blob/master/notebooks/Baselines%20for%20Mortality%20and%20LOS%20prediction%20-%20GRU-D.ipynb\n",
    "\n",
    "Create an environment: `/opt/miniconda3/bin/conda create --name=mimic-extract-2`\n",
    "\n",
    "Install packages:\n",
    "- Analysis: `conda install pandas pytables scipy numpy ipython ipykernel pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54daf2-d985-4401-beb3-3e746659a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0e606-8bc2-45bc-a93f-d83317b55a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math, os, pickle, time, pandas as pd, numpy as np, scipy.stats as ss, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf51d74-a56e-4483-90c1-dc8c902b083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH     = '/home/iandre3/mimic-extract-data/all_hourly_data.h5'\n",
    "GAP_TIME          = 2  # In hours\n",
    "WINDOW_SIZE       = 48 # In hours\n",
    "SEED              = 1\n",
    "ID_COLS           = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "GPU               = '2'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8638d10d-cf94-41c1-9d79-045ed1f74c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "    \n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377479b9-a5fc-482e-8330-71c64d89f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_full_lvl2 = pd.read_hdf(DATA_FILEPATH, 'vitals_labs')\n",
    "statics        = pd.read_hdf(DATA_FILEPATH, 'patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7a6bf-2759-4e01-b91a-c8f37445dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_lvl2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f6112-3e0f-48e6-82c4-2a9ab267bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "statics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699686f0-5dc2-4152-8f0a-c8701b7dba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "def simple_imputer(df):\n",
    "    idx = pd.IndexSlice\n",
    "    df = df.copy()\n",
    "    if len(df.columns.names) > 2: df.columns = df.columns.droplevel(('label', 'LEVEL1', 'LEVEL2'))\n",
    "    \n",
    "    df_out = df.loc[:, idx[:, ['mean', 'count']]]\n",
    "    icustay_means = df_out.loc[:, idx[:, 'mean']].groupby(ID_COLS).mean()\n",
    "    \n",
    "    df_out.loc[:,idx[:,'mean']] = df_out.loc[:,idx[:,'mean']].groupby(ID_COLS).fillna(\n",
    "        method='ffill'\n",
    "    ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n",
    "    \n",
    "    df_out.loc[:, idx[:, 'count']] = (df.loc[:, idx[:, 'count']] > 0).astype(float)\n",
    "    df_out.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
    "    \n",
    "    is_absent = (1 - df_out.loc[:, idx[:, 'mask']])\n",
    "    hours_of_absence = is_absent.cumsum()\n",
    "    time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
    "    time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "    df_out = pd.concat((df_out, time_since_measured), axis=1)\n",
    "    df_out.loc[:, idx[:, 'time_since_measured']] = df_out.loc[:, idx[:, 'time_since_measured']].fillna(100)\n",
    "    \n",
    "    df_out.sort_index(axis=1, inplace=True)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb0b9e-9307-49d7-a944-e48dbc272fa7",
   "metadata": {},
   "source": [
    "Data Loaded - Start cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607bfc12-3dc0-42e0-8824-c8955046f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Ys = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['mort_hosp', 'mort_icu', 'los_icu']]\n",
    "Ys['los_3'] = Ys['los_icu'] > 3\n",
    "Ys['los_7'] = Ys['los_icu'] > 7\n",
    "Ys.drop(columns=['los_icu'], inplace=True)\n",
    "Ys.astype(float)\n",
    "\n",
    "lvl2 = data_full_lvl2[\n",
    "    (data_full_lvl2.index.get_level_values('icustay_id').isin(set(Ys.index.get_level_values('icustay_id')))) &\n",
    "    (data_full_lvl2.index.get_level_values('hours_in') < WINDOW_SIZE)\n",
    "]\n",
    "\n",
    "train_frac, test_frac, validate_frac = 0.8, 0.1, 0.1\n",
    "lvl2_subj_idx, Ys_subj_idx = [df.index.get_level_values('subject_id') for df in (lvl2, Ys)]\n",
    "lvl2_subjects = set(lvl2_subj_idx)\n",
    "assert lvl2_subjects == set(Ys_subj_idx), \"Subject ID pools differ!\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "subjects, N = np.random.permutation(list(lvl2_subjects)), len(lvl2_subjects)\n",
    "N_train, N_test, N_validate = int(train_frac * N), int(test_frac * N), int(validate_frac * N)\n",
    "train_subj = subjects[:N_train]\n",
    "test_subj   = subjects[N_train:N_train + N_test]\n",
    "validate_subj  = subjects[N_train+N_test:]\n",
    "\n",
    "[(lvl2_train, lvl2_test, lvl2_validate), (Ys_train, Ys_test, Ys_validate)] = [\n",
    "    [df[df.index.get_level_values('subject_id').isin(s)] for s in (train_subj, test_subj, validate_subj)] \\\n",
    "    for df in (lvl2, Ys)\n",
    "]\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "lvl2_means, lvl2_stds = lvl2_train.loc[:, idx[:,'mean']].mean(axis=0), lvl2_train.loc[:, idx[:,'mean']].std(axis=0)\n",
    "\n",
    "lvl2_train.loc[:, idx[:,'mean']] = (lvl2_train.loc[:, idx[:,'mean']] - lvl2_means)/lvl2_stds\n",
    "lvl2_test.loc[:, idx[:,'mean']] = (lvl2_test.loc[:, idx[:,'mean']] - lvl2_means)/lvl2_stds\n",
    "lvl2_validate.loc[:, idx[:,'mean']] = (lvl2_validate.loc[:, idx[:,'mean']] - lvl2_means)/lvl2_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd872b-d541-4be2-b3cc-82626191023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lvl2_train, lvl2_test, lvl2_validate = [\n",
    "    simple_imputer(df) for df in (lvl2_train, lvl2_test, lvl2_validate)\n",
    "]\n",
    "\n",
    "for df in lvl2_train, lvl2_test, lvl2_validate: assert not df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25aa7d4-5657-41dc-b11d-e54611d536c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ys = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['mort_hosp', 'mort_icu']]\n",
    "Ys.astype(float)\n",
    "[(Ys_train, Ys_test, Ys_validate)] = [\n",
    "    [df[df.index.get_level_values('subject_id').isin(s)] for s in (train_subj, test_subj, validate_subj)] \\\n",
    "    for df in (Ys,)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49183c-bb0f-4934-8240-d553ec6c3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ys_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0dbc5-c06a-45bf-9d0a-f4c53ddcb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl2_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23094198-4a3b-4702-8d22-30728e3ec0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [(\"Ys_train\", Ys_train), (\"Ys_test\", Ys_test), (\"Ys_validate\", Ys_validate), (\"lvl2_train\", lvl2_train), (\"lvl2_test\", lvl2_test), (\"lvl2_validate\", lvl2_validate)]:\n",
    "    with open(f'data/{name[0]}.pkl', 'wb+') as f:\n",
    "        pickle.dump(name[1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f0c80-01c3-4ff9-9913-fe52008b6a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mimic-extract-2]",
   "language": "python",
   "name": "conda-env-.conda-mimic-extract-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
