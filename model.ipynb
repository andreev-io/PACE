{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bfb9fe-a383-473b-9d49-6f9561ab932e",
   "metadata": {},
   "source": [
    "# PACE Model Replication Pipeline\n",
    "\n",
    "**Required packages:**\n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `pytorch`\n",
    "- `imbalanced-learn`\n",
    "- `scikit-learn`\n",
    "- `matplotlib`\n",
    "\n",
    "**Optional packages:**\n",
    "- `ipython`\n",
    "- `ipykernel`\n",
    "\n",
    "**Required python version:** 3.9.x\n",
    "We cannot use a newer version of python until pytorch adds support for it: https://github.com/pytorch/pytorch/issues/66424\n",
    "\n",
    "**Installation commands:**\n",
    "\n",
    "- `/path/to/conda create --name=dl4h-39 python=3.9.12`\n",
    "- `conda activate dl4h-39`\n",
    "- `conda install pandas pytables scipy numpy ipython ipykernel pytorch scikit-learn matplotlib`\n",
    "- `conda install -c conda-forge imbalanced-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb27d3c-0696-46c9-a4b5-6dbcb92a50df",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0279f1fb-953e-4b4e-8553-2d55baebc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.5\n",
    "N = 16\n",
    "LAMBDA = 1.3\n",
    "LR = 0.001\n",
    "OMEGA = 0.001\n",
    "K = 1\n",
    "TOLERANCE = 0.0\n",
    "PATIENCE = 5\n",
    "\n",
    "USE_MODIFIED_LOSS_FUNCTION = True\n",
    "USE_SELF_PACED_LEARNING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004fd1a-a7be-4d86-8f2d-e891dcb035f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79433514-990a-42ba-8197-3836049dbc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea30443d-dedd-44c1-ae88-f97e32e5b868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c60dd9-709f-4fa4-a956-e6f66902aef6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e192f873-41d5-4c2c-8cc4-672880a7e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_info(features, masks, labels):\n",
    "    print(f\"Shape of features: {features.size()}\")\n",
    "    print(f\"Shape of masks: {masks.size()}\")\n",
    "    print(f\"Shape of labels: {labels.size()}\")\n",
    "    \n",
    "    print(f\"Labels (format: [(label, count)]: {list(zip(*torch.unique(labels, return_counts = True)))}\")\n",
    "\n",
    "    assert len(features) == len(labels)\n",
    "    assert len(masks) == len(labels)\n",
    "    assert features.size() == masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f80d9d4c-33ac-46f3-ae23-9dddc2e4c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset\n",
      "Shape of features: torch.Size([23062, 48, 104])\n",
      "Shape of masks: torch.Size([23062, 48, 104])\n",
      "Shape of labels: torch.Size([23062])\n",
      "Labels (format: [(label, count)]: [(tensor(0., device='cuda:0'), tensor(11531, device='cuda:0')), (tensor(1., device='cuda:0'), tensor(11531, device='cuda:0'))]\n",
      "\n",
      "Test dataset\n",
      "Shape of features: torch.Size([1560, 48, 104])\n",
      "Shape of masks: torch.Size([1560, 48, 104])\n",
      "Shape of labels: torch.Size([1560])\n",
      "Labels (format: [(label, count)]: [(tensor(0., device='cuda:0'), tensor(1431, device='cuda:0')), (tensor(1., device='cuda:0'), tensor(129, device='cuda:0'))]\n",
      "\n",
      "Validation dataset\n",
      "Shape of features: torch.Size([1404, 48, 104])\n",
      "Shape of masks: torch.Size([1404, 48, 104])\n",
      "Shape of labels: torch.Size([1404])\n",
      "Labels (format: [(label, count)]: [(tensor(0., device='cuda:0'), tensor(1279, device='cuda:0')), (tensor(1., device='cuda:0'), tensor(125, device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/train_features.pkl\", \"rb\") as f:\n",
    "    train_features = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/train_masks.pkl\", \"rb\") as f:\n",
    "    train_masks = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/train_labels.pkl\", \"rb\") as f:\n",
    "    train_labels = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/test_features.pkl\", \"rb\") as f:\n",
    "    test_features = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/test_masks.pkl\", \"rb\") as f:\n",
    "    test_masks = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/test_labels.pkl\", \"rb\") as f:\n",
    "    test_labels = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/val_features.pkl\", \"rb\") as f:\n",
    "    val_features = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/val_masks.pkl\", \"rb\") as f:\n",
    "    val_masks = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/val_labels.pkl\", \"rb\") as f:\n",
    "    val_labels = pickle.load(f).to(device)\n",
    "    \n",
    "print(\"Training dataset\")\n",
    "print_data_info(train_features, train_masks, train_labels)\n",
    "\n",
    "print(\"\\nTest dataset\")\n",
    "print_data_info(test_features, test_masks, test_labels)\n",
    "\n",
    "print(\"\\nValidation dataset\")\n",
    "print_data_info(val_features, val_masks, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d747a-a518-4f92-8dc9-f6063accd022",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363adfb-a6a1-4a16-a257-3ede3b6f3992",
   "metadata": {},
   "source": [
    "### Train Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b01308c8-e8cd-4789-8e96-ce67834fbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_easy_tasks(model, x, mask, y, N, K, epoch):\n",
    "    criterion = nn.BCELoss(reduction='none')\n",
    "    model.train(False)\n",
    "    y_hat = model(x, mask)\n",
    "\n",
    "    # Pick tasks in the batch for which the loss is less than 1 / N.\n",
    "    loss = torch.add((1 / GAMMA) * criterion(y_hat, y), -1 / N) if USE_MODIFIED_LOSS_FUNCTION else torch.add(criterion(y_hat, y), -1 / N)\n",
    "    easy_indices = list((loss < 0).nonzero()) if epoch >= K else list(loss.nonzero())\n",
    "\n",
    "    easy_timeseries = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_masks = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_labels = torch.empty((len(easy_indices),))\n",
    "\n",
    "    for i, index in enumerate(easy_indices):\n",
    "        index = index.item()\n",
    "        easy_timeseries[i] = x[index]\n",
    "        easy_masks[i] = mask[index]\n",
    "        easy_labels[i] = y[index]\n",
    "\n",
    "    print(f\"samples picked: {len(easy_indices)}, positive samples picked: {sum(easy_labels)}, K: {K}, epoch: {epoch}\")\n",
    "\n",
    "    model.train(True)\n",
    "    return torch.Tensor(easy_timeseries).to(device), torch.Tensor(easy_masks).to(device), torch.Tensor(easy_labels).to(device)\n",
    "\n",
    "def create_easy_train_dataloader(model, N, K, epoch):\n",
    "    easy_timeseries, easy_masks, easy_labels = pick_easy_tasks(model, train_features, train_masks, train_labels, N, K, epoch)\n",
    "    easy_train_dataset = TensorDataset(easy_timeseries, easy_masks, easy_labels)\n",
    "    if len(easy_train_dataset) == 0:\n",
    "        return None, len(train_features)\n",
    "    return DataLoader(easy_train_dataset, batch_size=32, shuffle=True), len(train_features)\n",
    "\n",
    "train_dataset = TensorDataset(train_features, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d68739-ab0f-458b-8a53-5a029540735f",
   "metadata": {},
   "source": [
    "### Test and Validation Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0292051f-8b86-4fb8-9b6c-fcb9726101ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(test_features, test_masks, test_labels)\n",
    "val_dataset = TensorDataset(val_features, val_masks, val_labels)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1a9c4-56e0-4da3-872d-08f85083d512",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06d52678-4129-434a-824a-b7c10d5837f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F1 score constant 1: 0.1635055591890124\n",
      "Baseline F1 score uniform: 0.14896214896214896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "constant_classifier = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "uniform_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "inputs, labels = None, None\n",
    "for inp, mask, lab in val_dataloader:\n",
    "    inputs, labels = inp, lab\n",
    "    labels = labels.clone().cpu().detach()\n",
    "\n",
    "constant_classifier.fit([0] * len(labels), labels)\n",
    "uniform_classifier.fit([0] * len(labels), labels)\n",
    "y_constant = constant_classifier.predict([0] * len(labels))\n",
    "y_uniform = uniform_classifier.predict([0] * len(labels))\n",
    "\n",
    "print(f\"Baseline F1 score constant 1: {f1_score(labels, y_constant)}\")\n",
    "print(f\"Baseline F1 score uniform: {f1_score(labels, y_uniform)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64cb3f-67e7-42e3-8a34-2a5fca799c70",
   "metadata": {},
   "source": [
    "## PACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833cdbf-e076-45ad-a602-204d36d7f594",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73f76a59-a975-4e9b-b5f7-fc1cfd85ee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyModel(\n",
      "  (rnn): GRU(104, 32, batch_first=True)\n",
      "  (fc1): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=104, hidden_size=32, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        try:\n",
    "            x = x * mask\n",
    "        except:\n",
    "            print(x, mask)\n",
    "        _, h_n = self.rnn(x)\n",
    "        fc1_out = self.fc1(torch.squeeze(h_n))\n",
    "        r = fc1_out\n",
    "        \n",
    "        # Multiply prediction value before sigmoid activation by gamma.\n",
    "        # This is part of the micro framework.\n",
    "        if USE_MODIFIED_LOSS_FUNCTION:\n",
    "            r = torch.mul(fc1_out, GAMMA)\n",
    "            \n",
    "        res = torch.sigmoid(r)\n",
    "        return torch.squeeze(res)\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "tinymodel.to(device)\n",
    "\n",
    "print(tinymodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162135f6-9d68-47a9-a6d7-6600ed8b0565",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8df6ef20-818a-480b-8b02-e4fe68516dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(tinymodel.parameters(), LR)\n",
    "\n",
    "def loss_function(y_hat, y, model):\n",
    "    # Add a dimension when there is only one item in the batch.     \n",
    "    if len(y_hat.size()) == 0:\n",
    "        y_hat = torch.unsqueeze(y_hat, dim=0)\n",
    "    \n",
    "    l1 = OMEGA * sum(p.abs().sum() for p in model.parameters())\n",
    "    cr = criterion(y_hat, y)\n",
    "    \n",
    "    return l1 + cr\n",
    "\n",
    "def modified_loss_function(y_hat, y, model):\n",
    "    # Add a dimension when there is only one item in the batch.     \n",
    "    if len(y_hat.size()) == 0:\n",
    "        y_hat = torch.unsqueeze(y_hat, dim=0)\n",
    "\n",
    "    l1 = OMEGA * sum(p.abs().sum() for p in model.parameters())\n",
    "    cr = (1 / GAMMA) * criterion(y_hat, y)\n",
    "    \n",
    "    # Calculate the loss. The regularization is just an l1 regularization with a coefficient.\n",
    "    # The criterion is the binary cross-entropy multiplied by 1 / GAMMA, a part of the micro framework.\n",
    "    return l1 + cr\n",
    "\n",
    "    # The paper also says to subtract batch_size / N, but this value is different for different batches depending\n",
    "    # on the number of easy tasks in the batch, making it hard to compare loss across batches. Crucially, this \n",
    "    # is confusing when you look at the loss on training data (non-full batches) vs loss on testing data (full-size batches).\n",
    "    # This makes me think that actually the paper meant that you first pick easy tasks across the entire testing set,\n",
    "    # and then split it into batches rather than split into batches first and pick easy tasks after.\n",
    "    # batch_size = y_hat.size()[0]    \n",
    "    # neg = batch_size / N\n",
    "    # return l1 + cr - neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4c85b-d054-4f6d-adaf-bb72e6593e26",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a819e10-4fc0-4c74-8193-4d37277affcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N is set to 16\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 0\n",
      "Epoch: [1, training batch   100] running training loss: 2.028\n",
      "Epoch: [1, training batch   200] running training loss: 1.343\n",
      "Epoch: [1, training batch   300] running training loss: 1.124\n",
      "Epoch: [1, training batch   400] running training loss: 1.062\n",
      "Epoch: [1, training batch   500] running training loss: 1.087\n",
      "Epoch: [1, training batch   600] running training loss: 1.041\n",
      "Epoch: [1, training batch   700] running training loss: 1.041\n",
      "In epoch 1, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 1.034\n",
      "\n",
      "\n",
      "\n",
      "N is set to 16\n",
      "samples picked: 60, positive samples picked: 60.0, K: 1, epoch: 1\n",
      "In epoch 2, number of examples trained on is 60 out of 23062\n",
      "Validation loss: 0.162\n",
      "\n",
      "\n",
      "\n",
      "N is set to 12.307692307692307\n",
      "samples picked: 395, positive samples picked: 395.0, K: 1, epoch: 2\n",
      "In epoch 3, number of examples trained on is 395 out of 23062\n",
      "Validation loss: 0.174\n",
      "\n",
      "\n",
      "\n",
      "N is set to 9.467455621301774\n",
      "samples picked: 1198, positive samples picked: 1193.0, K: 1, epoch: 3\n",
      "In epoch 4, number of examples trained on is 1198 out of 23062\n",
      "Validation loss: 0.159\n",
      "\n",
      "\n",
      "\n",
      "N is set to 7.282658170232134\n",
      "samples picked: 4843, positive samples picked: 4644.0, K: 1, epoch: 4\n",
      "Epoch: [5, training batch   100] running training loss: 0.157\n",
      "In epoch 5, number of examples trained on is 4843 out of 23062\n",
      "Validation loss: 0.134\n",
      "\n",
      "\n",
      "\n",
      "N is set to 5.60204474633241\n",
      "samples picked: 13927, positive samples picked: 9629.0, K: 1, epoch: 5\n",
      "Epoch: [6, training batch   100] running training loss: 0.214\n",
      "Epoch: [6, training batch   200] running training loss: 0.213\n",
      "Epoch: [6, training batch   300] running training loss: 0.165\n",
      "Epoch: [6, training batch   400] running training loss: 0.165\n",
      "In epoch 6, number of examples trained on is 13927 out of 23062\n",
      "Validation loss: 0.125\n",
      "\n",
      "\n",
      "\n",
      "N is set to 4.309265189486469\n",
      "samples picked: 15858, positive samples picked: 9818.0, K: 1, epoch: 6\n",
      "Epoch: [7, training batch   100] running training loss: 0.198\n",
      "Epoch: [7, training batch   200] running training loss: 0.206\n",
      "Epoch: [7, training batch   300] running training loss: 0.178\n",
      "Epoch: [7, training batch   400] running training loss: 0.187\n",
      "In epoch 7, number of examples trained on is 15858 out of 23062\n",
      "Validation loss: 0.136\n",
      "\n",
      "\n",
      "\n",
      "N is set to 3.314819376528053\n",
      "samples picked: 16322, positive samples picked: 9908.0, K: 1, epoch: 7\n",
      "Epoch: [8, training batch   100] running training loss: 0.192\n",
      "Epoch: [8, training batch   200] running training loss: 0.173\n",
      "Epoch: [8, training batch   300] running training loss: 0.162\n",
      "Epoch: [8, training batch   400] running training loss: 0.165\n",
      "Epoch: [8, training batch   500] running training loss: 0.180\n",
      "In epoch 8, number of examples trained on is 16322 out of 23062\n",
      "Validation loss: 0.127\n",
      "\n",
      "\n",
      "\n",
      "N is set to 2.5498610588677333\n",
      "samples picked: 16573, positive samples picked: 9773.0, K: 1, epoch: 8\n",
      "Epoch: [9, training batch   100] running training loss: 0.140\n",
      "Epoch: [9, training batch   200] running training loss: 0.131\n",
      "Epoch: [9, training batch   300] running training loss: 0.127\n",
      "Epoch: [9, training batch   400] running training loss: 0.122\n",
      "Epoch: [9, training batch   500] running training loss: 0.123\n",
      "In epoch 9, number of examples trained on is 16573 out of 23062\n",
      "Validation loss: 0.118\n",
      "\n",
      "\n",
      "\n",
      "N is set to 1.96143158374441\n",
      "samples picked: 16938, positive samples picked: 9811.0, K: 1, epoch: 9\n",
      "Epoch: [10, training batch   100] running training loss: 0.157\n",
      "Epoch: [10, training batch   200] running training loss: 0.155\n",
      "Epoch: [10, training batch   300] running training loss: 0.138\n",
      "Epoch: [10, training batch   400] running training loss: 0.143\n",
      "Epoch: [10, training batch   500] running training loss: 0.150\n",
      "In epoch 10, number of examples trained on is 16938 out of 23062\n",
      "Validation loss: 0.187\n",
      "\n",
      "\n",
      "\n",
      "N is set to 1.5087935259572385\n",
      "samples picked: 17165, positive samples picked: 9839.0, K: 1, epoch: 10\n",
      "Epoch: [11, training batch   100] running training loss: 0.156\n",
      "Epoch: [11, training batch   200] running training loss: 0.197\n",
      "Epoch: [11, training batch   300] running training loss: 0.153\n",
      "Epoch: [11, training batch   400] running training loss: 0.149\n",
      "Epoch: [11, training batch   500] running training loss: 0.165\n",
      "In epoch 11, number of examples trained on is 17165 out of 23062\n",
      "Validation loss: 0.121\n",
      "\n",
      "\n",
      "\n",
      "N is set to 1.1606104045824912\n",
      "samples picked: 17325, positive samples picked: 9774.0, K: 1, epoch: 11\n",
      "Epoch: [12, training batch   100] running training loss: 0.183\n",
      "Epoch: [12, training batch   200] running training loss: 0.173\n",
      "Epoch: [12, training batch   300] running training loss: 0.185\n",
      "Epoch: [12, training batch   400] running training loss: 0.197\n",
      "Epoch: [12, training batch   500] running training loss: 0.174\n",
      "In epoch 12, number of examples trained on is 17325 out of 23062\n",
      "Validation loss: 0.133\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.892777234294224\n",
      "samples picked: 17657, positive samples picked: 9720.0, K: 1, epoch: 12\n",
      "Epoch: [13, training batch   100] running training loss: 0.191\n",
      "Epoch: [13, training batch   200] running training loss: 0.156\n",
      "Epoch: [13, training batch   300] running training loss: 0.152\n",
      "Epoch: [13, training batch   400] running training loss: 0.154\n",
      "Epoch: [13, training batch   500] running training loss: 0.170\n",
      "In epoch 13, number of examples trained on is 17657 out of 23062\n",
      "Validation loss: 0.130\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.6867517186878646\n",
      "samples picked: 17776, positive samples picked: 9727.0, K: 1, epoch: 13\n",
      "Epoch: [14, training batch   100] running training loss: 0.163\n",
      "Epoch: [14, training batch   200] running training loss: 0.181\n",
      "Epoch: [14, training batch   300] running training loss: 0.153\n",
      "Epoch: [14, training batch   400] running training loss: 0.166\n",
      "Epoch: [14, training batch   500] running training loss: 0.156\n",
      "In epoch 14, number of examples trained on is 17776 out of 23062\n",
      "Validation loss: 0.408\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.5282705528368189\n",
      "samples picked: 17950, positive samples picked: 9702.0, K: 1, epoch: 14\n",
      "Epoch: [15, training batch   100] running training loss: 0.181\n",
      "Epoch: [15, training batch   200] running training loss: 0.180\n",
      "Epoch: [15, training batch   300] running training loss: 0.171\n",
      "Epoch: [15, training batch   400] running training loss: 0.164\n",
      "Epoch: [15, training batch   500] running training loss: 0.165\n",
      "In epoch 15, number of examples trained on is 17950 out of 23062\n",
      "Validation loss: 0.144\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.4063619637206299\n",
      "samples picked: 18222, positive samples picked: 9748.0, K: 1, epoch: 15\n",
      "Epoch: [16, training batch   100] running training loss: 0.175\n",
      "Epoch: [16, training batch   200] running training loss: 0.186\n",
      "Epoch: [16, training batch   300] running training loss: 0.184\n",
      "Epoch: [16, training batch   400] running training loss: 0.191\n",
      "Epoch: [16, training batch   500] running training loss: 0.207\n",
      "In epoch 16, number of examples trained on is 18222 out of 23062\n",
      "Validation loss: 0.121\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.312586125938946\n",
      "samples picked: 18524, positive samples picked: 9534.0, K: 1, epoch: 16\n",
      "Epoch: [17, training batch   100] running training loss: 0.235\n",
      "Epoch: [17, training batch   200] running training loss: 0.261\n",
      "Epoch: [17, training batch   300] running training loss: 0.241\n",
      "Epoch: [17, training batch   400] running training loss: 0.238\n",
      "Epoch: [17, training batch   500] running training loss: 0.215\n",
      "In epoch 17, number of examples trained on is 18524 out of 23062\n",
      "Validation loss: 0.165\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.24045086610688154\n",
      "samples picked: 18923, positive samples picked: 9664.0, K: 1, epoch: 17\n",
      "Epoch: [18, training batch   100] running training loss: 0.292\n",
      "Epoch: [18, training batch   200] running training loss: 0.242\n",
      "Epoch: [18, training batch   300] running training loss: 0.264\n",
      "Epoch: [18, training batch   400] running training loss: 0.261\n",
      "Epoch: [18, training batch   500] running training loss: 0.289\n",
      "In epoch 18, number of examples trained on is 18923 out of 23062\n",
      "Validation loss: 0.464\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.1849622046976012\n",
      "samples picked: 19275, positive samples picked: 9689.0, K: 1, epoch: 18\n",
      "Epoch: [19, training batch   100] running training loss: 0.340\n",
      "Epoch: [19, training batch   200] running training loss: 0.313\n",
      "Epoch: [19, training batch   300] running training loss: 0.300\n",
      "Epoch: [19, training batch   400] running training loss: 0.323\n",
      "Epoch: [19, training batch   500] running training loss: 0.308\n",
      "Epoch: [19, training batch   600] running training loss: 0.295\n",
      "In epoch 19, number of examples trained on is 19275 out of 23062\n",
      "Validation loss: 0.661\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.14227861899815475\n",
      "samples picked: 19859, positive samples picked: 9894.0, K: 1, epoch: 19\n",
      "Epoch: [20, training batch   100] running training loss: 0.462\n",
      "Epoch: [20, training batch   200] running training loss: 0.432\n",
      "Epoch: [20, training batch   300] running training loss: 0.428\n",
      "Epoch: [20, training batch   400] running training loss: 0.393\n",
      "Epoch: [20, training batch   500] running training loss: 0.408\n",
      "Epoch: [20, training batch   600] running training loss: 0.397\n",
      "In epoch 20, number of examples trained on is 19859 out of 23062\n",
      "Validation loss: 0.189\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.10944509153704211\n",
      "samples picked: 21745, positive samples picked: 10591.0, K: 1, epoch: 20\n",
      "Epoch: [21, training batch   100] running training loss: 0.824\n",
      "Epoch: [21, training batch   200] running training loss: 0.762\n",
      "Epoch: [21, training batch   300] running training loss: 0.756\n",
      "Epoch: [21, training batch   400] running training loss: 0.772\n",
      "Epoch: [21, training batch   500] running training loss: 0.740\n",
      "Epoch: [21, training batch   600] running training loss: 0.723\n",
      "In epoch 21, number of examples trained on is 21745 out of 23062\n",
      "Validation loss: 0.451\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.08418853195157085\n",
      "samples picked: 23044, positive samples picked: 11514.0, K: 1, epoch: 21\n",
      "Epoch: [22, training batch   100] running training loss: 1.025\n",
      "Epoch: [22, training batch   200] running training loss: 0.943\n",
      "Epoch: [22, training batch   300] running training loss: 0.956\n",
      "Epoch: [22, training batch   400] running training loss: 0.970\n",
      "Epoch: [22, training batch   500] running training loss: 1.013\n",
      "Epoch: [22, training batch   600] running training loss: 0.947\n",
      "Epoch: [22, training batch   700] running training loss: 0.948\n",
      "In epoch 22, number of examples trained on is 23044 out of 23062\n",
      "Validation loss: 0.650\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.06476040919351603\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 22\n",
      "Epoch: [23, training batch   100] running training loss: 0.986\n",
      "Epoch: [23, training batch   200] running training loss: 0.934\n",
      "Epoch: [23, training batch   300] running training loss: 0.910\n",
      "Epoch: [23, training batch   400] running training loss: 0.964\n",
      "Epoch: [23, training batch   500] running training loss: 0.944\n",
      "Epoch: [23, training batch   600] running training loss: 0.965\n",
      "Epoch: [23, training batch   700] running training loss: 0.930\n",
      "In epoch 23, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.748\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.04981569937962771\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 23\n",
      "Epoch: [24, training batch   100] running training loss: 0.928\n",
      "Epoch: [24, training batch   200] running training loss: 0.949\n",
      "Epoch: [24, training batch   300] running training loss: 0.886\n",
      "Epoch: [24, training batch   400] running training loss: 0.918\n",
      "Epoch: [24, training batch   500] running training loss: 0.921\n",
      "Epoch: [24, training batch   600] running training loss: 0.938\n",
      "Epoch: [24, training batch   700] running training loss: 0.917\n",
      "In epoch 24, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.806\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.038319768753559774\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 24\n",
      "Epoch: [25, training batch   100] running training loss: 0.928\n",
      "Epoch: [25, training batch   200] running training loss: 0.900\n",
      "Epoch: [25, training batch   300] running training loss: 0.915\n",
      "Epoch: [25, training batch   400] running training loss: 0.914\n",
      "Epoch: [25, training batch   500] running training loss: 0.884\n",
      "Epoch: [25, training batch   600] running training loss: 0.887\n",
      "Epoch: [25, training batch   700] running training loss: 0.907\n",
      "In epoch 25, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.700\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.029476745195045978\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 25\n",
      "Epoch: [26, training batch   100] running training loss: 0.875\n",
      "Epoch: [26, training batch   200] running training loss: 0.891\n",
      "Epoch: [26, training batch   300] running training loss: 0.905\n",
      "Epoch: [26, training batch   400] running training loss: 0.893\n",
      "Epoch: [26, training batch   500] running training loss: 0.892\n",
      "Epoch: [26, training batch   600] running training loss: 0.898\n",
      "Epoch: [26, training batch   700] running training loss: 0.895\n",
      "In epoch 26, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.760\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.022674419380804597\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 26\n",
      "Epoch: [27, training batch   100] running training loss: 0.898\n",
      "Epoch: [27, training batch   200] running training loss: 0.896\n",
      "Epoch: [27, training batch   300] running training loss: 0.852\n",
      "Epoch: [27, training batch   400] running training loss: 0.859\n",
      "Epoch: [27, training batch   500] running training loss: 0.866\n",
      "Epoch: [27, training batch   600] running training loss: 0.886\n",
      "Epoch: [27, training batch   700] running training loss: 0.894\n",
      "In epoch 27, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 1.054\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.017441861062157383\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 27\n",
      "Epoch: [28, training batch   100] running training loss: 0.870\n",
      "Epoch: [28, training batch   200] running training loss: 0.854\n",
      "Epoch: [28, training batch   300] running training loss: 0.829\n",
      "Epoch: [28, training batch   400] running training loss: 0.856\n",
      "Epoch: [28, training batch   500] running training loss: 0.891\n",
      "Epoch: [28, training batch   600] running training loss: 0.891\n",
      "Epoch: [28, training batch   700] running training loss: 0.885\n",
      "In epoch 28, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.870\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.013416816201659526\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 28\n",
      "Epoch: [29, training batch   100] running training loss: 0.849\n",
      "Epoch: [29, training batch   200] running training loss: 0.889\n",
      "Epoch: [29, training batch   300] running training loss: 0.829\n",
      "Epoch: [29, training batch   400] running training loss: 0.881\n",
      "Epoch: [29, training batch   500] running training loss: 0.866\n",
      "Epoch: [29, training batch   600] running training loss: 0.887\n",
      "Epoch: [29, training batch   700] running training loss: 0.863\n",
      "In epoch 29, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 1.093\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.010320627847430404\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 29\n",
      "Epoch: [30, training batch   100] running training loss: 0.865\n",
      "Epoch: [30, training batch   200] running training loss: 0.888\n",
      "Epoch: [30, training batch   300] running training loss: 0.846\n",
      "Epoch: [30, training batch   400] running training loss: 0.859\n",
      "Epoch: [30, training batch   500] running training loss: 0.828\n",
      "Epoch: [30, training batch   600] running training loss: 0.841\n",
      "Epoch: [30, training batch   700] running training loss: 0.861\n",
      "In epoch 30, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.868\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.007938944498023388\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 30\n",
      "Epoch: [31, training batch   100] running training loss: 0.859\n",
      "Epoch: [31, training batch   200] running training loss: 0.862\n",
      "Epoch: [31, training batch   300] running training loss: 0.845\n",
      "Epoch: [31, training batch   400] running training loss: 0.820\n",
      "Epoch: [31, training batch   500] running training loss: 0.859\n",
      "Epoch: [31, training batch   600] running training loss: 0.871\n",
      "Epoch: [31, training batch   700] running training loss: 0.831\n",
      "In epoch 31, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 1.006\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.006106880383094914\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 31\n",
      "Epoch: [32, training batch   100] running training loss: 0.840\n",
      "Epoch: [32, training batch   200] running training loss: 0.830\n",
      "Epoch: [32, training batch   300] running training loss: 0.787\n",
      "Epoch: [32, training batch   400] running training loss: 0.853\n",
      "Epoch: [32, training batch   500] running training loss: 0.870\n",
      "Epoch: [32, training batch   600] running training loss: 0.848\n",
      "Epoch: [32, training batch   700] running training loss: 0.864\n",
      "In epoch 32, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.683\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.004697600294688395\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 32\n",
      "Epoch: [33, training batch   100] running training loss: 0.841\n",
      "Epoch: [33, training batch   200] running training loss: 0.804\n",
      "Epoch: [33, training batch   300] running training loss: 0.826\n",
      "Epoch: [33, training batch   400] running training loss: 0.841\n",
      "Epoch: [33, training batch   500] running training loss: 0.817\n",
      "Epoch: [33, training batch   600] running training loss: 0.835\n",
      "Epoch: [33, training batch   700] running training loss: 0.849\n",
      "In epoch 33, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.791\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.0036135386882218423\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 33\n",
      "Epoch: [34, training batch   100] running training loss: 0.829\n",
      "Epoch: [34, training batch   200] running training loss: 0.802\n",
      "Epoch: [34, training batch   300] running training loss: 0.837\n",
      "Epoch: [34, training batch   400] running training loss: 0.806\n",
      "Epoch: [34, training batch   500] running training loss: 0.847\n",
      "Epoch: [34, training batch   600] running training loss: 0.835\n",
      "Epoch: [34, training batch   700] running training loss: 0.799\n",
      "In epoch 34, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.849\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.0027796451447860324\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 34\n",
      "Epoch: [35, training batch   100] running training loss: 0.792\n",
      "Epoch: [35, training batch   200] running training loss: 0.814\n",
      "Epoch: [35, training batch   300] running training loss: 0.818\n",
      "Epoch: [35, training batch   400] running training loss: 0.818\n",
      "Epoch: [35, training batch   500] running training loss: 0.848\n",
      "Epoch: [35, training batch   600] running training loss: 0.828\n",
      "Epoch: [35, training batch   700] running training loss: 0.791\n",
      "In epoch 35, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.799\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.0021381885729123327\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 35\n",
      "Epoch: [36, training batch   100] running training loss: 0.784\n",
      "Epoch: [36, training batch   200] running training loss: 0.818\n",
      "Epoch: [36, training batch   300] running training loss: 0.793\n",
      "Epoch: [36, training batch   400] running training loss: 0.815\n",
      "Epoch: [36, training batch   500] running training loss: 0.816\n",
      "Epoch: [36, training batch   600] running training loss: 0.829\n",
      "Epoch: [36, training batch   700] running training loss: 0.793\n",
      "In epoch 36, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 1.580\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.0016447604407017944\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 36\n",
      "Epoch: [37, training batch   100] running training loss: 0.826\n",
      "Epoch: [37, training batch   200] running training loss: 0.824\n",
      "Epoch: [37, training batch   300] running training loss: 0.799\n",
      "Epoch: [37, training batch   400] running training loss: 0.820\n",
      "Epoch: [37, training batch   500] running training loss: 0.802\n",
      "Epoch: [37, training batch   600] running training loss: 0.756\n",
      "Epoch: [37, training batch   700] running training loss: 0.775\n",
      "In epoch 37, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.443\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.0012652003390013803\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 37\n",
      "Epoch: [38, training batch   100] running training loss: 0.789\n",
      "Epoch: [38, training batch   200] running training loss: 0.822\n",
      "Epoch: [38, training batch   300] running training loss: 0.771\n",
      "Epoch: [38, training batch   400] running training loss: 0.780\n",
      "Epoch: [38, training batch   500] running training loss: 0.802\n",
      "Epoch: [38, training batch   600] running training loss: 0.794\n",
      "Epoch: [38, training batch   700] running training loss: 0.797\n",
      "In epoch 38, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.774\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.0009732310300010617\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 38\n",
      "Epoch: [39, training batch   100] running training loss: 0.771\n",
      "Epoch: [39, training batch   200] running training loss: 0.814\n",
      "Epoch: [39, training batch   300] running training loss: 0.789\n",
      "Epoch: [39, training batch   400] running training loss: 0.749\n",
      "Epoch: [39, training batch   500] running training loss: 0.763\n",
      "Epoch: [39, training batch   600] running training loss: 0.825\n",
      "Epoch: [39, training batch   700] running training loss: 0.809\n",
      "In epoch 39, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.626\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.0007486392538469705\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 39\n",
      "Epoch: [40, training batch   100] running training loss: 0.781\n",
      "Epoch: [40, training batch   200] running training loss: 0.777\n",
      "Epoch: [40, training batch   300] running training loss: 0.802\n",
      "Epoch: [40, training batch   400] running training loss: 0.795\n",
      "Epoch: [40, training batch   500] running training loss: 0.773\n",
      "Epoch: [40, training batch   600] running training loss: 0.776\n",
      "Epoch: [40, training batch   700] running training loss: 0.778\n",
      "In epoch 40, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.711\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.0005758763491130542\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 40\n",
      "Epoch: [41, training batch   100] running training loss: 0.777\n",
      "Epoch: [41, training batch   200] running training loss: 0.779\n",
      "Epoch: [41, training batch   300] running training loss: 0.781\n",
      "Epoch: [41, training batch   400] running training loss: 0.768\n",
      "Epoch: [41, training batch   500] running training loss: 0.781\n",
      "Epoch: [41, training batch   600] running training loss: 0.804\n",
      "Epoch: [41, training batch   700] running training loss: 0.749\n",
      "In epoch 41, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.389\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.0004429818070100417\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 41\n",
      "Epoch: [42, training batch   100] running training loss: 0.779\n",
      "Epoch: [42, training batch   200] running training loss: 0.772\n",
      "Epoch: [42, training batch   300] running training loss: 0.766\n",
      "Epoch: [42, training batch   400] running training loss: 0.780\n",
      "Epoch: [42, training batch   500] running training loss: 0.779\n",
      "Epoch: [42, training batch   600] running training loss: 0.765\n",
      "Epoch: [42, training batch   700] running training loss: 0.755\n",
      "In epoch 42, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.967\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.00034075523616157054\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 42\n",
      "Epoch: [43, training batch   100] running training loss: 0.732\n",
      "Epoch: [43, training batch   200] running training loss: 0.777\n",
      "Epoch: [43, training batch   300] running training loss: 0.804\n",
      "Epoch: [43, training batch   400] running training loss: 0.770\n",
      "Epoch: [43, training batch   500] running training loss: 0.774\n",
      "Epoch: [43, training batch   600] running training loss: 0.763\n",
      "Epoch: [43, training batch   700] running training loss: 0.756\n",
      "In epoch 43, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.559\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.00026211941243197735\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 43\n",
      "Epoch: [44, training batch   100] running training loss: 0.766\n",
      "Epoch: [44, training batch   200] running training loss: 0.754\n",
      "Epoch: [44, training batch   300] running training loss: 0.753\n",
      "Epoch: [44, training batch   400] running training loss: 0.758\n",
      "Epoch: [44, training batch   500] running training loss: 0.768\n",
      "Epoch: [44, training batch   600] running training loss: 0.764\n",
      "Epoch: [44, training batch   700] running training loss: 0.804\n",
      "In epoch 44, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.460\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.0002016303172553672\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 44\n",
      "Epoch: [45, training batch   100] running training loss: 0.778\n",
      "Epoch: [45, training batch   200] running training loss: 0.766\n",
      "Epoch: [45, training batch   300] running training loss: 0.747\n",
      "Epoch: [45, training batch   400] running training loss: 0.747\n",
      "Epoch: [45, training batch   500] running training loss: 0.751\n",
      "Epoch: [45, training batch   600] running training loss: 0.794\n",
      "Epoch: [45, training batch   700] running training loss: 0.766\n",
      "In epoch 45, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.955\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.00015510024404259015\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 45\n",
      "Epoch: [46, training batch   100] running training loss: 0.772\n",
      "Epoch: [46, training batch   200] running training loss: 0.772\n",
      "Epoch: [46, training batch   300] running training loss: 0.732\n",
      "Epoch: [46, training batch   400] running training loss: 0.755\n",
      "Epoch: [46, training batch   500] running training loss: 0.750\n",
      "Epoch: [46, training batch   600] running training loss: 0.748\n",
      "Epoch: [46, training batch   700] running training loss: 0.746\n",
      "In epoch 46, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.964\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 0.00011930788003276164\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 46\n",
      "Epoch: [47, training batch   100] running training loss: 0.728\n",
      "Epoch: [47, training batch   200] running training loss: 0.727\n",
      "Epoch: [47, training batch   300] running training loss: 0.761\n",
      "Epoch: [47, training batch   400] running training loss: 0.753\n",
      "Epoch: [47, training batch   500] running training loss: 0.761\n",
      "Epoch: [47, training batch   600] running training loss: 0.805\n",
      "Epoch: [47, training batch   700] running training loss: 0.728\n",
      "In epoch 47, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 1.597\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 9.177529233289356e-05\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 47\n",
      "Epoch: [48, training batch   100] running training loss: 0.767\n",
      "Epoch: [48, training batch   200] running training loss: 0.755\n",
      "Epoch: [48, training batch   300] running training loss: 0.726\n",
      "Epoch: [48, training batch   400] running training loss: 0.721\n",
      "Epoch: [48, training batch   500] running training loss: 0.762\n",
      "Epoch: [48, training batch   600] running training loss: 0.797\n",
      "Epoch: [48, training batch   700] running training loss: 0.740\n",
      "In epoch 48, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.828\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 7.059637871761043e-05\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 48\n",
      "Epoch: [49, training batch   100] running training loss: 0.747\n",
      "Epoch: [49, training batch   200] running training loss: 0.763\n",
      "Epoch: [49, training batch   300] running training loss: 0.744\n",
      "Epoch: [49, training batch   400] running training loss: 0.739\n",
      "Epoch: [49, training batch   500] running training loss: 0.736\n",
      "Epoch: [49, training batch   600] running training loss: 0.722\n",
      "Epoch: [49, training batch   700] running training loss: 0.764\n",
      "In epoch 49, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.761\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 5.430490670585417e-05\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 49\n",
      "Epoch: [50, training batch   100] running training loss: 0.742\n",
      "Epoch: [50, training batch   200] running training loss: 0.733\n",
      "Epoch: [50, training batch   300] running training loss: 0.767\n",
      "Epoch: [50, training batch   400] running training loss: 0.710\n",
      "Epoch: [50, training batch   500] running training loss: 0.746\n",
      "Epoch: [50, training batch   600] running training loss: 0.740\n",
      "Epoch: [50, training batch   700] running training loss: 0.718\n",
      "In epoch 50, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.657\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 4.177300515834936e-05\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 50\n",
      "Epoch: [51, training batch   100] running training loss: 0.745\n",
      "Epoch: [51, training batch   200] running training loss: 0.729\n",
      "Epoch: [51, training batch   300] running training loss: 0.757\n",
      "Epoch: [51, training batch   400] running training loss: 0.733\n",
      "Epoch: [51, training batch   500] running training loss: 0.752\n",
      "Epoch: [51, training batch   600] running training loss: 0.746\n",
      "Epoch: [51, training batch   700] running training loss: 0.755\n",
      "In epoch 51, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.500\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 3.213308089103797e-05\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 51\n",
      "Epoch: [52, training batch   100] running training loss: 0.779\n",
      "Epoch: [52, training batch   200] running training loss: 0.730\n",
      "Epoch: [52, training batch   300] running training loss: 0.735\n",
      "Epoch: [52, training batch   400] running training loss: 0.782\n",
      "Epoch: [52, training batch   500] running training loss: 0.703\n",
      "Epoch: [52, training batch   600] running training loss: 0.694\n",
      "Epoch: [52, training batch   700] running training loss: 0.733\n",
      "In epoch 52, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 1.389\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 2.4717754531567666e-05\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 52\n",
      "Epoch: [53, training batch   100] running training loss: 0.733\n",
      "Epoch: [53, training batch   200] running training loss: 0.723\n",
      "Epoch: [53, training batch   300] running training loss: 0.748\n",
      "Epoch: [53, training batch   400] running training loss: 0.734\n",
      "Epoch: [53, training batch   500] running training loss: 0.744\n",
      "Epoch: [53, training batch   600] running training loss: 0.739\n",
      "Epoch: [53, training batch   700] running training loss: 0.718\n",
      "In epoch 53, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 1.308\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 1.9013657331975126e-05\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 53\n",
      "Epoch: [54, training batch   100] running training loss: 0.736\n",
      "Epoch: [54, training batch   200] running training loss: 0.725\n",
      "Epoch: [54, training batch   300] running training loss: 0.740\n",
      "Epoch: [54, training batch   400] running training loss: 0.731\n",
      "Epoch: [54, training batch   500] running training loss: 0.710\n",
      "Epoch: [54, training batch   600] running training loss: 0.765\n",
      "Epoch: [54, training batch   700] running training loss: 0.723\n",
      "In epoch 54, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 1.365\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 1.4625890255365481e-05\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 54\n",
      "Epoch: [55, training batch   100] running training loss: 0.705\n",
      "Epoch: [55, training batch   200] running training loss: 0.741\n",
      "Epoch: [55, training batch   300] running training loss: 0.741\n",
      "Epoch: [55, training batch   400] running training loss: 0.741\n",
      "Epoch: [55, training batch   500] running training loss: 0.732\n",
      "Epoch: [55, training batch   600] running training loss: 0.698\n",
      "Epoch: [55, training batch   700] running training loss: 0.728\n",
      "In epoch 55, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.748\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 1.12506848118196e-05\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 55\n",
      "Epoch: [56, training batch   100] running training loss: 0.720\n",
      "Epoch: [56, training batch   200] running training loss: 0.727\n",
      "Epoch: [56, training batch   300] running training loss: 0.759\n",
      "Epoch: [56, training batch   400] running training loss: 0.747\n",
      "Epoch: [56, training batch   500] running training loss: 0.703\n",
      "Epoch: [56, training batch   600] running training loss: 0.679\n",
      "Epoch: [56, training batch   700] running training loss: 0.736\n",
      "In epoch 56, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.382\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 8.654372932168923e-06\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 56\n",
      "Epoch: [57, training batch   100] running training loss: 0.752\n",
      "Epoch: [57, training batch   200] running training loss: 0.713\n",
      "Epoch: [57, training batch   300] running training loss: 0.731\n",
      "Epoch: [57, training batch   400] running training loss: 0.722\n",
      "Epoch: [57, training batch   500] running training loss: 0.715\n",
      "Epoch: [57, training batch   600] running training loss: 0.744\n",
      "Epoch: [57, training batch   700] running training loss: 0.737\n",
      "In epoch 57, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.618\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 6.657209947822248e-06\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 57\n",
      "Epoch: [58, training batch   100] running training loss: 0.731\n",
      "Epoch: [58, training batch   200] running training loss: 0.698\n",
      "Epoch: [58, training batch   300] running training loss: 0.741\n",
      "Epoch: [58, training batch   400] running training loss: 0.704\n",
      "Epoch: [58, training batch   500] running training loss: 0.733\n",
      "Epoch: [58, training batch   600] running training loss: 0.730\n",
      "Epoch: [58, training batch   700] running training loss: 0.721\n",
      "In epoch 58, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.487\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 5.120930729094037e-06\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 58\n",
      "Epoch: [59, training batch   100] running training loss: 0.709\n",
      "Epoch: [59, training batch   200] running training loss: 0.749\n",
      "Epoch: [59, training batch   300] running training loss: 0.746\n",
      "Epoch: [59, training batch   400] running training loss: 0.734\n",
      "Epoch: [59, training batch   500] running training loss: 0.723\n",
      "Epoch: [59, training batch   600] running training loss: 0.737\n",
      "Epoch: [59, training batch   700] running training loss: 0.745\n",
      "In epoch 59, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.440\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 3.9391774839184896e-06\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 59\n",
      "Epoch: [60, training batch   100] running training loss: 0.749\n",
      "Epoch: [60, training batch   200] running training loss: 0.700\n",
      "Epoch: [60, training batch   300] running training loss: 0.695\n",
      "Epoch: [60, training batch   400] running training loss: 0.715\n",
      "Epoch: [60, training batch   500] running training loss: 0.709\n",
      "Epoch: [60, training batch   600] running training loss: 0.750\n",
      "Epoch: [60, training batch   700] running training loss: 0.775\n",
      "In epoch 60, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.636\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 3.0301365260911456e-06\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 60\n",
      "Epoch: [61, training batch   100] running training loss: 0.688\n",
      "Epoch: [61, training batch   200] running training loss: 0.726\n",
      "Epoch: [61, training batch   300] running training loss: 0.699\n",
      "Epoch: [61, training batch   400] running training loss: 0.729\n",
      "Epoch: [61, training batch   500] running training loss: 0.720\n",
      "Epoch: [61, training batch   600] running training loss: 0.725\n",
      "Epoch: [61, training batch   700] running training loss: 0.760\n",
      "In epoch 61, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.390\n",
      "Being patient\n",
      "\n",
      "\n",
      "\n",
      "N is set to 2.3308742508393427e-06\n",
      "samples picked: 23062, positive samples picked: 11531.0, K: 1, epoch: 61\n",
      "Epoch: [62, training batch   100] running training loss: 0.761\n",
      "Epoch: [62, training batch   200] running training loss: 0.704\n",
      "Epoch: [62, training batch   300] running training loss: 0.734\n",
      "Epoch: [62, training batch   400] running training loss: 0.710\n",
      "Epoch: [62, training batch   500] running training loss: 0.725\n",
      "Epoch: [62, training batch   600] running training loss: 0.670\n",
      "Epoch: [62, training batch   700] running training loss: 0.707\n",
      "In epoch 62, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.917\n",
      "Early stopping, as -0.528957724571228 < 0.01\n",
      "\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "previous_loss = 1e10\n",
    "previous_state_dict = None\n",
    "optimistic_iters = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    if USE_SELF_PACED_LEARNING:\n",
    "        print(f\"N is set to {N}\")\n",
    "    running_loss = 0.0\n",
    "    number_examples_used = 0\n",
    "    dataloader, max_num_examples = create_easy_train_dataloader(tinymodel, N, K, epoch) if USE_SELF_PACED_LEARNING else (train_dataloader, len(train_dataset))\n",
    "    \n",
    "    if dataloader is not None:\n",
    "        for i, (features, masks, labels) in enumerate(dataloader):\n",
    "            tinymodel.train(True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Train on easy sequences.\n",
    "            output = tinymodel(features, masks)\n",
    "            loss = modified_loss_function(output, labels, tinymodel) if USE_MODIFIED_LOSS_FUNCTION else loss_function(output, labels, tinymodel)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            number_examples_used += len(features)\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch: [{epoch + 1}, training batch {i + 1:5d}] running training loss: {(running_loss / 100):.3f}')\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    tinymodel.train(False)\n",
    "    print(f\"In epoch {epoch + 1}, number of examples trained on is {number_examples_used} out of {max_num_examples}\")\n",
    "\n",
    "    running_val_loss = 0.0\n",
    "    for i, (val_inputs, val_masks, val_labels) in enumerate(val_dataloader):\n",
    "        val_outputs = tinymodel(val_inputs, val_masks)\n",
    "        val_loss = modified_loss_function(output, labels, tinymodel) if USE_MODIFIED_LOSS_FUNCTION else loss_function(output, labels, tinymodel)\n",
    "        running_val_loss += val_loss.item()\n",
    "    \n",
    "    normalized_running_val_loss = running_val_loss / len(val_dataloader)\n",
    "    print(f\"Validation loss: {normalized_running_val_loss:.3f}\")\n",
    "    \n",
    "    if previous_loss - normalized_running_val_loss < TOLERANCE and epoch > 50 and previous_state_dict is not None:\n",
    "        if optimistic_iters < PATIENCE:\n",
    "            print(\"Being patient\")\n",
    "            optimistic_iters += 1\n",
    "        else:\n",
    "            print(f\"Early stopping, as {previous_loss - normalized_running_val_loss} < {TOLERANCE}\")\n",
    "            tinymodel.load_state_dict(previous_state_dict)\n",
    "            break\n",
    "    else:\n",
    "        previous_loss = normalized_running_val_loss\n",
    "        previous_state_dict = tinymodel.state_dict()\n",
    "        optimistic_iters = 0\n",
    "    \n",
    "    # Once we have passed K warm-up epochs, decrease N by a factor of lambd\n",
    "    # and as such increase 1 / N, so increase the loss threshold that defines\n",
    "    # what tasks are considrered easy.\n",
    "    if epoch >= K:\n",
    "        N = N / LAMBDA\n",
    "        \n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print(\"\\n\\nFinished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b72798-671b-4794-aeaf-789940adf2cd",
   "metadata": {},
   "source": [
    "### Save Trained Model to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18207808-1174-467b-aa04-f93a76168ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled_features = []\n",
    "if USE_MODIFIED_LOSS_FUNCTION:\n",
    "    enabled_features.append(\"modified_loss\")\n",
    "if USE_SELF_PACED_LEARNING:\n",
    "    enabled_features.append(\"spl\")\n",
    "\n",
    "features_string = '_and_'.join(enabled_features)\n",
    "\n",
    "model_name = f\"model_with_{features_string}.pt\" if len(features_string) > 0 else \"base_model.pt\"\n",
    "\n",
    "torch.save(tinymodel.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554a18c-b725-4dc4-874f-adf77c04ea6c",
   "metadata": {},
   "source": [
    "## PACE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "70f8f3fd-834f-4f0c-8f40-3134b29499d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.3851203501094092\n",
      "ROC AUC: 0.757227828969821\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAASjElEQVR4nO3df4xd513n8fcHm4a2psRO2pGJAzbIAZyoXTWzIfzUmCAlLRXOKo3kAK23G8mCzZaAVtomIGFVyFIREoKqBDRqqrpq5cEkhRhE2EbGQ0DUzcalNHFMEm+jTb3Jxrhulk5AoXa//HGPpSt3HN+5d+ZOZ573Sxrdc5/zPPd5vmPrc4/PPfc4VYUkqQ3fttwLkCSNj6EvSQ0x9CWpIYa+JDXE0Jekhqxd7gVcypVXXlmbN28eauwrr7zCG9/4xsVd0Lc4a25DazW3Vi+MXvPRo0dPV9WbL2z/lg/9zZs38/jjjw81dnZ2lqmpqcVd0Lc4a25DazW3Vi+MXnOS/zNfu6d3JKkhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpId/y38gdxTNfeYYP7vvg2Oc9vOvw2OeUpEF4pC9JDblk6Cf5WJJTSZ7sa/vtJP+Y5ItJ/iTJ5X377k1yIsnTSW7ua78+yRPdvg8nyaJXI0l6TYMc6X8cuOWCtkeA66rqrcAzwL0ASbYBO4FruzH3JVnTjfkDYDewtfu58DUlSUvskqFfVY8CZy5o+0xVne2eHgE2dds7gJmqerWqngNOADck2Qi8qao+W73/if0TwK2LVIMkaUCL8UHufwH+qNu+it6bwHknu7avd9sXts8ryW56/ypgYmKC2dnZoRa2Yc0G7lh3x1BjRzHsehfD3Nzcss6/HKx59WutXli6mkcK/SS/DpwFPnW+aZ5u9Rrt86qqaWAaYHJysoa9p/T0g9Psn9s/1NhRHL5t+a7e8b7jbWit5tbqhaWreejQT7ILeBdwU3fKBnpH8Ff3ddsEvNC1b5qnXZI0RkNdspnkFuADwM9W1b/07ToI7ExyWZIt9D6wfayqXgS+luTG7qqd9wIPjbh2SdICXfJIP8l+YAq4MslJYA+9q3UuAx7prrw8UlW/WFXHkhwAnqJ32ueuqjrXvdQv0bsS6PXAw92PJGmMLhn6VTXfJ6H3v0b/vcDeedofB65b0OokSYvKb+RKUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMuGfpJPpbkVJIn+9o2JHkkybPd4/q+ffcmOZHk6SQ397Vfn+SJbt+Hk2Txy5EkvZZBjvQ/DtxyQds9wKGq2goc6p6TZBuwE7i2G3NfkjXdmD8AdgNbu58LX1OStMQuGfpV9Shw5oLmHcC+bnsfcGtf+0xVvVpVzwEngBuSbATeVFWfraoCPtE3RpI0JmuHHDdRVS8CVNWLSd7StV8FHOnrd7Jr+3q3fWH7vJLspvevAiYmJpidnR1qkRvWbOCOdXcMNXYUw653MczNzS3r/MvBmle/1uqFpat52NC/mPnO09drtM+rqqaBaYDJycmampoaajHTD06zf27/UGNHcfi2w2Of87zZ2VmG/X2tVNa8+rVWLyxdzcNevfNSd8qG7vFU134SuLqv3ybgha590zztkqQxGjb0DwK7uu1dwEN97TuTXJZkC70PbB/rTgV9LcmN3VU77+0bI0kak0ue3kmyH5gCrkxyEtgDfAg4kORO4HngdoCqOpbkAPAUcBa4q6rOdS/1S/SuBHo98HD3I0kao0uGflVd7JPQmy7Sfy+wd572x4HrFrQ6SdKi8hu5ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0JekhowU+kl+NcmxJE8m2Z/kO5JsSPJIkme7x/V9/e9NciLJ00luHn35kqSFGDr0k1wF/DIwWVXXAWuAncA9wKGq2goc6p6TZFu3/1rgFuC+JGtGW74kaSFGPb2zFnh9krXAG4AXgB3Avm7/PuDWbnsHMFNVr1bVc8AJ4IYR55ckLUCqavjByd3AXuBfgc9U1c8nebmqLu/r89WqWp/kI8CRqvpk134/8HBVPTDP6+4GdgNMTExcPzMzM9T6Tr98mjPnzgw1dhTXXHHN2Oc8b25ujnXr1i3b/MvBmle/1uqF0Wvevn370aqavLB97bAv2J2r3wFsAV4G/jjJL7zWkHna5n3HqappYBpgcnKypqamhlrj9IPT7J/bP9TYURy+7fDY5zxvdnaWYX9fK5U1r36t1QtLV/Mop3d+Gniuqv6pqr4OfBr4UeClJBsBusdTXf+TwNV94zfROx0kSRqTUUL/eeDGJG9IEuAm4DhwENjV9dkFPNRtHwR2JrksyRZgK/DYCPNLkhZo6NM7VfW5JA8AnwfOAn9P75TMOuBAkjvpvTHc3vU/luQA8FTX/66qOjfi+iVJCzB06ANU1R5gzwXNr9I76p+v/156H/xKkpaB38iVpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIaMFPpJLk/yQJJ/THI8yY8k2ZDkkSTPdo/r+/rfm+REkqeT3Dz68iVJCzHqkf7vAX9ZVT8IvA04DtwDHKqqrcCh7jlJtgE7gWuBW4D7kqwZcX5J0gIMHfpJ3gT8JHA/QFX9W1W9DOwA9nXd9gG3dts7gJmqerWqngNOADcMO78kaeFSVcMNTP4DMA08Re8o/yhwN/B/q+ryvn5frar1ST4CHKmqT3bt9wMPV9UD87z2bmA3wMTExPUzMzNDrfH0y6c5c+7MUGNHcc0V14x9zvPm5uZYt27dss2/HKx59WutXhi95u3btx+tqskL29eOsKa1wNuB91fV55L8Ht2pnIvIPG3zvuNU1TS9NxQmJydrampqqAVOPzjN/rn9Q40dxeHbDo99zvNmZ2cZ9ve1Ulnz6tdavbB0NY9yTv8kcLKqPtc9f4Dem8BLSTYCdI+n+vpf3Td+E/DCCPNLkhZo6NCvqv8HfDnJD3RNN9E71XMQ2NW17QIe6rYPAjuTXJZkC7AVeGzY+SVJCzfK6R2A9wOfSvI64EvA++i9kRxIcifwPHA7QFUdS3KA3hvDWeCuqjo34vySpAUYKfSr6gvAN31QQO+of77+e4G9o8wpSRqe38iVpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1ZOTQT7Imyd8n+fPu+YYkjyR5tntc39f33iQnkjyd5OZR55YkLcxiHOnfDRzve34PcKiqtgKHuuck2QbsBK4FbgHuS7JmEeaXJA1opNBPsgn4GeCjfc07gH3d9j7g1r72map6taqeA04AN4wyvyRpYUY90v9d4H8A3+hrm6iqFwG6x7d07VcBX+7rd7JrkySNydphByZ5F3Cqqo4mmRpkyDxtdZHX3g3sBpiYmGB2dnaoNW5Ys4E71t0x1NhRDLvexTA3N7es8y8Ha179WqsXlq7moUMf+DHgZ5O8E/gO4E1JPgm8lGRjVb2YZCNwqut/Eri6b/wm4IX5XriqpoFpgMnJyZqamhpqgdMPTrN/bv9QY0dx+LbDY5/zvNnZWYb9fa1U1rz6tVYvLF3NQ5/eqap7q2pTVW2m9wHtX1XVLwAHgV1dt13AQ932QWBnksuSbAG2Ao8NvXJJ0oKNcqR/MR8CDiS5E3geuB2gqo4lOQA8BZwF7qqqc0swvyTpIhYl9KtqFpjttr8C3HSRfnuBvYsxpyRp4fxGriQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDVmKL2dJ0qqxfd/2ZZl3z/fuWZLX9Uhfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGjJ06Ce5OsnhJMeTHEtyd9e+IckjSZ7tHtf3jbk3yYkkTye5eTEKkCQNbpQj/bPAf6+qHwJuBO5Ksg24BzhUVVuBQ91zun07gWuBW4D7kqwZZfGSpIUZOvSr6sWq+ny3/TXgOHAVsAPY13XbB9zabe8AZqrq1ap6DjgB3DDs/JKkhUtVjf4iyWbgUeA64Pmqurxv31eran2SjwBHquqTXfv9wMNV9cA8r7cb2A0wMTFx/czMzFDrOv3yac6cOzPU2FFcc8U1Y5/zvLm5OdatW7ds8y8Ha179lrPeZ77yzLLM+92XffdINW/fvv1oVU1e2L52pFUBSdYBDwK/UlX/nOSiXedpm/cdp6qmgWmAycnJmpqaGmpt0w9Os39u/1BjR3H4tsNjn/O82dlZhv19rVTWvPotZ70f3PfBZZl3zxV7lqTmka7eSfLt9AL/U1X16a75pSQbu/0bgVNd+0ng6r7hm4AXRplfkrQwo1y9E+B+4HhV/U7froPArm57F/BQX/vOJJcl2QJsBR4bdn5J0sKNcnrnx4D3AE8k+ULX9mvAh4ADSe4EngduB6iqY0kOAE/Ru/Lnrqo6N8L8kqQFGjr0q+pvmf88PcBNFxmzF9g77JySpNH4jVxJaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JasjYQz/JLUmeTnIiyT3jnl+SWjbW0E+yBvh94B3ANuCOJNvGuQZJatm4j/RvAE5U1Zeq6t+AGWDHmNcgSc1aO+b5rgK+3Pf8JPDDF3ZKshvY3T2dS/L0kPNdCZwecuzQ8p8z7in7LUvNy8yaV7/W6mWW2VFr/t75Gscd+vOlYX1TQ9U0MD3yZMnjVTU56uusJNbchtZqbq1eWLqax3165yRwdd/zTcALY16DJDVr3KH/v4CtSbYkeR2wEzg45jVIUrPGenqnqs4m+W/A/wTWAB+rqmNLOOXIp4hWIGtuQ2s1t1YvLFHNqfqmU+qSpFXKb+RKUkMMfUlqyKoI/Uvd2iE9H+72fzHJ25djnYtlgHp/vqvzi0n+LsnblmOdi2nQ23ck+Y9JziV59zjXtxQGqTnJVJIvJDmW5K/HvcbFNsDf7e9K8mdJ/qGr+X3Lsc7FkuRjSU4lefIi+xc/u6pqRf/Q+0D4fwPfB7wO+Adg2wV93gk8TO97AjcCn1vudS9xvT8KrO+237GS6x205r5+fwX8BfDu5V73GP6cLweeAr6ne/6W5V73GGr+NeC3uu03A2eA1y332keo+SeBtwNPXmT/omfXajjSH+TWDjuAT1TPEeDyJBvHvdBFcsl6q+rvquqr3dMj9L4PsZINevuO9wMPAqfGubglMkjNPwd8uqqeB6iqlV73IDUX8J1JAqyjF/pnx7vMxVNVj9Kr4WIWPbtWQ+jPd2uHq4bos1IstJY76R0prGSXrDnJVcB/Av5wjOtaSoP8OV8DrE8ym+RokveObXVLY5CaPwL8EL0vdT4B3F1V3xjP8pbFomfXuG/DsBQGubXDQLd/WCEGriXJdnqh/+NLuqKlN0jNvwt8oKrO9Q4CV7xBal4LXA/cBLwe+GySI1X1zFIvbokMUvPNwBeAnwK+H3gkyd9U1T8v8dqWy6Jn12oI/UFu7bCabv8wUC1J3gp8FHhHVX1lTGtbKoPUPAnMdIF/JfDOJGer6k/HssLFN+jf69NV9QrwSpJHgbcBKzX0B6n5fcCHqnfC+0SS54AfBB4bzxLHbtGzazWc3hnk1g4Hgfd2n4TfCPz/qnpx3AtdJJesN8n3AJ8G3rOCj/r6XbLmqtpSVZurajPwAPBfV3Dgw2B/rx8CfiLJ2iRvoHfH2uNjXudiGqTm5+n9y4YkE8APAF8a6yrHa9Gza8Uf6ddFbu2Q5Be7/X9I72qOdwIngH+hd7SwIg1Y728AVwD3dUe+Z2sF36FwwJpXlUFqrqrjSf4S+CLwDeCjVTXvpX8rwYB/zr8JfDzJE/ROfXygqlbsLZeT7AemgCuTnAT2AN8OS5dd3oZBkhqyGk7vSJIGZOhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhvw7ZMzG7bWsoCgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the metric-coverage plot: 0.5989393650703642\n"
     ]
    }
   ],
   "source": [
    "tinymodel.train(False)\n",
    "\n",
    "out = None\n",
    "lab = None\n",
    "for inputs, masks, labels in test_dataloader:\n",
    "    outputs = tinymodel(inputs, masks)\n",
    "    out = outputs\n",
    "    outputs = outputs > 0.5\n",
    "    lab = labels\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    print(f\"F1 score: {f1_score(labels, outputs)}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(labels, outputs)}\")\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "coverages = []\n",
    "calculated_metrics = []\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(out.detach().cpu().numpy(), density=False, facecolor='g', alpha=0.75)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(lab.detach().cpu().numpy(), density=False, facecolor='g', alpha=0.75)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(outputs * 1.0, density=False, facecolor='g', alpha=0.75)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "    \n",
    "AUC = 0\n",
    "confidences = 0.5 + abs(out - 0.5)\n",
    "ordered_indices = torch.argsort(confidences, descending=True)\n",
    "ordered_outputs = torch.index_select(out, 0, ordered_indices)\n",
    "ordered_labels = torch.index_select(lab, 0, ordered_indices)\n",
    "\n",
    "for confidence_threshold in np.arange(1, 0.0001, -0.001):\n",
    "    above_confidence_threshold_indices = torch.nonzero(ordered_outputs[ordered_outputs > confidence_threshold]).squeeze()\n",
    "    above_confidence_outputs = torch.index_select(ordered_outputs, 0, above_confidence_threshold_indices)\n",
    "    above_confidence_labels = torch.index_select(ordered_labels, 0, above_confidence_threshold_indices)\n",
    "    \n",
    "    if torch.sum(above_confidence_labels) == 0:\n",
    "        continue\n",
    "\n",
    "    metric = roc_auc_score(above_confidence_labels.cpu().detach().numpy(), above_confidence_outputs.cpu().detach().numpy())\n",
    "    calculated_metrics.append(metric)\n",
    "    coverage = len(above_confidence_outputs) / len(ordered_outputs)\n",
    "    coverages.append(coverage)\n",
    "    \n",
    "    AUC += metric * 0.001\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "# ax.set_linewidths([3])\n",
    "ax.plot(coverages, calculated_metrics, linewidth=3)\n",
    "ax.set_title('Metric-Coverage')\n",
    "ax.set_xlabel('Coverage')\n",
    "ax.set_ylabel('AUC')\n",
    "plt.show()\n",
    "print(f\"Area under the metric-coverage plot: {AUC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac4354-277d-43a8-b34c-3279625d362e",
   "metadata": {},
   "source": [
    "## Loading the state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51d883-0368-4527-bb1a-fd40d0a48ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load('model.pt')\n",
    "new_model = TinyModel()\n",
    "new_model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02f08b-2f03-4846-99da-df342b7c84bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.5.1]",
   "language": "python",
   "name": "conda-env-opence-v1.5.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
