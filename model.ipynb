{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7635ebcf-72d0-4401-a8f1-30c209378284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63172a0b-e8d6-4334-a41d-7d49155b9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lvl2_train.pkl\tYs_train.pkl\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca607a3-4d07-4e3e-8f40-c65c2afd40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d667d28-8c3f-4f9d-ab39-eca008a4855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_easy_tasks(model, x, mask, y, N, K, epoch):\n",
    "    criterion = nn.BCELoss(reduction='none')\n",
    "    model.train(False)\n",
    "    y_hat = model(x, mask)\n",
    "\n",
    "    # Pick tasks in the batch for which the loss is less than 1 / N.\n",
    "    loss = torch.add((1 / GAMMA) * criterion(y_hat, y), -1 / N)\n",
    "    easy_indices = list((loss < 0).nonzero()) if epoch >= K else list(loss.nonzero())\n",
    "    print(f\"indices picked: {len(easy_indices)}, K: {K}, epoch: {epoch}\")\n",
    "\n",
    "    easy_timeseries = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_masks = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_labels = torch.empty((len(easy_indices),))\n",
    "\n",
    "    for i, index in enumerate(easy_indices):\n",
    "        index = index.item()\n",
    "        easy_timeseries[i] = x[index]\n",
    "        easy_masks[i] = mask[index]\n",
    "        easy_labels[i] = y[index]\n",
    "\n",
    "    model.train(True)\n",
    "    return torch.Tensor(easy_timeseries).to(device), torch.Tensor(easy_masks).to(device), torch.Tensor(easy_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80cac9b9-7b0a-41b1-8216-b0e4e852d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1908592/3938625581.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /scratch/env/opence1.5.1/conda-bld/pytorch-base_1643072044833/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  features = torch.tensor(features, dtype=torch.float).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features: torch.Size([15591, 48, 104])\n",
      "Shape of masks: torch.Size([15591, 48, 104])\n",
      "Shape of labels: torch.Size([15591])\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/lvl2_train.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f).sort_values(['subject_id', 'hadm_id', 'icustay_id'])\n",
    "\n",
    "with open(\"data/Ys_train.pkl\", \"rb\") as f:\n",
    "    labels = pickle.load(f).sort_values(['subject_id', 'hadm_id', 'icustay_id'])\n",
    "    \n",
    "data = data.reset_index(drop=True).droplevel(level=\"LEVEL2\", axis=1)\n",
    "features = data[\"mean\"].to_numpy()\n",
    "masks = data[\"mask\"].to_numpy()\n",
    "\n",
    "features = np.split(features, [48 * i for i in range(1, len(features) // 48)])\n",
    "masks = np.split(masks, [48 * i for i in range(1, len(masks) // 48)])\n",
    "\n",
    "features = torch.tensor(features, dtype=torch.float).to(device)\n",
    "masks = torch.tensor(masks, dtype=torch.float).to(device)\n",
    "labels = torch.squeeze(torch.tensor(labels.to_numpy(), dtype=torch.float).to(device))\n",
    "\n",
    "print(f\"Shape of features: {features.size()}\")\n",
    "print(f\"Shape of masks: {masks.size()}\")\n",
    "print(f\"Shape of labels: {labels.size()}\")\n",
    "assert len(features) == len(labels)\n",
    "assert features.size() == masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab338ea2-8610-40ed-87aa-210e368df325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 12473 ICU stays with 48-hour timeseries for 104 measurements each\n",
      "Test dataset size: 1559 ICU stays with 48-hour timeseries for 104 measurements each\n",
      "Validation dataset size: 1559 ICU stays with 48-hour timeseries for 104 measurements each\n"
     ]
    }
   ],
   "source": [
    "lengths = [int(round(len(features) * 8 / 10)), int(round(len(features) / 10)), int(round(len(features) / 10))]\n",
    "splits = [lengths[0], lengths[0] + lengths[1]]\n",
    "\n",
    "train_features = features[0:splits[0]]\n",
    "train_masks = masks[0:splits[0]]\n",
    "train_labels = labels[0:splits[0]]\n",
    "\n",
    "def create_easy_train_dataloader(model, N, K, epoch):\n",
    "    easy_timeseries, easy_masks, easy_labels = pick_easy_tasks(model, train_features, train_masks, train_labels, N, K, epoch)\n",
    "    easy_train_dataset = TensorDataset(easy_timeseries, easy_masks, easy_labels)\n",
    "    if len(easy_train_dataset) == 0:\n",
    "        return None, len(train_features)\n",
    "    return DataLoader(easy_train_dataset, batch_size=32, shuffle=True), len(train_features)\n",
    "\n",
    "test_dataset = TensorDataset(features[splits[0]:splits[1]], masks[splits[0]:splits[1]], labels[splits[0]:splits[1]])\n",
    "validation_dataset = TensorDataset(features[splits[1]:], masks[splits[1]:], labels[splits[1]:])\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=len(validation_dataset), shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_features)} ICU stays with 48-hour timeseries for {features.size()[2]} measurements each\")\n",
    "print(f\"Test dataset size: {len(test_dataset)} ICU stays with 48-hour timeseries for {features.size()[2]} measurements each\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)} ICU stays with 48-hour timeseries for {features.size()[2]} measurements each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aacdfe67-c25f-4bf9-9f0f-6b6137173db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F1 score constant 1: 0.1506524317912218\n",
      "Baseline F1 score uniform: 0.14722536806342015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "constant_classifier = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "uniform_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "inputs, labels = None, None\n",
    "for inp, mask, lab in validation_dataloader:\n",
    "    inputs, labels = inp, lab\n",
    "    labels = labels.clone().cpu().detach()\n",
    "\n",
    "constant_classifier.fit([0] * len(labels), labels)\n",
    "uniform_classifier.fit([0] * len(labels), labels)\n",
    "y_constant = constant_classifier.predict([0] * len(labels))\n",
    "y_uniform = uniform_classifier.predict([0] * len(labels))\n",
    "\n",
    "print(f\"Baseline F1 score constant 1: {f1_score(labels, y_constant)}\")\n",
    "print(f\"Baseline F1 score uniform: {f1_score(labels, y_uniform)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd26fc87-1982-4e16-9f41-142e0c57d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.5\n",
    "N = 16\n",
    "LAMBDA = 1.3\n",
    "LR = 0.001\n",
    "OMEGA = 0.001\n",
    "K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b48d7e17-b9bb-41f0-b4f4-73e8dda0a38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (rnn): GRU(104, 32, batch_first=True)\n",
      "  (fc1): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=104, hidden_size=32, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        try:\n",
    "            x = x * mask\n",
    "        except:\n",
    "            print(x, mask)\n",
    "        _, h_n = self.rnn(x)\n",
    "        fc1_out = self.fc1(torch.squeeze(h_n))\n",
    "        # Multiply prediction value before sigmoid activation by gamma.\n",
    "        # This is part of the micro framework.\n",
    "        r = torch.mul(fc1_out, GAMMA)\n",
    "        res = torch.sigmoid(r)\n",
    "        return torch.squeeze(res)\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "tinymodel.to(device)\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae80b486-c86a-41e7-947c-87afbf624ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(tinymodel.parameters(), LR)\n",
    "\n",
    "def loss_func(y_hat, y, model):\n",
    "    # Add a dimension when there is only one item in the batch.     \n",
    "    if len(y_hat.size()) == 0:\n",
    "        y_hat = torch.unsqueeze(y_hat, dim=0)\n",
    "\n",
    "    l1 = OMEGA * sum(p.abs().sum() for p in model.parameters())\n",
    "    cr = (1 / GAMMA) * criterion(y_hat, y)\n",
    "    \n",
    "    # Calculate the loss. The regularization is just an l1 regularization with a coefficient.\n",
    "    # The criterion is the binary cross-entropy multiplied by 1 / GAMMA, a part of the micro framework.\n",
    "    return l1 + cr\n",
    "\n",
    "    # The paper also says to subtract batch_size / N, but this value is different for different batches depending\n",
    "    # on the number of easy tasks in the batch, making it hard to compare loss across batches. Crucially, this \n",
    "    # is confusing when you look at the loss on training data (non-full batches) vs loss on testing data (full-size batches).\n",
    "    # This makes me think that actually the paper meant that you first pick easy tasks across the entire testing set,\n",
    "    # and then split it into batches rather than split into batches first and pick easy tasks after.\n",
    "#     batch_size = y_hat.size()[0]    \n",
    "#     neg = batch_size / N\n",
    "#     return l1 + cr - neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f1ea2c4-ff8c-4aec-a221-2a32c9f9b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices picked: 12473, K: 1, epoch: 0\n",
      "Epoch: [1, training batch    50] running training loss: 2.235\n",
      "Epoch: [1, training batch   100] running training loss: 1.338\n",
      "Epoch: [1, training batch   150] running training loss: 0.862\n",
      "Epoch: [1, training batch   200] running training loss: 0.641\n",
      "Epoch: [1, training batch   250] running training loss: 0.599\n",
      "Epoch: [1, training batch   300] running training loss: 0.584\n",
      "Epoch: [1, training batch   350] running training loss: 0.590\n",
      "In epoch 1, number of examples trained on is 12473 out of 12473\n",
      "Test loss: 0.6373447775840759\n",
      "N is now 16\n",
      "\n",
      "\n",
      "indices picked: 1, K: 1, epoch: 1\n",
      "In epoch 2, number of examples trained on is 1 out of 12473\n",
      "Test loss: 0.6367767453193665\n",
      "N is now 12.307692307692307\n",
      "\n",
      "\n",
      "indices picked: 107, K: 1, epoch: 2\n",
      "In epoch 3, number of examples trained on is 107 out of 12473\n",
      "Test loss: 0.6357297301292419\n",
      "N is now 9.467455621301774\n",
      "\n",
      "\n",
      "indices picked: 2366, K: 1, epoch: 3\n",
      "Epoch: [4, training batch    50] running training loss: 0.187\n",
      "In epoch 4, number of examples trained on is 2366 out of 12473\n",
      "Test loss: 0.7035498023033142\n",
      "N is now 7.282658170232134\n",
      "\n",
      "\n",
      "indices picked: 11015, K: 1, epoch: 4\n",
      "Epoch: [5, training batch    50] running training loss: 0.166\n",
      "Epoch: [5, training batch   100] running training loss: 0.157\n",
      "Epoch: [5, training batch   150] running training loss: 0.147\n",
      "Epoch: [5, training batch   200] running training loss: 0.140\n",
      "Epoch: [5, training batch   250] running training loss: 0.133\n",
      "Epoch: [5, training batch   300] running training loss: 0.131\n",
      "In epoch 5, number of examples trained on is 11015 out of 12473\n",
      "Test loss: 0.8232068419456482\n",
      "N is now 5.60204474633241\n",
      "\n",
      "\n",
      "indices picked: 11376, K: 1, epoch: 5\n",
      "Epoch: [6, training batch    50] running training loss: 0.126\n",
      "Epoch: [6, training batch   100] running training loss: 0.131\n",
      "Epoch: [6, training batch   150] running training loss: 0.135\n",
      "Epoch: [6, training batch   200] running training loss: 0.132\n",
      "Epoch: [6, training batch   250] running training loss: 0.131\n",
      "Epoch: [6, training batch   300] running training loss: 0.123\n",
      "Epoch: [6, training batch   350] running training loss: 0.111\n",
      "In epoch 6, number of examples trained on is 11376 out of 12473\n",
      "Test loss: 0.8793737292289734\n",
      "N is now 4.309265189486469\n",
      "\n",
      "\n",
      "indices picked: 11437, K: 1, epoch: 6\n",
      "Epoch: [7, training batch    50] running training loss: 0.109\n",
      "Epoch: [7, training batch   100] running training loss: 0.107\n",
      "Epoch: [7, training batch   150] running training loss: 0.110\n",
      "Epoch: [7, training batch   200] running training loss: 0.117\n",
      "Epoch: [7, training batch   250] running training loss: 0.123\n",
      "Epoch: [7, training batch   300] running training loss: 0.109\n",
      "Epoch: [7, training batch   350] running training loss: 0.103\n",
      "In epoch 7, number of examples trained on is 11437 out of 12473\n",
      "Test loss: 0.9096556305885315\n",
      "N is now 3.314819376528053\n",
      "\n",
      "\n",
      "indices picked: 11460, K: 1, epoch: 7\n",
      "Epoch: [8, training batch    50] running training loss: 0.101\n",
      "Epoch: [8, training batch   100] running training loss: 0.101\n",
      "Epoch: [8, training batch   150] running training loss: 0.099\n",
      "Epoch: [8, training batch   200] running training loss: 0.098\n",
      "Epoch: [8, training batch   250] running training loss: 0.096\n",
      "Epoch: [8, training batch   300] running training loss: 0.099\n",
      "Epoch: [8, training batch   350] running training loss: 0.111\n",
      "In epoch 8, number of examples trained on is 11460 out of 12473\n",
      "Test loss: 0.939277172088623\n",
      "N is now 2.5498610588677333\n",
      "\n",
      "\n",
      "indices picked: 11467, K: 1, epoch: 8\n",
      "Epoch: [9, training batch    50] running training loss: 0.098\n",
      "Epoch: [9, training batch   100] running training loss: 0.095\n",
      "Epoch: [9, training batch   150] running training loss: 0.102\n",
      "Epoch: [9, training batch   200] running training loss: 0.098\n",
      "Epoch: [9, training batch   250] running training loss: 0.091\n",
      "Epoch: [9, training batch   300] running training loss: 0.092\n",
      "Epoch: [9, training batch   350] running training loss: 0.091\n",
      "In epoch 9, number of examples trained on is 11467 out of 12473\n",
      "Test loss: 0.9364805221557617\n",
      "N is now 1.96143158374441\n",
      "\n",
      "\n",
      "indices picked: 11485, K: 1, epoch: 9\n",
      "Epoch: [10, training batch    50] running training loss: 0.093\n",
      "Epoch: [10, training batch   100] running training loss: 0.101\n",
      "Epoch: [10, training batch   150] running training loss: 0.095\n",
      "Epoch: [10, training batch   200] running training loss: 0.092\n",
      "Epoch: [10, training batch   250] running training loss: 0.089\n",
      "Epoch: [10, training batch   300] running training loss: 0.087\n",
      "Epoch: [10, training batch   350] running training loss: 0.085\n",
      "In epoch 10, number of examples trained on is 11485 out of 12473\n",
      "Test loss: 0.9487264156341553\n",
      "N is now 1.5087935259572385\n",
      "\n",
      "\n",
      "indices picked: 11494, K: 1, epoch: 10\n",
      "Epoch: [11, training batch    50] running training loss: 0.095\n",
      "Epoch: [11, training batch   100] running training loss: 0.096\n",
      "Epoch: [11, training batch   150] running training loss: 0.088\n",
      "Epoch: [11, training batch   200] running training loss: 0.083\n",
      "Epoch: [11, training batch   250] running training loss: 0.088\n",
      "Epoch: [11, training batch   300] running training loss: 0.083\n",
      "Epoch: [11, training batch   350] running training loss: 0.083\n",
      "In epoch 11, number of examples trained on is 11494 out of 12473\n",
      "Test loss: 0.9486435055732727\n",
      "N is now 1.1606104045824912\n",
      "\n",
      "\n",
      "indices picked: 11500, K: 1, epoch: 11\n",
      "Epoch: [12, training batch    50] running training loss: 0.082\n",
      "Epoch: [12, training batch   100] running training loss: 0.082\n",
      "Epoch: [12, training batch   150] running training loss: 0.089\n",
      "Epoch: [12, training batch   200] running training loss: 0.087\n",
      "Epoch: [12, training batch   250] running training loss: 0.086\n",
      "Epoch: [12, training batch   300] running training loss: 0.085\n",
      "Epoch: [12, training batch   350] running training loss: 0.086\n",
      "In epoch 12, number of examples trained on is 11500 out of 12473\n",
      "Test loss: 0.9362325072288513\n",
      "N is now 0.892777234294224\n",
      "\n",
      "\n",
      "indices picked: 11526, K: 1, epoch: 12\n",
      "Epoch: [13, training batch    50] running training loss: 0.086\n",
      "Epoch: [13, training batch   100] running training loss: 0.079\n",
      "Epoch: [13, training batch   150] running training loss: 0.089\n",
      "Epoch: [13, training batch   200] running training loss: 0.085\n",
      "Epoch: [13, training batch   250] running training loss: 0.089\n",
      "Epoch: [13, training batch   300] running training loss: 0.090\n",
      "Epoch: [13, training batch   350] running training loss: 0.091\n",
      "In epoch 13, number of examples trained on is 11526 out of 12473\n",
      "Test loss: 0.9344879984855652\n",
      "N is now 0.6867517186878646\n",
      "\n",
      "\n",
      "indices picked: 11540, K: 1, epoch: 13\n",
      "Epoch: [14, training batch    50] running training loss: 0.088\n",
      "Epoch: [14, training batch   100] running training loss: 0.090\n",
      "Epoch: [14, training batch   150] running training loss: 0.080\n",
      "Epoch: [14, training batch   200] running training loss: 0.088\n",
      "Epoch: [14, training batch   250] running training loss: 0.085\n",
      "Epoch: [14, training batch   300] running training loss: 0.107\n",
      "Epoch: [14, training batch   350] running training loss: 0.086\n",
      "In epoch 14, number of examples trained on is 11540 out of 12473\n",
      "Test loss: 0.9449017643928528\n",
      "N is now 0.5282705528368189\n",
      "\n",
      "\n",
      "indices picked: 11565, K: 1, epoch: 14\n",
      "Epoch: [15, training batch    50] running training loss: 0.087\n",
      "Epoch: [15, training batch   100] running training loss: 0.087\n",
      "Epoch: [15, training batch   150] running training loss: 0.095\n",
      "Epoch: [15, training batch   200] running training loss: 0.086\n",
      "Epoch: [15, training batch   250] running training loss: 0.090\n",
      "Epoch: [15, training batch   300] running training loss: 0.084\n",
      "Epoch: [15, training batch   350] running training loss: 0.086\n",
      "In epoch 15, number of examples trained on is 11565 out of 12473\n",
      "Test loss: 0.9820404052734375\n",
      "N is now 0.4063619637206299\n",
      "\n",
      "\n",
      "indices picked: 11583, K: 1, epoch: 15\n",
      "Epoch: [16, training batch    50] running training loss: 0.085\n",
      "Epoch: [16, training batch   100] running training loss: 0.095\n",
      "Epoch: [16, training batch   150] running training loss: 0.093\n",
      "Epoch: [16, training batch   200] running training loss: 0.088\n",
      "Epoch: [16, training batch   250] running training loss: 0.086\n",
      "Epoch: [16, training batch   300] running training loss: 0.094\n",
      "Epoch: [16, training batch   350] running training loss: 0.088\n",
      "In epoch 16, number of examples trained on is 11583 out of 12473\n",
      "Test loss: 0.8628273606300354\n",
      "N is now 0.312586125938946\n",
      "\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement early stopping\n",
    "for epoch in range(16):\n",
    "    running_loss = 0.0\n",
    "    number_examples_used = 0\n",
    "    dataloader, max_num_examples = create_easy_train_dataloader(tinymodel, N, K, epoch)\n",
    "    \n",
    "    if dataloader is not None:\n",
    "        for i, (features, masks, labels) in enumerate(dataloader):\n",
    "            tinymodel.train(True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Train on easy sequences.\n",
    "            output = tinymodel(features, masks)\n",
    "            loss = loss_func(output, labels, tinymodel)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            number_examples_used += len(features)\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f'Epoch: [{epoch + 1}, training batch {i + 1:5d}] running training loss: {running_loss / 50:.3f}')\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    running_test_loss = 0.0\n",
    "    tinymodel.train(False)\n",
    "    print(f\"In epoch {epoch + 1}, number of examples trained on is {number_examples_used} out of {max_num_examples}\")\n",
    "    for i, (test_inputs, test_masks, test_labels) in enumerate(test_dataloader):\n",
    "        test_outputs = tinymodel(test_inputs, test_masks)\n",
    "        test_loss = loss_func(test_outputs, test_labels, tinymodel)\n",
    "        running_test_loss += test_loss\n",
    "    \n",
    "    print(f\"Test loss: {running_test_loss / len(test_dataloader)}\")\n",
    "    # Once we have passed K warm-up epochs, decrease N by a factor of lambd\n",
    "    # and as such increase 1 / N, so increase the loss threshold that defines\n",
    "    # what tasks are considrered easy.\n",
    "    if epoch >= K:\n",
    "        N = N / LAMBDA\n",
    "    print(f\"N is now {N}\\n\\n\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6592c29c-748d-4424-8949-8fb5c0df3a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.07462686567164178\n",
      "ROC AUC: 0.5189867153477323\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "tinymodel.train(False)\n",
    "out = None\n",
    "lab = None\n",
    "for inputs, masks, labels in validation_dataloader:\n",
    "    outputs = tinymodel(inputs, masks)\n",
    "    out = outputs\n",
    "    outputs = outputs > 0.5\n",
    "    lab = labels\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    print(f\"F1 score: {f1_score(labels, outputs)}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(labels, outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b51e6675-004d-44e8-9554-4a838774891b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the metric-coverage plot: 0.07394205579075401\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "# Fill with real values\n",
    "preds = out.cpu().detach().numpy()\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "coverages = []\n",
    "calculated_metrics = []\n",
    "    \n",
    "criterion = nn.BCELoss(reduction='none')\n",
    "loss = (1 / GAMMA) * criterion(out, lab)\n",
    "ordered_indices = torch.argsort(loss)\n",
    "ordered_outputs = 1.0 * (torch.index_select(out, 0, ordered_indices) > 0.5)\n",
    "ordered_labels = torch.index_select(lab, 0, ordered_indices)\n",
    "\n",
    "ordered_outputs = ordered_outputs.cpu().detach().numpy()\n",
    "ordered_labels = ordered_labels.cpu().detach().numpy()\n",
    "\n",
    "AUC = 0\n",
    "\n",
    "for coverage in np.arange(0.9, 1.01, 0.01):\n",
    "    length = len(ordered_outputs)\n",
    "    cutoff_index = int(length * coverage) + 1\n",
    "    \n",
    "    included_outputs = ordered_outputs[0:cutoff_index]\n",
    "    included_labels = ordered_labels[0:cutoff_index]\n",
    "    \n",
    "    metric = metrics.roc_auc_score(included_labels, included_outputs)\n",
    "    calculated_metrics.append(metric)\n",
    "    coverages.append(coverage)\n",
    "    \n",
    "    AUC += metric * 0.01\n",
    "    \n",
    "plt.plot(coverages, calculated_metrics)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()\n",
    "print(f\"Area under the metric-coverage plot: {AUC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "809bca14-de0c-4abc-ae57-951a28e8ea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92, 0.93, 0.9400000000000001, 0.9500000000000001, 0.9600000000000001, 0.9700000000000001, 0.9800000000000001, 0.9900000000000001, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(coverages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5849a2f-8ac8-4a26-ab95-3212a5395a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.5.1]",
   "language": "python",
   "name": "conda-env-opence-v1.5.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
