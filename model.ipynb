{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7635ebcf-72d0-4401-a8f1-30c209378284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from imblearn.over_sampling import RandomOverSampler # conda install -c conda-forge imbalanced-learn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63172a0b-e8d6-4334-a41d-7d49155b9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lvl2_train.pkl\tYs_train.pkl\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca607a3-4d07-4e3e-8f40-c65c2afd40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d667d28-8c3f-4f9d-ab39-eca008a4855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_easy_tasks(model, x, mask, y, N, K, epoch):\n",
    "    criterion = nn.BCELoss(reduction='none')\n",
    "    model.train(False)\n",
    "    y_hat = model(x, mask)\n",
    "\n",
    "    # Pick tasks in the batch for which the loss is less than 1 / N.\n",
    "    loss = torch.add((1 / GAMMA) * criterion(y_hat, y), -1 / N)\n",
    "    easy_indices = list((loss < 0).nonzero()) if epoch >= K else list(loss.nonzero())\n",
    "\n",
    "    easy_timeseries = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_masks = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_labels = torch.empty((len(easy_indices),))\n",
    "\n",
    "    for i, index in enumerate(easy_indices):\n",
    "        index = index.item()\n",
    "        easy_timeseries[i] = x[index]\n",
    "        easy_masks[i] = mask[index]\n",
    "        easy_labels[i] = y[index]\n",
    "\n",
    "    print(f\"samples picked: {len(easy_indices)}, positive samples picked: {sum(easy_labels)}, K: {K}, epoch: {epoch}\")\n",
    "\n",
    "    model.train(True)\n",
    "    return torch.Tensor(easy_timeseries).to(device), torch.Tensor(easy_masks).to(device), torch.Tensor(easy_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80cac9b9-7b0a-41b1-8216-b0e4e852d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features: torch.Size([15591, 48, 104])\n",
      "Shape of masks: torch.Size([15591, 48, 104])\n",
      "Shape of labels: torch.Size([15591])\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/lvl2_train.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f).sort_values(['subject_id', 'hadm_id', 'icustay_id'])\n",
    "\n",
    "with open(\"data/Ys_train.pkl\", \"rb\") as f:\n",
    "    labels = pickle.load(f).sort_values(['subject_id', 'hadm_id', 'icustay_id'])\n",
    "    \n",
    "data = data.reset_index(drop=True).droplevel(level=\"LEVEL2\", axis=1)\n",
    "features = data[\"mean\"].to_numpy()\n",
    "masks = data[\"mask\"].to_numpy()\n",
    "\n",
    "features = np.split(features, [48 * i for i in range(1, len(features) // 48)])\n",
    "masks = np.split(masks, [48 * i for i in range(1, len(masks) // 48)])\n",
    "\n",
    "features = torch.tensor(features, dtype=torch.float).to(device)\n",
    "masks = torch.tensor(masks, dtype=torch.float).to(device)\n",
    "labels = torch.squeeze(torch.tensor(labels.to_numpy(), dtype=torch.float).to(device))\n",
    "\n",
    "print(f\"Shape of features: {features.size()}\")\n",
    "print(f\"Shape of masks: {masks.size()}\")\n",
    "print(f\"Shape of labels: {labels.size()}\")\n",
    "assert len(features) == len(labels)\n",
    "assert features.size() == masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9044a1b-216c-4dfc-8483-87981772c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(zip(features, masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f036cb14-0151-4b58-8fc8-9d5fc1164a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iandre3/.conda/envs/mimic-extract-2/lib/python3.9/site-packages/sklearn/utils/validation.py:746: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n",
      "/home/iandre3/.conda/envs/mimic-extract-2/lib/python3.9/site-packages/sklearn/utils/validation.py:746: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, labels_resampled = ros.fit_resample(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97abf6cc-917f-417d-b3a9-8a41e761aa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28482, 48, 104)\n",
      "(28482, 48, 104)\n",
      "(28482,)\n",
      "torch.Size([28482, 48, 104])\n",
      "torch.Size([28482, 48, 104])\n",
      "torch.Size([28482])\n"
     ]
    }
   ],
   "source": [
    "features_resampled, masks_resampled = list(zip(*X_resampled))\n",
    "\n",
    "features_resampled = np.array([np.array(xi) for xi in features_resampled])\n",
    "masks_resampled = np.array([np.array(xi) for xi in masks_resampled])\n",
    "\n",
    "print(features_resampled.shape)\n",
    "print(masks_resampled.shape)\n",
    "print(labels_resampled.shape)\n",
    "\n",
    "features = torch.tensor(features_resampled, dtype=torch.float).to(device)\n",
    "masks = torch.tensor(masks_resampled, dtype=torch.float).to(device)\n",
    "labels = torch.tensor(labels_resampled, dtype=torch.float).to(device)\n",
    "\n",
    "print(features.shape)\n",
    "print(masks.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab338ea2-8610-40ed-87aa-210e368df325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 23069 ICU stays with 48-hour timeseries for 104 measurements each\n",
      "Test dataset size: 2849 ICU stays with 48-hour timeseries for 104 measurements each\n",
      "Validation dataset size: 2564 ICU stays with 48-hour timeseries for 104 measurements each\n"
     ]
    }
   ],
   "source": [
    "train_features, test_features, train_masks, test_masks, train_labels, test_labels = train_test_split(features, masks, labels, test_size=0.1, random_state=42)\n",
    "train_features, validation_features, train_masks, validation_masks, train_labels, validation_labels = train_test_split(train_features, train_masks, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "def create_easy_train_dataloader(model, N, K, epoch):\n",
    "    easy_timeseries, easy_masks, easy_labels = pick_easy_tasks(model, train_features, train_masks, train_labels, N, K, epoch)\n",
    "    easy_train_dataset = TensorDataset(easy_timeseries, easy_masks, easy_labels)\n",
    "    if len(easy_train_dataset) == 0:\n",
    "        return None, len(train_features)\n",
    "    return DataLoader(easy_train_dataset, batch_size=32, shuffle=True), len(train_features)\n",
    "\n",
    "test_dataset = TensorDataset(test_features, test_masks, test_labels)\n",
    "validation_dataset = TensorDataset(validation_features, validation_masks, validation_labels)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=len(validation_dataset), shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_features)} ICU stays with 48-hour timeseries for {features.size()[2]} measurements each\")\n",
    "print(f\"Test dataset size: {len(test_dataset)} ICU stays with 48-hour timeseries for {features.size()[2]} measurements each\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)} ICU stays with 48-hour timeseries for {features.size()[2]} measurements each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aacdfe67-c25f-4bf9-9f0f-6b6137173db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F1 score constant 1: 0.6694343539180073\n",
      "Baseline F1 score uniform: 0.48359375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "constant_classifier = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "uniform_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "inputs, labels = None, None\n",
    "for inp, mask, lab in validation_dataloader:\n",
    "    inputs, labels = inp, lab\n",
    "    labels = labels.clone().cpu().detach()\n",
    "\n",
    "constant_classifier.fit([0] * len(labels), labels)\n",
    "uniform_classifier.fit([0] * len(labels), labels)\n",
    "y_constant = constant_classifier.predict([0] * len(labels))\n",
    "y_uniform = uniform_classifier.predict([0] * len(labels))\n",
    "\n",
    "print(f\"Baseline F1 score constant 1: {f1_score(labels, y_constant)}\")\n",
    "print(f\"Baseline F1 score uniform: {f1_score(labels, y_uniform)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd26fc87-1982-4e16-9f41-142e0c57d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.5\n",
    "N = 16\n",
    "LAMBDA = 1.3\n",
    "LR = 0.001\n",
    "OMEGA = 0.001\n",
    "K = 1\n",
    "TOLERANCE = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b48d7e17-b9bb-41f0-b4f4-73e8dda0a38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (rnn): GRU(104, 32, batch_first=True)\n",
      "  (fc1): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=104, hidden_size=32, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        try:\n",
    "            x = x * mask\n",
    "        except:\n",
    "            print(x, mask)\n",
    "        _, h_n = self.rnn(x)\n",
    "        fc1_out = self.fc1(torch.squeeze(h_n))\n",
    "        # Multiply prediction value before sigmoid activation by gamma.\n",
    "        # This is part of the micro framework.\n",
    "        r = torch.mul(fc1_out, GAMMA)\n",
    "        res = torch.sigmoid(r)\n",
    "        return torch.squeeze(res)\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "tinymodel.to(device)\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae80b486-c86a-41e7-947c-87afbf624ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(tinymodel.parameters(), LR)\n",
    "\n",
    "def loss_func(y_hat, y, model):\n",
    "    # Add a dimension when there is only one item in the batch.     \n",
    "    if len(y_hat.size()) == 0:\n",
    "        y_hat = torch.unsqueeze(y_hat, dim=0)\n",
    "\n",
    "    l1 = OMEGA * sum(p.abs().sum() for p in model.parameters())\n",
    "    cr = (1 / GAMMA) * criterion(y_hat, y)\n",
    "    \n",
    "    # Calculate the loss. The regularization is just an l1 regularization with a coefficient.\n",
    "    # The criterion is the binary cross-entropy multiplied by 1 / GAMMA, a part of the micro framework.\n",
    "    return l1 + cr\n",
    "\n",
    "    # The paper also says to subtract batch_size / N, but this value is different for different batches depending\n",
    "    # on the number of easy tasks in the batch, making it hard to compare loss across batches. Crucially, this \n",
    "    # is confusing when you look at the loss on training data (non-full batches) vs loss on testing data (full-size batches).\n",
    "    # This makes me think that actually the paper meant that you first pick easy tasks across the entire testing set,\n",
    "    # and then split it into batches rather than split into batches first and pick easy tasks after.\n",
    "#     batch_size = y_hat.size()[0]    \n",
    "#     neg = batch_size / N\n",
    "#     return l1 + cr - neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f1ea2c4-ff8c-4aec-a221-2a32c9f9b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N is set to 16\n",
      "\n",
      "\n",
      "samples picked: 23069, positive samples picked: 11510.0, K: 1, epoch: 0\n",
      "Epoch: [1, training batch    50] running training loss: 2.268\n",
      "Epoch: [1, training batch   100] running training loss: 1.768\n",
      "Epoch: [1, training batch   150] running training loss: 1.446\n",
      "Epoch: [1, training batch   200] running training loss: 1.206\n",
      "Epoch: [1, training batch   250] running training loss: 1.130\n",
      "Epoch: [1, training batch   300] running training loss: 1.098\n",
      "Epoch: [1, training batch   350] running training loss: 1.073\n",
      "Epoch: [1, training batch   400] running training loss: 1.103\n",
      "Epoch: [1, training batch   450] running training loss: 1.108\n",
      "Epoch: [1, training batch   500] running training loss: 1.073\n",
      "Epoch: [1, training batch   550] running training loss: 1.026\n",
      "Epoch: [1, training batch   600] running training loss: 1.069\n",
      "Epoch: [1, training batch   650] running training loss: 1.059\n",
      "Epoch: [1, training batch   700] running training loss: 1.035\n",
      "In epoch 1, number of examples trained on is 23069 out of 23069\n",
      "Test loss: 1.0160561800003052\n",
      "N is set to 16\n",
      "\n",
      "\n",
      "samples picked: 85, positive samples picked: 85.0, K: 1, epoch: 1\n",
      "In epoch 2, number of examples trained on is 85 out of 23069\n",
      "Test loss: 1.016838788986206\n",
      "N is set to 12.307692307692307\n",
      "\n",
      "\n",
      "samples picked: 304, positive samples picked: 304.0, K: 1, epoch: 2\n",
      "In epoch 3, number of examples trained on is 304 out of 23069\n",
      "Test loss: 1.015421748161316\n",
      "N is set to 9.467455621301774\n",
      "\n",
      "\n",
      "samples picked: 914, positive samples picked: 908.0, K: 1, epoch: 3\n",
      "In epoch 4, number of examples trained on is 914 out of 23069\n",
      "Test loss: 1.0194108486175537\n",
      "N is set to 7.282658170232134\n",
      "\n",
      "\n",
      "samples picked: 3015, positive samples picked: 2901.0, K: 1, epoch: 4\n",
      "Epoch: [5, training batch    50] running training loss: 0.173\n",
      "In epoch 5, number of examples trained on is 3015 out of 23069\n",
      "Test loss: 1.5376638174057007\n",
      "N is set to 5.60204474633241\n",
      "\n",
      "\n",
      "samples picked: 11773, positive samples picked: 8939.0, K: 1, epoch: 5\n",
      "Epoch: [6, training batch    50] running training loss: 0.210\n",
      "Epoch: [6, training batch   100] running training loss: 0.203\n",
      "Epoch: [6, training batch   150] running training loss: 0.165\n",
      "Epoch: [6, training batch   200] running training loss: 0.161\n",
      "Epoch: [6, training batch   250] running training loss: 0.167\n",
      "Epoch: [6, training batch   300] running training loss: 0.154\n",
      "Epoch: [6, training batch   350] running training loss: 0.140\n",
      "In epoch 6, number of examples trained on is 11773 out of 23069\n",
      "Test loss: 2.227078676223755\n",
      "N is set to 4.309265189486469\n",
      "\n",
      "\n",
      "samples picked: 15860, positive samples picked: 9670.0, K: 1, epoch: 6\n",
      "Epoch: [7, training batch    50] running training loss: 0.200\n",
      "Epoch: [7, training batch   100] running training loss: 0.239\n",
      "Epoch: [7, training batch   150] running training loss: 0.224\n",
      "Epoch: [7, training batch   200] running training loss: 0.234\n",
      "Epoch: [7, training batch   250] running training loss: 0.223\n",
      "Epoch: [7, training batch   300] running training loss: 0.236\n",
      "Epoch: [7, training batch   350] running training loss: 0.190\n",
      "Epoch: [7, training batch   400] running training loss: 0.208\n",
      "Epoch: [7, training batch   450] running training loss: 0.212\n",
      "In epoch 7, number of examples trained on is 15860 out of 23069\n",
      "Test loss: 2.265070676803589\n",
      "N is set to 3.314819376528053\n",
      "\n",
      "\n",
      "samples picked: 16355, positive samples picked: 9727.0, K: 1, epoch: 7\n",
      "Epoch: [8, training batch    50] running training loss: 0.166\n",
      "Epoch: [8, training batch   100] running training loss: 0.184\n",
      "Epoch: [8, training batch   150] running training loss: 0.151\n",
      "Epoch: [8, training batch   200] running training loss: 0.171\n",
      "Epoch: [8, training batch   250] running training loss: 0.194\n",
      "Epoch: [8, training batch   300] running training loss: 0.201\n",
      "Epoch: [8, training batch   350] running training loss: 0.188\n",
      "Epoch: [8, training batch   400] running training loss: 0.163\n",
      "Epoch: [8, training batch   450] running training loss: 0.198\n",
      "Epoch: [8, training batch   500] running training loss: 0.216\n",
      "In epoch 8, number of examples trained on is 16355 out of 23069\n",
      "Test loss: 2.0212199687957764\n",
      "N is set to 2.5498610588677333\n",
      "\n",
      "\n",
      "samples picked: 16778, positive samples picked: 9550.0, K: 1, epoch: 8\n",
      "Epoch: [9, training batch    50] running training loss: 0.165\n",
      "Epoch: [9, training batch   100] running training loss: 0.153\n",
      "Epoch: [9, training batch   150] running training loss: 0.157\n",
      "Epoch: [9, training batch   200] running training loss: 0.165\n",
      "Epoch: [9, training batch   250] running training loss: 0.154\n",
      "Epoch: [9, training batch   300] running training loss: 0.146\n",
      "Epoch: [9, training batch   350] running training loss: 0.146\n",
      "Epoch: [9, training batch   400] running training loss: 0.148\n",
      "Epoch: [9, training batch   450] running training loss: 0.167\n",
      "Epoch: [9, training batch   500] running training loss: 0.147\n",
      "In epoch 9, number of examples trained on is 16778 out of 23069\n",
      "Test loss: 2.274190664291382\n",
      "N is set to 1.96143158374441\n",
      "\n",
      "\n",
      "samples picked: 17146, positive samples picked: 9385.0, K: 1, epoch: 9\n",
      "Epoch: [10, training batch    50] running training loss: 0.159\n",
      "Epoch: [10, training batch   100] running training loss: 0.176\n",
      "Epoch: [10, training batch   150] running training loss: 0.193\n",
      "Epoch: [10, training batch   200] running training loss: 0.152\n",
      "Epoch: [10, training batch   250] running training loss: 0.162\n",
      "Epoch: [10, training batch   300] running training loss: 0.172\n",
      "Epoch: [10, training batch   350] running training loss: 0.177\n",
      "Epoch: [10, training batch   400] running training loss: 0.164\n",
      "Epoch: [10, training batch   450] running training loss: 0.167\n",
      "Epoch: [10, training batch   500] running training loss: 0.163\n",
      "In epoch 10, number of examples trained on is 17146 out of 23069\n",
      "Test loss: 2.1781811714172363\n",
      "N is set to 1.5087935259572385\n",
      "\n",
      "\n",
      "samples picked: 17414, positive samples picked: 9373.0, K: 1, epoch: 10\n",
      "Epoch: [11, training batch    50] running training loss: 0.154\n",
      "Epoch: [11, training batch   100] running training loss: 0.167\n",
      "Epoch: [11, training batch   150] running training loss: 0.153\n",
      "Epoch: [11, training batch   200] running training loss: 0.169\n",
      "Epoch: [11, training batch   250] running training loss: 0.166\n",
      "Epoch: [11, training batch   300] running training loss: 0.158\n",
      "Epoch: [11, training batch   350] running training loss: 0.162\n",
      "Epoch: [11, training batch   400] running training loss: 0.168\n",
      "Epoch: [11, training batch   450] running training loss: 0.165\n",
      "Epoch: [11, training batch   500] running training loss: 0.149\n",
      "In epoch 11, number of examples trained on is 17414 out of 23069\n",
      "Test loss: 2.245795249938965\n",
      "N is set to 1.1606104045824912\n",
      "\n",
      "\n",
      "samples picked: 17685, positive samples picked: 9419.0, K: 1, epoch: 11\n",
      "Epoch: [12, training batch    50] running training loss: 0.152\n",
      "Epoch: [12, training batch   100] running training loss: 0.148\n",
      "Epoch: [12, training batch   150] running training loss: 0.146\n",
      "Epoch: [12, training batch   200] running training loss: 0.172\n",
      "Epoch: [12, training batch   250] running training loss: 0.164\n",
      "Epoch: [12, training batch   300] running training loss: 0.149\n",
      "Epoch: [12, training batch   350] running training loss: 0.142\n",
      "Epoch: [12, training batch   400] running training loss: 0.155\n",
      "Epoch: [12, training batch   450] running training loss: 0.168\n",
      "Epoch: [12, training batch   500] running training loss: 0.221\n",
      "Epoch: [12, training batch   550] running training loss: 0.165\n",
      "In epoch 12, number of examples trained on is 17685 out of 23069\n",
      "Test loss: 2.2164828777313232\n",
      "N is set to 0.892777234294224\n",
      "\n",
      "\n",
      "samples picked: 17914, positive samples picked: 9554.0, K: 1, epoch: 12\n",
      "Epoch: [13, training batch    50] running training loss: 0.148\n",
      "Epoch: [13, training batch   100] running training loss: 0.146\n",
      "Epoch: [13, training batch   150] running training loss: 0.172\n",
      "Epoch: [13, training batch   200] running training loss: 0.173\n",
      "Epoch: [13, training batch   250] running training loss: 0.176\n",
      "Epoch: [13, training batch   300] running training loss: 0.186\n",
      "Epoch: [13, training batch   350] running training loss: 0.177\n",
      "Epoch: [13, training batch   400] running training loss: 0.175\n",
      "Epoch: [13, training batch   450] running training loss: 0.193\n",
      "Epoch: [13, training batch   500] running training loss: 0.198\n",
      "Epoch: [13, training batch   550] running training loss: 0.172\n",
      "In epoch 13, number of examples trained on is 17914 out of 23069\n",
      "Test loss: 2.2356338500976562\n",
      "N is set to 0.6867517186878646\n",
      "\n",
      "\n",
      "samples picked: 18094, positive samples picked: 9521.0, K: 1, epoch: 13\n",
      "Epoch: [14, training batch    50] running training loss: 0.158\n",
      "Epoch: [14, training batch   100] running training loss: 0.158\n",
      "Epoch: [14, training batch   150] running training loss: 0.198\n",
      "Epoch: [14, training batch   200] running training loss: 0.196\n",
      "Epoch: [14, training batch   250] running training loss: 0.203\n",
      "Epoch: [14, training batch   300] running training loss: 0.190\n",
      "Epoch: [14, training batch   350] running training loss: 0.165\n",
      "Epoch: [14, training batch   400] running training loss: 0.171\n",
      "Epoch: [14, training batch   450] running training loss: 0.191\n",
      "Epoch: [14, training batch   500] running training loss: 0.187\n",
      "Epoch: [14, training batch   550] running training loss: 0.162\n",
      "In epoch 14, number of examples trained on is 18094 out of 23069\n",
      "Test loss: 2.2152554988861084\n",
      "N is set to 0.5282705528368189\n",
      "\n",
      "\n",
      "samples picked: 18301, positive samples picked: 9454.0, K: 1, epoch: 14\n",
      "Epoch: [15, training batch    50] running training loss: 0.176\n",
      "Epoch: [15, training batch   100] running training loss: 0.193\n",
      "Epoch: [15, training batch   150] running training loss: 0.213\n",
      "Epoch: [15, training batch   200] running training loss: 0.198\n",
      "Epoch: [15, training batch   250] running training loss: 0.189\n",
      "Epoch: [15, training batch   300] running training loss: 0.209\n",
      "Epoch: [15, training batch   350] running training loss: 0.190\n",
      "Epoch: [15, training batch   400] running training loss: 0.203\n",
      "Epoch: [15, training batch   450] running training loss: 0.179\n",
      "Epoch: [15, training batch   500] running training loss: 0.204\n",
      "Epoch: [15, training batch   550] running training loss: 0.245\n",
      "In epoch 15, number of examples trained on is 18301 out of 23069\n",
      "Test loss: 2.1138761043548584\n",
      "N is set to 0.4063619637206299\n",
      "\n",
      "\n",
      "samples picked: 18542, positive samples picked: 9514.0, K: 1, epoch: 15\n",
      "Epoch: [16, training batch    50] running training loss: 0.203\n",
      "Epoch: [16, training batch   100] running training loss: 0.219\n",
      "Epoch: [16, training batch   150] running training loss: 0.219\n",
      "Epoch: [16, training batch   200] running training loss: 0.201\n",
      "Epoch: [16, training batch   250] running training loss: 0.244\n",
      "Epoch: [16, training batch   300] running training loss: 0.209\n",
      "Epoch: [16, training batch   350] running training loss: 0.205\n",
      "Epoch: [16, training batch   400] running training loss: 0.221\n",
      "Epoch: [16, training batch   450] running training loss: 0.213\n",
      "Epoch: [16, training batch   500] running training loss: 0.208\n",
      "Epoch: [16, training batch   550] running training loss: 0.201\n",
      "In epoch 16, number of examples trained on is 18542 out of 23069\n",
      "Test loss: 2.144192695617676\n",
      "N is set to 0.312586125938946\n",
      "\n",
      "\n",
      "samples picked: 18772, positive samples picked: 9549.0, K: 1, epoch: 16\n",
      "Epoch: [17, training batch    50] running training loss: 0.219\n",
      "Epoch: [17, training batch   100] running training loss: 0.241\n",
      "Epoch: [17, training batch   150] running training loss: 0.254\n",
      "Epoch: [17, training batch   200] running training loss: 0.227\n",
      "Epoch: [17, training batch   250] running training loss: 0.220\n",
      "Epoch: [17, training batch   300] running training loss: 0.207\n",
      "Epoch: [17, training batch   350] running training loss: 0.230\n",
      "Epoch: [17, training batch   400] running training loss: 0.235\n",
      "Epoch: [17, training batch   450] running training loss: 0.231\n",
      "Epoch: [17, training batch   500] running training loss: 0.227\n",
      "Epoch: [17, training batch   550] running training loss: 0.211\n",
      "In epoch 17, number of examples trained on is 18772 out of 23069\n",
      "Test loss: 1.996791124343872\n",
      "N is set to 0.24045086610688154\n",
      "\n",
      "\n",
      "samples picked: 19150, positive samples picked: 9648.0, K: 1, epoch: 17\n",
      "Epoch: [18, training batch    50] running training loss: 0.318\n",
      "Epoch: [18, training batch   100] running training loss: 0.282\n",
      "Epoch: [18, training batch   150] running training loss: 0.273\n",
      "Epoch: [18, training batch   200] running training loss: 0.280\n",
      "Epoch: [18, training batch   250] running training loss: 0.272\n",
      "Epoch: [18, training batch   300] running training loss: 0.314\n",
      "Epoch: [18, training batch   350] running training loss: 0.261\n",
      "Epoch: [18, training batch   400] running training loss: 0.270\n",
      "Epoch: [18, training batch   450] running training loss: 0.300\n",
      "Epoch: [18, training batch   500] running training loss: 0.275\n",
      "Epoch: [18, training batch   550] running training loss: 0.266\n",
      "In epoch 18, number of examples trained on is 19150 out of 23069\n",
      "Test loss: 1.796933889389038\n",
      "N is set to 0.1849622046976012\n",
      "\n",
      "\n",
      "samples picked: 19722, positive samples picked: 9767.0, K: 1, epoch: 18\n",
      "Epoch: [19, training batch    50] running training loss: 0.363\n",
      "Epoch: [19, training batch   100] running training loss: 0.395\n",
      "Epoch: [19, training batch   150] running training loss: 0.360\n",
      "Epoch: [19, training batch   200] running training loss: 0.373\n",
      "Epoch: [19, training batch   250] running training loss: 0.374\n",
      "Epoch: [19, training batch   300] running training loss: 0.384\n",
      "Epoch: [19, training batch   350] running training loss: 0.351\n",
      "Epoch: [19, training batch   400] running training loss: 0.364\n",
      "Epoch: [19, training batch   450] running training loss: 0.373\n",
      "Epoch: [19, training batch   500] running training loss: 0.350\n",
      "Epoch: [19, training batch   550] running training loss: 0.355\n",
      "Epoch: [19, training batch   600] running training loss: 0.363\n",
      "In epoch 19, number of examples trained on is 19722 out of 23069\n",
      "Test loss: 1.4809073209762573\n",
      "N is set to 0.14227861899815475\n",
      "\n",
      "\n",
      "samples picked: 20900, positive samples picked: 10348.0, K: 1, epoch: 19\n",
      "Epoch: [20, training batch    50] running training loss: 0.651\n",
      "Epoch: [20, training batch   100] running training loss: 0.562\n",
      "Epoch: [20, training batch   150] running training loss: 0.578\n",
      "Epoch: [20, training batch   200] running training loss: 0.556\n",
      "Epoch: [20, training batch   250] running training loss: 0.619\n",
      "Epoch: [20, training batch   300] running training loss: 0.588\n",
      "Epoch: [20, training batch   350] running training loss: 0.520\n",
      "Epoch: [20, training batch   400] running training loss: 0.564\n",
      "Epoch: [20, training batch   450] running training loss: 0.554\n",
      "Epoch: [20, training batch   500] running training loss: 0.569\n",
      "Epoch: [20, training batch   550] running training loss: 0.551\n",
      "Epoch: [20, training batch   600] running training loss: 0.561\n",
      "Epoch: [20, training batch   650] running training loss: 0.555\n",
      "In epoch 20, number of examples trained on is 20900 out of 23069\n",
      "Test loss: 1.1995679140090942\n",
      "N is set to 0.10944509153704211\n",
      "\n",
      "\n",
      "samples picked: 22529, positive samples picked: 11146.0, K: 1, epoch: 20\n",
      "Epoch: [21, training batch    50] running training loss: 0.941\n",
      "Epoch: [21, training batch   100] running training loss: 0.862\n",
      "Epoch: [21, training batch   150] running training loss: 0.872\n",
      "Epoch: [21, training batch   200] running training loss: 0.880\n",
      "Epoch: [21, training batch   250] running training loss: 0.877\n",
      "Epoch: [21, training batch   300] running training loss: 0.817\n",
      "Epoch: [21, training batch   350] running training loss: 0.868\n",
      "Epoch: [21, training batch   400] running training loss: 0.840\n",
      "Epoch: [21, training batch   450] running training loss: 0.836\n",
      "Epoch: [21, training batch   500] running training loss: 0.869\n",
      "Epoch: [21, training batch   550] running training loss: 0.855\n",
      "Epoch: [21, training batch   600] running training loss: 0.837\n",
      "Epoch: [21, training batch   650] running training loss: 0.863\n",
      "Epoch: [21, training batch   700] running training loss: 0.813\n",
      "In epoch 21, number of examples trained on is 22529 out of 23069\n",
      "Test loss: 0.9495630860328674\n",
      "N is set to 0.08418853195157085\n",
      "\n",
      "\n",
      "samples picked: 23069, positive samples picked: 11510.0, K: 1, epoch: 21\n",
      "Epoch: [22, training batch    50] running training loss: 0.970\n",
      "Epoch: [22, training batch   100] running training loss: 0.948\n",
      "Epoch: [22, training batch   150] running training loss: 0.924\n",
      "Epoch: [22, training batch   200] running training loss: 0.951\n",
      "Epoch: [22, training batch   250] running training loss: 0.951\n",
      "Epoch: [22, training batch   300] running training loss: 0.987\n",
      "Epoch: [22, training batch   350] running training loss: 0.932\n",
      "Epoch: [22, training batch   400] running training loss: 0.963\n",
      "Epoch: [22, training batch   450] running training loss: 0.951\n",
      "Epoch: [22, training batch   500] running training loss: 0.931\n",
      "Epoch: [22, training batch   550] running training loss: 0.954\n",
      "Epoch: [22, training batch   600] running training loss: 0.910\n",
      "Epoch: [22, training batch   650] running training loss: 0.892\n",
      "Epoch: [22, training batch   700] running training loss: 0.945\n",
      "In epoch 22, number of examples trained on is 23069 out of 23069\n",
      "Test loss: 0.9215891361236572\n",
      "N is set to 0.06476040919351603\n",
      "\n",
      "\n",
      "samples picked: 23069, positive samples picked: 11510.0, K: 1, epoch: 22\n",
      "Epoch: [23, training batch    50] running training loss: 0.931\n",
      "Epoch: [23, training batch   100] running training loss: 0.915\n",
      "Epoch: [23, training batch   150] running training loss: 0.982\n",
      "Epoch: [23, training batch   200] running training loss: 0.917\n",
      "Epoch: [23, training batch   250] running training loss: 0.909\n",
      "Epoch: [23, training batch   300] running training loss: 0.910\n",
      "Epoch: [23, training batch   350] running training loss: 0.873\n",
      "Epoch: [23, training batch   400] running training loss: 0.960\n",
      "Epoch: [23, training batch   450] running training loss: 0.920\n",
      "Epoch: [23, training batch   500] running training loss: 0.920\n",
      "Epoch: [23, training batch   550] running training loss: 0.954\n",
      "Epoch: [23, training batch   600] running training loss: 0.982\n",
      "Epoch: [23, training batch   650] running training loss: 0.924\n",
      "Epoch: [23, training batch   700] running training loss: 0.889\n",
      "In epoch 23, number of examples trained on is 23069 out of 23069\n",
      "Test loss: 0.9053583741188049\n",
      "N is set to 0.04981569937962771\n",
      "\n",
      "\n",
      "samples picked: 23069, positive samples picked: 11510.0, K: 1, epoch: 23\n",
      "Epoch: [24, training batch    50] running training loss: 0.944\n",
      "Epoch: [24, training batch   100] running training loss: 0.870\n",
      "Epoch: [24, training batch   150] running training loss: 0.941\n",
      "Epoch: [24, training batch   200] running training loss: 0.963\n",
      "Epoch: [24, training batch   250] running training loss: 0.899\n",
      "Epoch: [24, training batch   300] running training loss: 0.949\n",
      "Epoch: [24, training batch   350] running training loss: 0.920\n",
      "Epoch: [24, training batch   400] running training loss: 0.906\n",
      "Epoch: [24, training batch   450] running training loss: 0.948\n",
      "Epoch: [24, training batch   500] running training loss: 0.875\n",
      "Epoch: [24, training batch   550] running training loss: 0.871\n",
      "Epoch: [24, training batch   600] running training loss: 0.896\n",
      "Epoch: [24, training batch   650] running training loss: 0.892\n",
      "Epoch: [24, training batch   700] running training loss: 0.911\n",
      "In epoch 24, number of examples trained on is 23069 out of 23069\n",
      "Test loss: 0.8846687078475952\n",
      "N is set to 0.038319768753559774\n",
      "\n",
      "\n",
      "samples picked: 23069, positive samples picked: 11510.0, K: 1, epoch: 24\n",
      "Epoch: [25, training batch    50] running training loss: 0.912\n",
      "Epoch: [25, training batch   100] running training loss: 0.931\n",
      "Epoch: [25, training batch   150] running training loss: 0.898\n",
      "Epoch: [25, training batch   200] running training loss: 0.896\n",
      "Epoch: [25, training batch   250] running training loss: 0.856\n",
      "Epoch: [25, training batch   300] running training loss: 0.892\n",
      "Epoch: [25, training batch   350] running training loss: 0.915\n",
      "Epoch: [25, training batch   400] running training loss: 0.864\n",
      "Epoch: [25, training batch   450] running training loss: 0.906\n",
      "Epoch: [25, training batch   500] running training loss: 0.892\n",
      "Epoch: [25, training batch   550] running training loss: 0.865\n",
      "Epoch: [25, training batch   600] running training loss: 0.881\n",
      "Epoch: [25, training batch   650] running training loss: 0.929\n",
      "Epoch: [25, training batch   700] running training loss: 0.915\n",
      "In epoch 25, number of examples trained on is 23069 out of 23069\n",
      "Test loss: 0.8899953365325928\n",
      "Early stopping, as -0.005326628684997559 < 0.005\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "previous_loss = 1e10\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"\\n\\nN is set to {N}\")\n",
    "    running_loss = 0.0\n",
    "    number_examples_used = 0\n",
    "    dataloader, max_num_examples = create_easy_train_dataloader(tinymodel, N, K, epoch)\n",
    "    \n",
    "    if dataloader is not None:\n",
    "        for i, (features, masks, labels) in enumerate(dataloader):\n",
    "            tinymodel.train(True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Train on easy sequences.\n",
    "            output = tinymodel(features, masks)\n",
    "            loss = loss_func(output, labels, tinymodel)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            number_examples_used += len(features)\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f'Epoch: [{epoch + 1}, training batch {i + 1:5d}] running training loss: {running_loss / 50:.3f}')\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    running_test_loss = 0.0\n",
    "    tinymodel.train(False)\n",
    "    print(f\"In epoch {epoch + 1}, number of examples trained on is {number_examples_used} out of {max_num_examples}\")\n",
    "    for i, (test_inputs, test_masks, test_labels) in enumerate(test_dataloader):\n",
    "        test_outputs = tinymodel(test_inputs, test_masks)\n",
    "        test_loss = loss_func(test_outputs, test_labels, tinymodel)\n",
    "        running_test_loss += test_loss\n",
    "    \n",
    "    normalized_running_test_loss = running_test_loss / len(test_dataloader)\n",
    "    print(f\"Test loss: {normalized_running_test_loss}\")\n",
    "    \n",
    "    if previous_loss - normalized_running_test_loss < TOLERANCE and epoch > 20:\n",
    "        print(f\"Early stopping, as {previous_loss - normalized_running_test_loss} < {TOLERANCE}\")\n",
    "        break\n",
    "    \n",
    "    previous_loss = normalized_running_test_loss\n",
    "    \n",
    "    # Once we have passed K warm-up epochs, decrease N by a factor of lambd\n",
    "    # and as such increase 1 / N, so increase the loss threshold that defines\n",
    "    # what tasks are considrered easy.\n",
    "    if epoch >= K:\n",
    "        N = N / LAMBDA\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6592c29c-748d-4424-8949-8fb5c0df3a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8370745836350469\n",
      "ROC AUC: 0.8240431771993233\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "tinymodel.train(False)\n",
    "out = None\n",
    "lab = None\n",
    "for inputs, masks, labels in validation_dataloader:\n",
    "    outputs = tinymodel(inputs, masks)\n",
    "    out = outputs\n",
    "    outputs = outputs > 0.5\n",
    "    lab = labels\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    print(f\"F1 score: {f1_score(labels, outputs)}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(labels, outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51e6675-004d-44e8-9554-4a838774891b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m AUC \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# for coverage in np.arange(0, 1.005, 0.01):\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     length = len(ordered_outputs)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     cutoff_index = int(length * coverage) + 1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     AUC += metric * 0.01\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m confidences \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[43mout\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     42\u001b[0m ordered_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(confidences, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     43\u001b[0m ordered_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mindex_select(out, \u001b[38;5;241m0\u001b[39m, ordered_indices)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "# Fill with real values\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "coverages = []\n",
    "calculated_metrics = []\n",
    "    \n",
    "# criterion = nn.BCELoss(reduction='none')\n",
    "# loss = (1 / GAMMA) * criterion(out, lab)\n",
    "# ordered_indices = torch.argsort(loss)\n",
    "# ordered_outputs = 1.0 * (torch.index_select(out, 0, ordered_indices))\n",
    "# ordered_labels = torch.index_select(lab, 0, ordered_indices)\n",
    "\n",
    "# ordered_outputs = ordered_outputs.cpu().detach().numpy()\n",
    "# ordered_labels = ordered_labels.cpu().detach().numpy()\n",
    "\n",
    "AUC = 0\n",
    "\n",
    "# for coverage in np.arange(0, 1.005, 0.01):\n",
    "#     length = len(ordered_outputs)\n",
    "#     cutoff_index = int(length * coverage) + 1\n",
    "    \n",
    "#     included_outputs = ordered_outputs[0:cutoff_index]\n",
    "#     included_labels = ordered_labels[0:cutoff_index]\n",
    "    \n",
    "#     if sum(included_labels) == 0 or sum(included_outputs) == 0 or sum(included_labels) == cutoff_index or sum(included_outputs) == cutoff_index:\n",
    "#         print(coverage)\n",
    "#         continue\n",
    "\n",
    "#     metric = metrics.roc_auc_score(included_labels, included_outputs)\n",
    "#     calculated_metrics.append(metric)\n",
    "#     coverages.append(coverage)\n",
    "    \n",
    "#     AUC += metric * 0.01\n",
    "    \n",
    "    \n",
    "confidences = 0.5 + abs(out - 0.5)\n",
    "ordered_indices = torch.argsort(confidences, descending=True)\n",
    "ordered_outputs = torch.index_select(out, 0, ordered_indices)\n",
    "ordered_labels = torch.index_select(lab, 0, ordered_indices)\n",
    "\n",
    "for confidence_threshold in np.arange(0.99, 0.01, -0.01):\n",
    "    above_confidence_threshold_indices = torch.nonzero(ordered_outputs[ordered_outputs > confidence_threshold]).squeeze()\n",
    "    above_confidence_outputs = torch.index_select(ordered_outputs, 0, above_confidence_threshold_indices)\n",
    "    above_confidence_labels = torch.index_select(ordered_labels, 0, above_confidence_threshold_indices)\n",
    "    \n",
    "    metric = metrics.roc_auc_score(above_confidence_labels.cpu().detach().numpy(), above_confidence_outputs.cpu().detach().numpy())\n",
    "    calculated_metrics.append(metric)\n",
    "    coverage = len(above_confidence_outputs) / len(ordered_outputs)\n",
    "    coverages.append(coverage)\n",
    "    \n",
    "    AUC += metric * 0.01\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_linewidths([3])\n",
    "ax.plot(coverages, calculated_metrics, linewidth=3)\n",
    "ax.set_title('Metric-Coverage')\n",
    "ax.set_xlabel('Coverage')\n",
    "ax.set_ylabel('AUC')\n",
    "plt.show()\n",
    "print(f\"Area under the metric-coverage plot: {AUC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5849a2f-8ac8-4a26-ab95-3212a5395a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mimic-extract-2]",
   "language": "python",
   "name": "conda-env-.conda-mimic-extract-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
