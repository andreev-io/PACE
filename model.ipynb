{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7635ebcf-72d0-4401-a8f1-30c209378284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63172a0b-e8d6-4334-a41d-7d49155b9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lvl2_train.pkl\tYs_train.pkl\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ca607a3-4d07-4e3e-8f40-c65c2afd40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d667d28-8c3f-4f9d-ab39-eca008a4855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_easy_tasks(model, x, mask, y, N, K, epoch):\n",
    "    criterion = nn.BCELoss(reduction='none')\n",
    "    model.train(False)\n",
    "    y_hat = model(x, mask)\n",
    "\n",
    "    # Pick tasks in the batch for which the loss is less than 1 / N.\n",
    "    loss = torch.add((1 / GAMMA) * criterion(y_hat, y), -1 / N)\n",
    "    easy_indices = list((loss < 0).nonzero()) if epoch >= K else list(loss.nonzero())\n",
    "    print(f\"indices picked: {len(easy_indices)}, K: {K}, epoch: {epoch}\")\n",
    "\n",
    "    easy_timeseries = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_masks = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_labels = torch.empty((len(easy_indices),))\n",
    "\n",
    "    for i, index in enumerate(easy_indices):\n",
    "        index = index.item()\n",
    "        easy_timeseries[i] = x[index]\n",
    "        easy_masks[i] = mask[index]\n",
    "        easy_labels[i] = y[index]\n",
    "\n",
    "    model.train(True)\n",
    "    return torch.Tensor(easy_timeseries).to(device), torch.Tensor(easy_masks).to(device), torch.Tensor(easy_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80cac9b9-7b0a-41b1-8216-b0e4e852d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features: torch.Size([15591, 48, 104])\n",
      "Shape of masks: torch.Size([15591, 48, 104])\n",
      "Shape of labels: torch.Size([15591])\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/lvl2_train.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f).sort_values(['subject_id', 'hadm_id', 'icustay_id'])\n",
    "\n",
    "with open(\"data/Ys_train.pkl\", \"rb\") as f:\n",
    "    labels = pickle.load(f).sort_values(['subject_id', 'hadm_id', 'icustay_id'])\n",
    "    \n",
    "data = data.reset_index(drop=True).droplevel(level=\"LEVEL2\", axis=1)\n",
    "features = data[\"mean\"].to_numpy()\n",
    "masks = data[\"mask\"].to_numpy()\n",
    "\n",
    "features = np.split(features, [48 * i for i in range(1, len(features) // 48)])\n",
    "masks = np.split(masks, [48 * i for i in range(1, len(masks) // 48)])\n",
    "\n",
    "features = torch.tensor(features, dtype=torch.float).to(device)\n",
    "masks = torch.tensor(masks, dtype=torch.float).to(device)\n",
    "labels = torch.squeeze(torch.tensor(labels.to_numpy(), dtype=torch.float).to(device))\n",
    "\n",
    "print(f\"Shape of features: {features.size()}\")\n",
    "print(f\"Shape of masks: {masks.size()}\")\n",
    "print(f\"Shape of labels: {labels.size()}\")\n",
    "assert len(features) == len(labels)\n",
    "assert features.size() == masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab338ea2-8610-40ed-87aa-210e368df325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 12473 ICU stays with 48-hour timeseries for 104 measurements each\n",
      "Test dataset size: 1559 ICU stays with 48-hour timeseries for 104 measurements each\n",
      "Validation dataset size: 1559 ICU stays with 48-hour timeseries for 104 measurements each\n"
     ]
    }
   ],
   "source": [
    "lengths = [int(round(len(features) * 8 / 10)), int(round(len(features) / 10)), int(round(len(features) / 10))]\n",
    "splits = [lengths[0], lengths[0] + lengths[1]]\n",
    "\n",
    "train_features = features[0:splits[0]]\n",
    "train_masks = masks[0:splits[0]]\n",
    "train_labels = labels[0:splits[0]]\n",
    "\n",
    "def create_easy_train_dataloader(model, N, K, epoch):\n",
    "    easy_timeseries, easy_masks, easy_labels = pick_easy_tasks(model, train_features, train_masks, train_labels, N, K, epoch)\n",
    "    easy_train_dataset = TensorDataset(easy_timeseries, easy_masks, easy_labels)\n",
    "    if len(easy_train_dataset) == 0:\n",
    "        return None, len(train_features)\n",
    "    return DataLoader(easy_train_dataset, batch_size=32, shuffle=True), len(train_features)\n",
    "\n",
    "test_dataset = TensorDataset(features[splits[0]:splits[1]], masks[splits[0]:splits[1]], labels[splits[0]:splits[1]])\n",
    "validation_dataset = TensorDataset(features[splits[1]:], masks[splits[1]:], labels[splits[1]:])\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=len(validation_dataset), shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_features)} ICU stays with 48-hour timeseries for {features.size()[2]} measurements each\")\n",
    "print(f\"Test dataset size: {len(test_dataset)} ICU stays with 48-hour timeseries for {features.size()[2]} measurements each\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)} ICU stays with 48-hour timeseries for {features.size()[2]} measurements each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aacdfe67-c25f-4bf9-9f0f-6b6137173db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F1 score constant 1: 0.1506524317912218\n",
      "Baseline F1 score uniform: 0.15384615384615385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "constant_classifier = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "uniform_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "inputs, labels = None, None\n",
    "for inp, mask, lab in validation_dataloader:\n",
    "    inputs, labels = inp, lab\n",
    "    labels = labels.clone().cpu().detach()\n",
    "\n",
    "constant_classifier.fit([0] * len(labels), labels)\n",
    "uniform_classifier.fit([0] * len(labels), labels)\n",
    "y_constant = constant_classifier.predict([0] * len(labels))\n",
    "y_uniform = uniform_classifier.predict([0] * len(labels))\n",
    "\n",
    "print(f\"Baseline F1 score constant 1: {f1_score(labels, y_constant)}\")\n",
    "print(f\"Baseline F1 score uniform: {f1_score(labels, y_uniform)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd26fc87-1982-4e16-9f41-142e0c57d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.5\n",
    "N = 16\n",
    "LAMBDA = 1.3\n",
    "LR = 0.001\n",
    "OMEGA = 0.001\n",
    "K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b48d7e17-b9bb-41f0-b4f4-73e8dda0a38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (rnn): GRU(104, 32, batch_first=True)\n",
      "  (fc1): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=104, hidden_size=32, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        try:\n",
    "            x = x * mask\n",
    "        except:\n",
    "            print(x, mask)\n",
    "        _, h_n = self.rnn(x)\n",
    "        fc1_out = self.fc1(torch.squeeze(h_n))\n",
    "        # Multiply prediction value before sigmoid activation by gamma.\n",
    "        # This is part of the micro framework.\n",
    "        r = torch.mul(fc1_out, GAMMA)\n",
    "        res = torch.sigmoid(r)\n",
    "        return torch.squeeze(res)\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "tinymodel.to(device)\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae80b486-c86a-41e7-947c-87afbf624ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(tinymodel.parameters(), LR)\n",
    "\n",
    "def loss_func(y_hat, y, model):\n",
    "    # Add a dimension when there is only one item in the batch.     \n",
    "    if len(y_hat.size()) == 0:\n",
    "        y_hat = torch.unsqueeze(y_hat, dim=0)\n",
    "\n",
    "    l1 = OMEGA * sum(p.abs().sum() for p in model.parameters())\n",
    "    cr = (1 / GAMMA) * criterion(y_hat, y)\n",
    "    \n",
    "    # Calculate the loss. The regularization is just an l1 regularization with a coefficient.\n",
    "    # The criterion is the binary cross-entropy multiplied by 1 / GAMMA, a part of the micro framework.\n",
    "    return l1 + cr\n",
    "\n",
    "    # The paper also says to subtract batch_size / N, but this value is different for different batches depending\n",
    "    # on the number of easy tasks in the batch, making it hard to compare loss across batches. Crucially, this \n",
    "    # is confusing when you look at the loss on training data (non-full batches) vs loss on testing data (full-size batches).\n",
    "    # This makes me think that actually the paper meant that you first pick easy tasks across the entire testing set,\n",
    "    # and then split it into batches rather than split into batches first and pick easy tasks after.\n",
    "#     batch_size = y_hat.size()[0]    \n",
    "#     neg = batch_size / N\n",
    "#     return l1 + cr - neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f1ea2c4-ff8c-4aec-a221-2a32c9f9b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices picked: 12473, K: 1, epoch: 0\n",
      "Epoch: [1, training batch    50] running training loss: 2.190\n",
      "Epoch: [1, training batch   100] running training loss: 1.201\n",
      "Epoch: [1, training batch   150] running training loss: 0.847\n",
      "Epoch: [1, training batch   200] running training loss: 0.648\n",
      "Epoch: [1, training batch   250] running training loss: 0.630\n",
      "Epoch: [1, training batch   300] running training loss: 0.562\n",
      "Epoch: [1, training batch   350] running training loss: 0.610\n",
      "In epoch 1, number of examples trained on is 12473 out of 12473\n",
      "Test loss: 0.6259477734565735\n",
      "N is now 16\n",
      "\n",
      "\n",
      "indices picked: 0, K: 1, epoch: 1\n",
      "In epoch 2, number of examples trained on is 0 out of 12473\n",
      "Test loss: 0.6259477734565735\n",
      "N is now 12.307692307692307\n",
      "\n",
      "\n",
      "indices picked: 129, K: 1, epoch: 2\n",
      "In epoch 3, number of examples trained on is 129 out of 12473\n",
      "Test loss: 0.6259892582893372\n",
      "N is now 9.467455621301774\n",
      "\n",
      "\n",
      "indices picked: 2231, K: 1, epoch: 3\n",
      "Epoch: [4, training batch    50] running training loss: 0.190\n",
      "In epoch 4, number of examples trained on is 2231 out of 12473\n",
      "Test loss: 0.6885420083999634\n",
      "N is now 7.282658170232134\n",
      "\n",
      "\n",
      "indices picked: 11058, K: 1, epoch: 4\n",
      "Epoch: [5, training batch    50] running training loss: 0.166\n",
      "Epoch: [5, training batch   100] running training loss: 0.152\n",
      "Epoch: [5, training batch   150] running training loss: 0.143\n",
      "Epoch: [5, training batch   200] running training loss: 0.136\n",
      "Epoch: [5, training batch   250] running training loss: 0.131\n",
      "Epoch: [5, training batch   300] running training loss: 0.131\n",
      "In epoch 5, number of examples trained on is 11058 out of 12473\n",
      "Test loss: 0.8264933824539185\n",
      "N is now 5.60204474633241\n",
      "\n",
      "\n",
      "indices picked: 11389, K: 1, epoch: 5\n",
      "Epoch: [6, training batch    50] running training loss: 0.121\n",
      "Epoch: [6, training batch   100] running training loss: 0.122\n",
      "Epoch: [6, training batch   150] running training loss: 0.124\n",
      "Epoch: [6, training batch   200] running training loss: 0.132\n",
      "Epoch: [6, training batch   250] running training loss: 0.119\n",
      "Epoch: [6, training batch   300] running training loss: 0.116\n",
      "Epoch: [6, training batch   350] running training loss: 0.115\n",
      "In epoch 6, number of examples trained on is 11389 out of 12473\n",
      "Test loss: 0.8655766844749451\n",
      "N is now 4.309265189486469\n",
      "\n",
      "\n",
      "indices picked: 11430, K: 1, epoch: 6\n",
      "Epoch: [7, training batch    50] running training loss: 0.112\n",
      "Epoch: [7, training batch   100] running training loss: 0.112\n",
      "Epoch: [7, training batch   150] running training loss: 0.112\n",
      "Epoch: [7, training batch   200] running training loss: 0.109\n",
      "Epoch: [7, training batch   250] running training loss: 0.104\n",
      "Epoch: [7, training batch   300] running training loss: 0.102\n",
      "Epoch: [7, training batch   350] running training loss: 0.114\n",
      "In epoch 7, number of examples trained on is 11430 out of 12473\n",
      "Test loss: 0.9312757253646851\n",
      "N is now 3.314819376528053\n",
      "\n",
      "\n",
      "indices picked: 11447, K: 1, epoch: 7\n",
      "Epoch: [8, training batch    50] running training loss: 0.101\n",
      "Epoch: [8, training batch   100] running training loss: 0.101\n",
      "Epoch: [8, training batch   150] running training loss: 0.101\n",
      "Epoch: [8, training batch   200] running training loss: 0.109\n",
      "Epoch: [8, training batch   250] running training loss: 0.099\n",
      "Epoch: [8, training batch   300] running training loss: 0.095\n",
      "Epoch: [8, training batch   350] running training loss: 0.098\n",
      "In epoch 8, number of examples trained on is 11447 out of 12473\n",
      "Test loss: 0.9257230758666992\n",
      "N is now 2.5498610588677333\n",
      "\n",
      "\n",
      "indices picked: 11449, K: 1, epoch: 8\n",
      "Epoch: [9, training batch    50] running training loss: 0.093\n",
      "Epoch: [9, training batch   100] running training loss: 0.094\n",
      "Epoch: [9, training batch   150] running training loss: 0.095\n",
      "Epoch: [9, training batch   200] running training loss: 0.097\n",
      "Epoch: [9, training batch   250] running training loss: 0.100\n",
      "Epoch: [9, training batch   300] running training loss: 0.090\n",
      "Epoch: [9, training batch   350] running training loss: 0.090\n",
      "In epoch 9, number of examples trained on is 11449 out of 12473\n",
      "Test loss: 0.9079006910324097\n",
      "N is now 1.96143158374441\n",
      "\n",
      "\n",
      "indices picked: 11470, K: 1, epoch: 9\n",
      "Epoch: [10, training batch    50] running training loss: 0.091\n",
      "Epoch: [10, training batch   100] running training loss: 0.091\n",
      "Epoch: [10, training batch   150] running training loss: 0.088\n",
      "Epoch: [10, training batch   200] running training loss: 0.087\n",
      "Epoch: [10, training batch   250] running training loss: 0.087\n",
      "Epoch: [10, training batch   300] running training loss: 0.088\n",
      "Epoch: [10, training batch   350] running training loss: 0.086\n",
      "In epoch 10, number of examples trained on is 11470 out of 12473\n",
      "Test loss: 0.9486279487609863\n",
      "N is now 1.5087935259572385\n",
      "\n",
      "\n",
      "indices picked: 11486, K: 1, epoch: 10\n",
      "Epoch: [11, training batch    50] running training loss: 0.085\n",
      "Epoch: [11, training batch   100] running training loss: 0.085\n",
      "Epoch: [11, training batch   150] running training loss: 0.095\n",
      "Epoch: [11, training batch   200] running training loss: 0.103\n",
      "Epoch: [11, training batch   250] running training loss: 0.086\n",
      "Epoch: [11, training batch   300] running training loss: 0.085\n",
      "Epoch: [11, training batch   350] running training loss: 0.086\n",
      "In epoch 11, number of examples trained on is 11486 out of 12473\n",
      "Test loss: 0.9609825015068054\n",
      "N is now 1.1606104045824912\n",
      "\n",
      "\n",
      "indices picked: 11457, K: 1, epoch: 11\n",
      "Epoch: [12, training batch    50] running training loss: 0.080\n",
      "Epoch: [12, training batch   100] running training loss: 0.077\n",
      "Epoch: [12, training batch   150] running training loss: 0.077\n",
      "Epoch: [12, training batch   200] running training loss: 0.078\n",
      "Epoch: [12, training batch   250] running training loss: 0.077\n",
      "Epoch: [12, training batch   300] running training loss: 0.078\n",
      "Epoch: [12, training batch   350] running training loss: 0.084\n",
      "In epoch 12, number of examples trained on is 11457 out of 12473\n",
      "Test loss: 0.9692420959472656\n",
      "N is now 0.892777234294224\n",
      "\n",
      "\n",
      "indices picked: 11489, K: 1, epoch: 12\n",
      "Epoch: [13, training batch    50] running training loss: 0.077\n",
      "Epoch: [13, training batch   100] running training loss: 0.077\n",
      "Epoch: [13, training batch   150] running training loss: 0.078\n",
      "Epoch: [13, training batch   200] running training loss: 0.088\n",
      "Epoch: [13, training batch   250] running training loss: 0.085\n",
      "Epoch: [13, training batch   300] running training loss: 0.080\n",
      "Epoch: [13, training batch   350] running training loss: 0.078\n",
      "In epoch 13, number of examples trained on is 11489 out of 12473\n",
      "Test loss: 0.9687762260437012\n",
      "N is now 0.6867517186878646\n",
      "\n",
      "\n",
      "indices picked: 11508, K: 1, epoch: 13\n",
      "Epoch: [14, training batch    50] running training loss: 0.081\n",
      "Epoch: [14, training batch   100] running training loss: 0.081\n",
      "Epoch: [14, training batch   150] running training loss: 0.081\n",
      "Epoch: [14, training batch   200] running training loss: 0.080\n",
      "Epoch: [14, training batch   250] running training loss: 0.074\n",
      "Epoch: [14, training batch   300] running training loss: 0.075\n",
      "Epoch: [14, training batch   350] running training loss: 0.071\n",
      "In epoch 14, number of examples trained on is 11508 out of 12473\n",
      "Test loss: 0.9889185428619385\n",
      "N is now 0.5282705528368189\n",
      "\n",
      "\n",
      "indices picked: 11516, K: 1, epoch: 14\n",
      "Epoch: [15, training batch    50] running training loss: 0.079\n",
      "Epoch: [15, training batch   100] running training loss: 0.088\n",
      "Epoch: [15, training batch   150] running training loss: 0.082\n",
      "Epoch: [15, training batch   200] running training loss: 0.079\n",
      "Epoch: [15, training batch   250] running training loss: 0.079\n",
      "Epoch: [15, training batch   300] running training loss: 0.078\n",
      "Epoch: [15, training batch   350] running training loss: 0.076\n",
      "In epoch 15, number of examples trained on is 11516 out of 12473\n",
      "Test loss: 0.9794362783432007\n",
      "N is now 0.4063619637206299\n",
      "\n",
      "\n",
      "indices picked: 11535, K: 1, epoch: 15\n",
      "Epoch: [16, training batch    50] running training loss: 0.076\n",
      "Epoch: [16, training batch   100] running training loss: 0.080\n",
      "Epoch: [16, training batch   150] running training loss: 0.074\n",
      "Epoch: [16, training batch   200] running training loss: 0.079\n",
      "Epoch: [16, training batch   250] running training loss: 0.092\n",
      "Epoch: [16, training batch   300] running training loss: 0.080\n",
      "Epoch: [16, training batch   350] running training loss: 0.072\n",
      "In epoch 16, number of examples trained on is 11535 out of 12473\n",
      "Test loss: 0.907896876335144\n",
      "N is now 0.312586125938946\n",
      "\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement early stopping\n",
    "for epoch in range(16):\n",
    "    running_loss = 0.0\n",
    "    number_examples_used = 0\n",
    "    dataloader, max_num_examples = create_easy_train_dataloader(tinymodel, N, K, epoch)\n",
    "    \n",
    "    if dataloader is not None:\n",
    "        for i, (features, masks, labels) in enumerate(dataloader):\n",
    "            tinymodel.train(True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Train on easy sequences.\n",
    "            output = tinymodel(features, masks)\n",
    "            loss = loss_func(output, labels, tinymodel)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            number_examples_used += len(features)\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f'Epoch: [{epoch + 1}, training batch {i + 1:5d}] running training loss: {running_loss / 50:.3f}')\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    running_test_loss = 0.0\n",
    "    tinymodel.train(False)\n",
    "    print(f\"In epoch {epoch + 1}, number of examples trained on is {number_examples_used} out of {max_num_examples}\")\n",
    "    for i, (test_inputs, test_masks, test_labels) in enumerate(test_dataloader):\n",
    "        test_outputs = tinymodel(test_inputs, test_masks)\n",
    "        test_loss = loss_func(test_outputs, test_labels, tinymodel)\n",
    "        running_test_loss += test_loss\n",
    "    \n",
    "    print(f\"Test loss: {running_test_loss / len(test_dataloader)}\")\n",
    "    # Once we have passed K warm-up epochs, decrease N by a factor of lambd\n",
    "    # and as such increase 1 / N, so increase the loss threshold that defines\n",
    "    # what tasks are considrered easy.\n",
    "    if epoch >= K:\n",
    "        N = N / LAMBDA\n",
    "    print(f\"N is now {N}\\n\\n\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6592c29c-748d-4424-8949-8fb5c0df3a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.11594202898550723\n",
      "ROC AUC: 0.5304485769586064\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "tinymodel.train(False)\n",
    "out = None\n",
    "lab = None\n",
    "for inputs, masks, labels in validation_dataloader:\n",
    "    outputs = tinymodel(inputs, masks)\n",
    "    out = outputs\n",
    "    outputs = outputs > 0.5\n",
    "    lab = labels\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    print(f\"F1 score: {f1_score(labels, outputs)}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(labels, outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b51e6675-004d-44e8-9554-4a838774891b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the metric-coverage plot: 0.05631287585738518\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "# Fill with real values\n",
    "preds = out.cpu().detach().numpy()\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "coverages = []\n",
    "calculated_metrics = []\n",
    "    \n",
    "criterion = nn.BCELoss(reduction='none')\n",
    "loss = (1 / GAMMA) * criterion(out, lab)\n",
    "ordered_indices = torch.argsort(loss)\n",
    "ordered_outputs = 1.0 * (torch.index_select(out, 0, ordered_indices) > 0.5)\n",
    "ordered_labels = torch.index_select(lab, 0, ordered_indices)\n",
    "\n",
    "AUC = 0\n",
    "\n",
    "for coverage in np.arange(0.92, 1.01, 0.01):\n",
    "    length = len(ordered_outputs)\n",
    "    cutoff_index = int(length * coverage) + 1\n",
    "    \n",
    "    included_outputs = ordered_outputs[0:cutoff_index]\n",
    "    included_labels = ordered_labels[0:cutoff_index]\n",
    "    \n",
    "    metric = metrics.roc_auc_score(included_labels, included_outputs)\n",
    "    calculated_metrics.append(metric)\n",
    "    coverages.append(coverage)\n",
    "    \n",
    "    AUC += metric * 0.01\n",
    "    \n",
    "plt.plot(coverages, calculated_metrics)\n",
    "plt.ylim(0, 1) \n",
    "plt.show()\n",
    "print(f\"Area under the metric-coverage plot: {AUC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bca14-de0c-4abc-ae57-951a28e8ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coverages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61cfcea-38cd-430b-b2f4-dcafd2d95176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mimic-extract-2]",
   "language": "python",
   "name": "conda-env-.conda-mimic-extract-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
