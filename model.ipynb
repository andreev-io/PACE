{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7635ebcf-72d0-4401-a8f1-30c209378284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63172a0b-e8d6-4334-a41d-7d49155b9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icu  measurements  preprocessed\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80cac9b9-7b0a-41b1-8216-b0e4e852d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/preprocessed\"        \n",
    "\n",
    "class MortalityDataset(Dataset):    \n",
    "    def __init__(self):\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        \n",
    "        sequences = []\n",
    "        directory = os.fsencode(f\"{PATH}\")\n",
    "        fs = os.listdir(directory)\n",
    "        for idx, file in enumerate(fs):\n",
    "            filename = os.fsdecode(file)\n",
    "            if not filename.endswith(\".csv\"): \n",
    "                continue\n",
    "                \n",
    "            df = pd.read_csv(f\"{PATH}/{filename}\")\n",
    "            y_true = df[\"y_true\"][0]\n",
    "            del df[\"y_true\"]\n",
    "            \n",
    "            data = torch.from_numpy(df.values)\n",
    "            self.sequences.append(data)\n",
    "            self.labels.append(y_true)\n",
    "            \n",
    "            if idx % 500 == 0:\n",
    "                print(f\"loaded {idx+1} out of {len(fs)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c185c05-b215-48be-9c62-c845016f4fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1 out of 20030\n",
      "loaded 501 out of 20030\n",
      "loaded 1001 out of 20030\n",
      "loaded 1501 out of 20030\n",
      "loaded 2001 out of 20030\n",
      "loaded 2501 out of 20030\n",
      "loaded 3001 out of 20030\n",
      "loaded 3501 out of 20030\n",
      "loaded 4001 out of 20030\n",
      "loaded 4501 out of 20030\n",
      "loaded 5001 out of 20030\n",
      "loaded 5501 out of 20030\n",
      "loaded 6001 out of 20030\n",
      "loaded 6501 out of 20030\n",
      "loaded 7001 out of 20030\n",
      "loaded 7501 out of 20030\n",
      "loaded 8001 out of 20030\n",
      "loaded 8501 out of 20030\n",
      "loaded 9001 out of 20030\n",
      "loaded 9501 out of 20030\n",
      "loaded 10001 out of 20030\n",
      "loaded 10501 out of 20030\n",
      "loaded 11001 out of 20030\n",
      "loaded 11501 out of 20030\n",
      "loaded 12001 out of 20030\n",
      "loaded 12501 out of 20030\n",
      "loaded 13001 out of 20030\n",
      "loaded 13501 out of 20030\n",
      "loaded 14001 out of 20030\n",
      "loaded 14501 out of 20030\n",
      "loaded 15001 out of 20030\n",
      "loaded 15501 out of 20030\n",
      "loaded 16001 out of 20030\n",
      "loaded 16501 out of 20030\n",
      "loaded 17001 out of 20030\n",
      "loaded 17501 out of 20030\n",
      "loaded 18001 out of 20030\n",
      "loaded 18501 out of 20030\n",
      "loaded 19001 out of 20030\n",
      "loaded 19501 out of 20030\n",
      "loaded 20001 out of 20030\n",
      "(tensor([[46,  0,  0,  ...,  0, 65,  0],\n",
      "        [46,  0,  0,  ...,  0, 65,  0],\n",
      "        [46,  0,  0,  ...,  0, 65,  0],\n",
      "        ...,\n",
      "        [46,  0,  0,  ..., 37, 65,  0],\n",
      "        [46,  0,  0,  ...,  0, 65,  0],\n",
      "        [46,  0,  0,  ...,  0, 65,  0]]), 0)\n",
      "(tensor([[59,  0,  0,  ...,  0, 89,  0],\n",
      "        [59,  0,  0,  ...,  0, 89,  0],\n",
      "        [59,  0,  0,  ...,  0, 89,  0],\n",
      "        ...,\n",
      "        [59,  0,  0,  ...,  0, 89,  0],\n",
      "        [59,  0,  0,  ...,  0, 89,  0],\n",
      "        [59,  0,  0,  ...,  0, 89,  0]]), 0)\n"
     ]
    }
   ],
   "source": [
    "m = MortalityDataset()\n",
    "print(m[0])\n",
    "print(m[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c0c971-ab20-4857-8684-86ea2e952c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1bcfb8b2-423c-4615-8df5-1dd6859d083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "def create_batch(inp):\n",
    "    features = [i[0].type(torch.float).to(device) for i in inp]\n",
    "    labels = [i[1] for i in inp]\n",
    "    labels = torch.tensor(labels, dtype=torch.float).to(device)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "13a4048b-a691-4ea6-8bfd-8507961d33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "train_data, test_data, validation_data = random_split(m, [int(0.8 * len(m)), int(0.1 * len(m)), int(0.1 * len(m))], torch.Generator())\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=create_batch, num_workers=0)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True, collate_fn=create_batch, num_workers=0)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=len(validation_data), shuffle=True, collate_fn=create_batch, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "aacdfe67-c25f-4bf9-9f0f-6b6137173db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F1 score constant 1: 0.19630796938316075\n",
      "Baseline F1 score uniform: 0.17905675459632295\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "constant_classifier = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "uniform_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "inputs, labels = None, None\n",
    "for inp, lab in validation_dataloader:\n",
    "    inputs, labels = inp, lab\n",
    "    labels = labels.clone().cpu().detach()\n",
    "\n",
    "constant_classifier.fit([0] * len(labels), labels)\n",
    "uniform_classifier.fit([0] * len(labels), labels)\n",
    "y_constant = constant_classifier.predict([0] * len(labels))\n",
    "y_uniform = uniform_classifier.predict([0] * len(labels))\n",
    "\n",
    "print(f\"Baseline F1 score constant 1: {f1_score(labels, y_constant)}\")\n",
    "print(f\"Baseline F1 score uniform: {f1_score(labels, y_uniform)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fd26fc87-1982-4e16-9f41-142e0c57d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.5\n",
    "N = 16\n",
    "LAMBDA = 1.3\n",
    "LR = 0.001\n",
    "OMEGA = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b48d7e17-b9bb-41f0-b4f4-73e8dda0a38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (rnn): GRU(149, 32)\n",
      "  (fc1): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=149, hidden_size=32, num_layers=1, bidirectional=False)\n",
    "        self.fc1 = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, h_n = self.rnn(x)\n",
    "        fc1_out = self.fc1(torch.squeeze(h_n))\n",
    "        # Multiply prediction value before sigmoid activation by gamma.\n",
    "        # This is part of the micro framework.\n",
    "        r = torch.mul(fc1_out, GAMMA)\n",
    "        res = torch.sigmoid(r)\n",
    "        return torch.squeeze(res)\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "tinymodel.to(device)\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ae80b486-c86a-41e7-947c-87afbf624ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(tinymodel.parameters(), LR)\n",
    "\n",
    "def loss_func(y_hat, y, model):\n",
    "    # Add a dimension when there is only one item in the batch.     \n",
    "    if len(y_hat.size()) == 0:\n",
    "        y_hat = torch.unsqueeze(y_hat, dim=0)\n",
    "\n",
    "    l1 = OMEGA * sum(p.abs().sum() for p in model.parameters())\n",
    "    cr = (1 / GAMMA) * criterion(y_hat, y)\n",
    "    \n",
    "    # Calculate the loss. The regularization is just an l1 regularization with a coefficient.\n",
    "    # The criterion is the binary cross-entropy multiplied by 1 / GAMMA, a part of the micro framework.\n",
    "    return l1 + cr\n",
    "\n",
    "    # The paper also says to subtract batch_size / N, but this value is different for different batches depending\n",
    "    # on the number of easy tasks in the batch, making it hard to compare loss across batches. Crucially, this \n",
    "    # is confusing when you look at the loss on training data (non-full batches) vs loss on testing data (full-size batches).\n",
    "    # This makes me think that actually the paper meant that you first pick easy tasks across the entire testing set,\n",
    "    # and then split it into batches rather than split into batches first and pick easy tasks after.\n",
    "    # batch_size = y_hat.size()[0]    \n",
    "    # neg = batch_size / N\n",
    "    # return l1 + cr - neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e21b5d56-f275-41e9-8f83-721a35783f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_easy_tasks(model, x, y, N):\n",
    "    criterion = nn.BCELoss(reduction='none')\n",
    "    model.train(False)\n",
    "    y_hat = model(pack_sequence(x, enforce_sorted=False))\n",
    "\n",
    "    # Pick tasks in the batch for which the loss is less than 1 / N.\n",
    "    loss = torch.add((1 / GAMMA) * criterion(y_hat, y), -1 / N)\n",
    "    easy_indices = list((loss < 0).nonzero())\n",
    "\n",
    "    easy_sequences = []\n",
    "    easy_labels = []\n",
    "    for index in easy_indices:\n",
    "        index = index.item()\n",
    "        easy_sequences.append(x[index])\n",
    "        easy_labels.append(labels[index])\n",
    "\n",
    "    model.train(True)\n",
    "    return easy_sequences, torch.Tensor(easy_labels).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3f1ea2c4-ff8c-4aec-a221-2a32c9f9b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 1, number of examples trained on is 0 out of 16024\n",
      "Test loss: 2.9737424850463867\n",
      "In epoch 2, number of examples trained on is 0 out of 16024\n",
      "Test loss: 2.972846269607544\n",
      "In epoch 3, number of examples trained on is 0 out of 16024\n",
      "Test loss: 2.9738006591796875\n",
      "In epoch 4, number of examples trained on is 0 out of 16024\n",
      "Test loss: 2.9738314151763916\n",
      "In epoch 5, number of examples trained on is 0 out of 16024\n",
      "Test loss: 2.9733190536499023\n",
      "In epoch 6, number of examples trained on is 0 out of 16024\n",
      "Test loss: 2.9731040000915527\n",
      "In epoch 7, number of examples trained on is 0 out of 16024\n",
      "Test loss: 2.9730639457702637\n",
      "In epoch 8, number of examples trained on is 0 out of 16024\n",
      "Test loss: 2.9738051891326904\n",
      "In epoch 9, number of examples trained on is 0 out of 16024\n",
      "Test loss: 2.973593235015869\n",
      "In epoch 10, number of examples trained on is 0 out of 16024\n",
      "Test loss: 2.9729905128479004\n",
      "Epoch: [11, training sample   200] running training loss: 0.888\n",
      "Epoch: [11, training sample   250] running training loss: 1.243\n",
      "Epoch: [11, training sample   300] running training loss: 1.034\n",
      "Epoch: [11, training sample   350] running training loss: 0.460\n",
      "Epoch: [11, training sample   400] running training loss: 0.250\n",
      "Epoch: [11, training sample   450] running training loss: 0.183\n",
      "Epoch: [11, training sample   500] running training loss: 0.146\n",
      "In epoch 11, number of examples trained on is 1130 out of 16024\n",
      "Test loss: 5.878747940063477\n",
      "Epoch: [12, training sample    50] running training loss: 0.126\n",
      "Epoch: [12, training sample   100] running training loss: 0.111\n",
      "Epoch: [12, training sample   150] running training loss: 0.095\n",
      "Epoch: [12, training sample   200] running training loss: 0.090\n",
      "Epoch: [12, training sample   250] running training loss: 0.082\n",
      "Epoch: [12, training sample   300] running training loss: 0.074\n",
      "Epoch: [12, training sample   350] running training loss: 0.068\n",
      "Epoch: [12, training sample   400] running training loss: 0.067\n",
      "Epoch: [12, training sample   450] running training loss: 0.063\n",
      "Epoch: [12, training sample   500] running training loss: 0.058\n",
      "In epoch 12, number of examples trained on is 2066 out of 16024\n",
      "Test loss: 8.161431312561035\n",
      "Epoch: [13, training sample    50] running training loss: 0.056\n",
      "Epoch: [13, training sample   100] running training loss: 0.054\n",
      "Epoch: [13, training sample   150] running training loss: 0.052\n",
      "Epoch: [13, training sample   200] running training loss: 0.048\n",
      "Epoch: [13, training sample   250] running training loss: 0.048\n",
      "Epoch: [13, training sample   300] running training loss: 0.046\n",
      "Epoch: [13, training sample   350] running training loss: 0.046\n",
      "Epoch: [13, training sample   400] running training loss: 0.044\n",
      "Epoch: [13, training sample   450] running training loss: 0.040\n",
      "Epoch: [13, training sample   500] running training loss: 0.042\n",
      "In epoch 13, number of examples trained on is 2066 out of 16024\n",
      "Test loss: 9.384451866149902\n",
      "Epoch: [14, training sample    50] running training loss: 0.040\n",
      "Epoch: [14, training sample   100] running training loss: 0.040\n",
      "Epoch: [14, training sample   150] running training loss: 0.039\n",
      "Epoch: [14, training sample   200] running training loss: 0.038\n",
      "Epoch: [14, training sample   250] running training loss: 0.036\n",
      "Epoch: [14, training sample   300] running training loss: 0.035\n",
      "Epoch: [14, training sample   350] running training loss: 0.036\n",
      "Epoch: [14, training sample   400] running training loss: 0.035\n",
      "Epoch: [14, training sample   450] running training loss: 0.034\n",
      "Epoch: [14, training sample   500] running training loss: 0.033\n",
      "In epoch 14, number of examples trained on is 2066 out of 16024\n",
      "Test loss: 10.231759071350098\n",
      "Epoch: [15, training sample    50] running training loss: 0.032\n",
      "Epoch: [15, training sample   100] running training loss: 0.032\n",
      "Epoch: [15, training sample   150] running training loss: 0.031\n",
      "Epoch: [15, training sample   200] running training loss: 0.031\n",
      "Epoch: [15, training sample   250] running training loss: 0.030\n",
      "Epoch: [15, training sample   300] running training loss: 0.030\n",
      "Epoch: [15, training sample   350] running training loss: 0.029\n",
      "Epoch: [15, training sample   400] running training loss: 0.028\n",
      "Epoch: [15, training sample   450] running training loss: 0.028\n",
      "Epoch: [15, training sample   500] running training loss: 0.027\n",
      "In epoch 15, number of examples trained on is 2066 out of 16024\n",
      "Test loss: 10.7846097946167\n",
      "Epoch: [16, training sample    50] running training loss: 0.028\n",
      "Epoch: [16, training sample   100] running training loss: 0.027\n",
      "Epoch: [16, training sample   150] running training loss: 0.027\n",
      "Epoch: [16, training sample   200] running training loss: 0.026\n",
      "Epoch: [16, training sample   250] running training loss: 0.025\n",
      "Epoch: [16, training sample   300] running training loss: 0.025\n",
      "Epoch: [16, training sample   350] running training loss: 0.026\n",
      "Epoch: [16, training sample   400] running training loss: 0.025\n",
      "Epoch: [16, training sample   450] running training loss: 0.025\n",
      "Epoch: [16, training sample   500] running training loss: 0.025\n",
      "In epoch 16, number of examples trained on is 2066 out of 16024\n",
      "Test loss: 11.156163215637207\n",
      "Epoch: [17, training sample    50] running training loss: 0.024\n",
      "Epoch: [17, training sample   100] running training loss: 0.024\n",
      "Epoch: [17, training sample   150] running training loss: 0.024\n",
      "Epoch: [17, training sample   200] running training loss: 0.024\n",
      "Epoch: [17, training sample   250] running training loss: 0.022\n",
      "Epoch: [17, training sample   300] running training loss: 0.024\n",
      "Epoch: [17, training sample   350] running training loss: 0.022\n",
      "Epoch: [17, training sample   400] running training loss: 0.023\n",
      "Epoch: [17, training sample   450] running training loss: 0.023\n",
      "Epoch: [17, training sample   500] running training loss: 0.022\n",
      "In epoch 17, number of examples trained on is 2066 out of 16024\n",
      "Test loss: 11.496306419372559\n",
      "Epoch: [18, training sample    50] running training loss: 0.023\n",
      "Epoch: [18, training sample   100] running training loss: 0.024\n",
      "Epoch: [18, training sample   150] running training loss: 0.023\n",
      "Epoch: [18, training sample   200] running training loss: 0.022\n",
      "Epoch: [18, training sample   250] running training loss: 0.023\n",
      "Epoch: [18, training sample   300] running training loss: 0.022\n",
      "Epoch: [18, training sample   350] running training loss: 0.022\n",
      "Epoch: [18, training sample   400] running training loss: 0.022\n",
      "Epoch: [18, training sample   450] running training loss: 0.021\n",
      "Epoch: [18, training sample   500] running training loss: 0.022\n",
      "In epoch 18, number of examples trained on is 2066 out of 16024\n",
      "Test loss: 11.670353889465332\n",
      "Epoch: [19, training sample   100] running training loss: 0.042\n",
      "Epoch: [19, training sample   150] running training loss: 0.021\n",
      "Epoch: [19, training sample   200] running training loss: 0.020\n",
      "Epoch: [19, training sample   250] running training loss: 0.021\n",
      "Epoch: [19, training sample   300] running training loss: 0.021\n",
      "Epoch: [19, training sample   350] running training loss: 0.021\n",
      "Epoch: [19, training sample   400] running training loss: 0.021\n",
      "Epoch: [19, training sample   450] running training loss: 0.021\n",
      "Epoch: [19, training sample   500] running training loss: 0.020\n",
      "In epoch 19, number of examples trained on is 2066 out of 16024\n",
      "Test loss: 11.900056838989258\n",
      "Epoch: [20, training sample    50] running training loss: 0.021\n",
      "Epoch: [20, training sample   100] running training loss: 0.020\n",
      "Epoch: [20, training sample   150] running training loss: 0.020\n",
      "Epoch: [20, training sample   200] running training loss: 0.020\n",
      "Epoch: [20, training sample   250] running training loss: 0.020\n",
      "Epoch: [20, training sample   300] running training loss: 0.020\n",
      "Epoch: [20, training sample   350] running training loss: 0.146\n",
      "Epoch: [20, training sample   400] running training loss: 0.066\n",
      "Epoch: [20, training sample   450] running training loss: 0.051\n",
      "Epoch: [20, training sample   500] running training loss: 0.044\n",
      "In epoch 20, number of examples trained on is 6129 out of 16024\n",
      "Test loss: 1.755301833152771\n",
      "Epoch: [21, training sample    50] running training loss: 0.797\n",
      "Epoch: [21, training sample   100] running training loss: 0.876\n",
      "Epoch: [21, training sample   150] running training loss: 0.804\n",
      "Epoch: [21, training sample   200] running training loss: 0.791\n",
      "Epoch: [21, training sample   250] running training loss: 0.815\n",
      "Epoch: [21, training sample   300] running training loss: 0.791\n",
      "Epoch: [21, training sample   350] running training loss: 0.774\n",
      "Epoch: [21, training sample   400] running training loss: 0.741\n",
      "Epoch: [21, training sample   450] running training loss: 0.737\n",
      "Epoch: [21, training sample   500] running training loss: 0.821\n",
      "In epoch 21, number of examples trained on is 15924 out of 16024\n",
      "Test loss: 0.808566689491272\n",
      "Epoch: [22, training sample    50] running training loss: 0.781\n",
      "Epoch: [22, training sample   100] running training loss: 0.794\n",
      "Epoch: [22, training sample   150] running training loss: 0.762\n",
      "Epoch: [22, training sample   200] running training loss: 0.782\n",
      "Epoch: [22, training sample   250] running training loss: 0.760\n",
      "Epoch: [22, training sample   300] running training loss: 0.786\n",
      "Epoch: [22, training sample   350] running training loss: 0.762\n",
      "Epoch: [22, training sample   400] running training loss: 0.794\n",
      "Epoch: [22, training sample   450] running training loss: 0.788\n",
      "Epoch: [22, training sample   500] running training loss: 0.816\n",
      "In epoch 22, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.8062981367111206\n",
      "Epoch: [23, training sample    50] running training loss: 0.770\n",
      "Epoch: [23, training sample   100] running training loss: 0.756\n",
      "Epoch: [23, training sample   150] running training loss: 0.753\n",
      "Epoch: [23, training sample   200] running training loss: 0.763\n",
      "Epoch: [23, training sample   250] running training loss: 0.829\n",
      "Epoch: [23, training sample   300] running training loss: 0.830\n",
      "Epoch: [23, training sample   350] running training loss: 0.790\n",
      "Epoch: [23, training sample   400] running training loss: 0.781\n",
      "Epoch: [23, training sample   450] running training loss: 0.729\n",
      "Epoch: [23, training sample   500] running training loss: 0.738\n",
      "In epoch 23, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.78782719373703\n",
      "Epoch: [24, training sample    50] running training loss: 0.784\n",
      "Epoch: [24, training sample   100] running training loss: 0.786\n",
      "Epoch: [24, training sample   150] running training loss: 0.775\n",
      "Epoch: [24, training sample   200] running training loss: 0.734\n",
      "Epoch: [24, training sample   250] running training loss: 0.752\n",
      "Epoch: [24, training sample   300] running training loss: 0.754\n",
      "Epoch: [24, training sample   350] running training loss: 0.752\n",
      "Epoch: [24, training sample   400] running training loss: 0.748\n",
      "Epoch: [24, training sample   450] running training loss: 0.789\n",
      "Epoch: [24, training sample   500] running training loss: 0.733\n",
      "In epoch 24, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7698634266853333\n",
      "Epoch: [25, training sample    50] running training loss: 0.764\n",
      "Epoch: [25, training sample   100] running training loss: 0.738\n",
      "Epoch: [25, training sample   150] running training loss: 0.768\n",
      "Epoch: [25, training sample   200] running training loss: 0.741\n",
      "Epoch: [25, training sample   250] running training loss: 0.739\n",
      "Epoch: [25, training sample   300] running training loss: 0.722\n",
      "Epoch: [25, training sample   350] running training loss: 0.730\n",
      "Epoch: [25, training sample   400] running training loss: 0.763\n",
      "Epoch: [25, training sample   450] running training loss: 0.716\n",
      "Epoch: [25, training sample   500] running training loss: 0.821\n",
      "In epoch 25, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7580658793449402\n",
      "Epoch: [26, training sample    50] running training loss: 0.699\n",
      "Epoch: [26, training sample   100] running training loss: 0.763\n",
      "Epoch: [26, training sample   150] running training loss: 0.762\n",
      "Epoch: [26, training sample   200] running training loss: 0.717\n",
      "Epoch: [26, training sample   250] running training loss: 0.748\n",
      "Epoch: [26, training sample   300] running training loss: 0.736\n",
      "Epoch: [26, training sample   350] running training loss: 0.686\n",
      "Epoch: [26, training sample   400] running training loss: 0.753\n",
      "Epoch: [26, training sample   450] running training loss: 0.748\n",
      "Epoch: [26, training sample   500] running training loss: 0.730\n",
      "In epoch 26, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7653437256813049\n",
      "Epoch: [27, training sample    50] running training loss: 0.735\n",
      "Epoch: [27, training sample   100] running training loss: 0.719\n",
      "Epoch: [27, training sample   150] running training loss: 0.703\n",
      "Epoch: [27, training sample   200] running training loss: 0.688\n",
      "Epoch: [27, training sample   250] running training loss: 0.729\n",
      "Epoch: [27, training sample   300] running training loss: 0.695\n",
      "Epoch: [27, training sample   350] running training loss: 0.757\n",
      "Epoch: [27, training sample   400] running training loss: 0.740\n",
      "Epoch: [27, training sample   450] running training loss: 0.703\n",
      "Epoch: [27, training sample   500] running training loss: 0.752\n",
      "In epoch 27, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7359301447868347\n",
      "Epoch: [28, training sample    50] running training loss: 0.719\n",
      "Epoch: [28, training sample   100] running training loss: 0.727\n",
      "Epoch: [28, training sample   150] running training loss: 0.708\n",
      "Epoch: [28, training sample   200] running training loss: 0.690\n",
      "Epoch: [28, training sample   250] running training loss: 0.688\n",
      "Epoch: [28, training sample   300] running training loss: 0.762\n",
      "Epoch: [28, training sample   350] running training loss: 0.702\n",
      "Epoch: [28, training sample   400] running training loss: 0.679\n",
      "Epoch: [28, training sample   450] running training loss: 0.717\n",
      "Epoch: [28, training sample   500] running training loss: 0.709\n",
      "In epoch 28, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7395157814025879\n",
      "Epoch: [29, training sample    50] running training loss: 0.717\n",
      "Epoch: [29, training sample   100] running training loss: 0.688\n",
      "Epoch: [29, training sample   150] running training loss: 0.759\n",
      "Epoch: [29, training sample   200] running training loss: 0.658\n",
      "Epoch: [29, training sample   250] running training loss: 0.655\n",
      "Epoch: [29, training sample   300] running training loss: 0.747\n",
      "Epoch: [29, training sample   350] running training loss: 0.740\n",
      "Epoch: [29, training sample   400] running training loss: 0.693\n",
      "Epoch: [29, training sample   450] running training loss: 0.684\n",
      "Epoch: [29, training sample   500] running training loss: 0.712\n",
      "In epoch 29, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7401266098022461\n",
      "Epoch: [30, training sample    50] running training loss: 0.716\n",
      "Epoch: [30, training sample   100] running training loss: 0.700\n",
      "Epoch: [30, training sample   150] running training loss: 0.667\n",
      "Epoch: [30, training sample   200] running training loss: 0.735\n",
      "Epoch: [30, training sample   250] running training loss: 0.706\n",
      "Epoch: [30, training sample   300] running training loss: 0.718\n",
      "Epoch: [30, training sample   350] running training loss: 0.711\n",
      "Epoch: [30, training sample   400] running training loss: 0.702\n",
      "Epoch: [30, training sample   450] running training loss: 0.688\n",
      "Epoch: [30, training sample   500] running training loss: 0.664\n",
      "In epoch 30, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7343428730964661\n",
      "Epoch: [31, training sample    50] running training loss: 0.673\n",
      "Epoch: [31, training sample   100] running training loss: 0.764\n",
      "Epoch: [31, training sample   150] running training loss: 0.688\n",
      "Epoch: [31, training sample   200] running training loss: 0.708\n",
      "Epoch: [31, training sample   250] running training loss: 0.714\n",
      "Epoch: [31, training sample   300] running training loss: 0.639\n",
      "Epoch: [31, training sample   350] running training loss: 0.710\n",
      "Epoch: [31, training sample   400] running training loss: 0.709\n",
      "Epoch: [31, training sample   450] running training loss: 0.661\n",
      "Epoch: [31, training sample   500] running training loss: 0.681\n",
      "In epoch 31, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7386974692344666\n",
      "Epoch: [32, training sample    50] running training loss: 0.764\n",
      "Epoch: [32, training sample   100] running training loss: 0.677\n",
      "Epoch: [32, training sample   150] running training loss: 0.713\n",
      "Epoch: [32, training sample   200] running training loss: 0.625\n",
      "Epoch: [32, training sample   250] running training loss: 0.695\n",
      "Epoch: [32, training sample   300] running training loss: 0.667\n",
      "Epoch: [32, training sample   350] running training loss: 0.735\n",
      "Epoch: [32, training sample   400] running training loss: 0.704\n",
      "Epoch: [32, training sample   450] running training loss: 0.724\n",
      "Epoch: [32, training sample   500] running training loss: 0.663\n",
      "In epoch 32, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7331192493438721\n",
      "Epoch: [33, training sample    50] running training loss: 0.702\n",
      "Epoch: [33, training sample   100] running training loss: 0.750\n",
      "Epoch: [33, training sample   150] running training loss: 0.669\n",
      "Epoch: [33, training sample   200] running training loss: 0.663\n",
      "Epoch: [33, training sample   250] running training loss: 0.688\n",
      "Epoch: [33, training sample   300] running training loss: 0.643\n",
      "Epoch: [33, training sample   350] running training loss: 0.704\n",
      "Epoch: [33, training sample   400] running training loss: 0.693\n",
      "Epoch: [33, training sample   450] running training loss: 0.699\n",
      "Epoch: [33, training sample   500] running training loss: 0.693\n",
      "In epoch 33, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.740917980670929\n",
      "Epoch: [34, training sample    50] running training loss: 0.662\n",
      "Epoch: [34, training sample   100] running training loss: 0.743\n",
      "Epoch: [34, training sample   150] running training loss: 0.666\n",
      "Epoch: [34, training sample   200] running training loss: 0.696\n",
      "Epoch: [34, training sample   250] running training loss: 0.661\n",
      "Epoch: [34, training sample   300] running training loss: 0.667\n",
      "Epoch: [34, training sample   350] running training loss: 0.677\n",
      "Epoch: [34, training sample   400] running training loss: 0.730\n",
      "Epoch: [34, training sample   450] running training loss: 0.705\n",
      "Epoch: [34, training sample   500] running training loss: 0.696\n",
      "In epoch 34, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7096958756446838\n",
      "Epoch: [35, training sample    50] running training loss: 0.673\n",
      "Epoch: [35, training sample   100] running training loss: 0.694\n",
      "Epoch: [35, training sample   150] running training loss: 0.709\n",
      "Epoch: [35, training sample   200] running training loss: 0.681\n",
      "Epoch: [35, training sample   250] running training loss: 0.711\n",
      "Epoch: [35, training sample   300] running training loss: 0.699\n",
      "Epoch: [35, training sample   350] running training loss: 0.657\n",
      "Epoch: [35, training sample   400] running training loss: 0.675\n",
      "Epoch: [35, training sample   450] running training loss: 0.671\n",
      "Epoch: [35, training sample   500] running training loss: 0.693\n",
      "In epoch 35, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7149452567100525\n",
      "Epoch: [36, training sample    50] running training loss: 0.719\n",
      "Epoch: [36, training sample   100] running training loss: 0.683\n",
      "Epoch: [36, training sample   150] running training loss: 0.689\n",
      "Epoch: [36, training sample   200] running training loss: 0.674\n",
      "Epoch: [36, training sample   250] running training loss: 0.612\n",
      "Epoch: [36, training sample   300] running training loss: 0.708\n",
      "Epoch: [36, training sample   350] running training loss: 0.699\n",
      "Epoch: [36, training sample   400] running training loss: 0.709\n",
      "Epoch: [36, training sample   450] running training loss: 0.692\n",
      "Epoch: [36, training sample   500] running training loss: 0.716\n",
      "In epoch 36, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.726809024810791\n",
      "Epoch: [37, training sample    50] running training loss: 0.681\n",
      "Epoch: [37, training sample   100] running training loss: 0.663\n",
      "Epoch: [37, training sample   150] running training loss: 0.652\n",
      "Epoch: [37, training sample   200] running training loss: 0.698\n",
      "Epoch: [37, training sample   250] running training loss: 0.679\n",
      "Epoch: [37, training sample   300] running training loss: 0.739\n",
      "Epoch: [37, training sample   350] running training loss: 0.734\n",
      "Epoch: [37, training sample   400] running training loss: 0.698\n",
      "Epoch: [37, training sample   450] running training loss: 0.715\n",
      "Epoch: [37, training sample   500] running training loss: 0.690\n",
      "In epoch 37, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7278273105621338\n",
      "Epoch: [38, training sample    50] running training loss: 0.679\n",
      "Epoch: [38, training sample   100] running training loss: 0.735\n",
      "Epoch: [38, training sample   150] running training loss: 0.672\n",
      "Epoch: [38, training sample   200] running training loss: 0.703\n",
      "Epoch: [38, training sample   250] running training loss: 0.669\n",
      "Epoch: [38, training sample   300] running training loss: 0.717\n",
      "Epoch: [38, training sample   350] running training loss: 0.661\n",
      "Epoch: [38, training sample   400] running training loss: 0.686\n",
      "Epoch: [38, training sample   450] running training loss: 0.645\n",
      "Epoch: [38, training sample   500] running training loss: 0.692\n",
      "In epoch 38, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7051414251327515\n",
      "Epoch: [39, training sample    50] running training loss: 0.655\n",
      "Epoch: [39, training sample   100] running training loss: 0.716\n",
      "Epoch: [39, training sample   150] running training loss: 0.647\n",
      "Epoch: [39, training sample   200] running training loss: 0.708\n",
      "Epoch: [39, training sample   250] running training loss: 0.666\n",
      "Epoch: [39, training sample   300] running training loss: 0.707\n",
      "Epoch: [39, training sample   350] running training loss: 0.706\n",
      "Epoch: [39, training sample   400] running training loss: 0.658\n",
      "Epoch: [39, training sample   450] running training loss: 0.632\n",
      "Epoch: [39, training sample   500] running training loss: 0.671\n",
      "In epoch 39, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.7036635279655457\n",
      "Epoch: [40, training sample    50] running training loss: 0.657\n",
      "Epoch: [40, training sample   100] running training loss: 0.717\n",
      "Epoch: [40, training sample   150] running training loss: 0.648\n",
      "Epoch: [40, training sample   200] running training loss: 0.675\n",
      "Epoch: [40, training sample   250] running training loss: 0.707\n",
      "Epoch: [40, training sample   300] running training loss: 0.701\n",
      "Epoch: [40, training sample   350] running training loss: 0.655\n",
      "Epoch: [40, training sample   400] running training loss: 0.602\n",
      "Epoch: [40, training sample   450] running training loss: 0.678\n",
      "Epoch: [40, training sample   500] running training loss: 0.650\n",
      "In epoch 40, number of examples trained on is 16024 out of 16024\n",
      "Test loss: 0.6846925020217896\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement early stopping\n",
    "for epoch in range(40):\n",
    "    running_loss = 0.0\n",
    "    number_examples_used = 0\n",
    "    num_examples = 0\n",
    "    for i, (list_of_sequences, labels) in enumerate(train_dataloader):\n",
    "        num_examples = num_examples + len(list_of_sequences)\n",
    "        tinymodel.train(True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pick easy sequences with the current N. If no sequences are easy, skip the batch.\n",
    "        easy_sequences, easy_labels = pick_easy_tasks(tinymodel, list_of_sequences, labels, N)\n",
    "        if len(easy_sequences) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            number_examples_used = number_examples_used + len(easy_sequences)\n",
    "\n",
    "        # Train on easy sequences.\n",
    "        packed_input = pack_sequence(easy_sequences, enforce_sorted=False)\n",
    "        easy_ouput = tinymodel(packed_input)\n",
    "        \n",
    "        loss = loss_func(easy_ouput, easy_labels, tinymodel)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'Epoch: [{epoch + 1}, training sample {i + 1:5d}] running training loss: {running_loss / 50:.3f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    running_vloss = 0.0\n",
    "    tinymodel.train(False)\n",
    "    print(f\"In epoch {epoch + 1}, number of examples trained on is {number_examples_used} out of {num_examples}\")\n",
    "    for i, (vinputs, vlabels) in enumerate(test_dataloader):\n",
    "        voutputs = tinymodel(pack_sequence(vinputs, enforce_sorted=False))\n",
    "        vloss = loss_func(voutputs, vlabels, tinymodel)\n",
    "        running_vloss += vloss\n",
    "    \n",
    "    print(f\"Test loss: {running_vloss / len(test_dataloader)}\")\n",
    "    # Decrease N by a factor of lambda and as such increase 1 / N, so increase the loss threshold that defines\n",
    "    # what tasks are considrered easy.\n",
    "    N = N / LAMBDA\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "6592c29c-748d-4424-8949-8fb5c0df3a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.2437275985663083\n",
      "ROC AUC: 0.5704186261660628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "tinymodel.train(False)\n",
    "for inputs, labels in validation_dataloader:\n",
    "    outputs = tinymodel(pack_sequence(inputs, enforce_sorted=False))\n",
    "    outputs = outputs > 0.5\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    print(f\"F1 score: {f1_score(labels, outputs)}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(labels, outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e6675-004d-44e8-9554-4a838774891b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61cfcea-38cd-430b-b2f4-dcafd2d95176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.5.1]",
   "language": "python",
   "name": "conda-env-opence-v1.5.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
