{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bfb9fe-a383-473b-9d49-6f9561ab932e",
   "metadata": {},
   "source": [
    "# PACE Model Replication Pipeline\n",
    "\n",
    "**Required packages:**\n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `pytorch`\n",
    "- `imbalanced-learn`\n",
    "- `scikit-learn`\n",
    "- `matplotlib`\n",
    "\n",
    "**Optional packages:**\n",
    "- `ipython`\n",
    "- `ipykernel`\n",
    "\n",
    "**Required python version:** 3.9.x\n",
    "We cannot use a newer version of python until pytorch adds support for it: https://github.com/pytorch/pytorch/issues/66424\n",
    "\n",
    "**Installation commands:**\n",
    "\n",
    "- `/path/to/conda create --name=dl4h-39 python=3.9.12`\n",
    "- `conda activate dl4h-39`\n",
    "- `conda install pandas pytables scipy numpy ipython ipykernel pytorch scikit-learn matplotlib`\n",
    "- `conda install -c conda-forge imbalanced-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb27d3c-0696-46c9-a4b5-6dbcb92a50df",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0279f1fb-953e-4b4e-8553-2d55baebc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.5\n",
    "N = 16\n",
    "LAMBDA = 1.3\n",
    "LR = 0.001\n",
    "OMEGA = 0.001\n",
    "K = 1\n",
    "TOLERANCE = 0.01\n",
    "PATIENCE = 2\n",
    "\n",
    "USE_MODIFIED_LOSS_FUNCTION = False\n",
    "USE_SELF_PACED_LEARNING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004fd1a-a7be-4d86-8f2d-e891dcb035f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79433514-990a-42ba-8197-3836049dbc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea30443d-dedd-44c1-ae88-f97e32e5b868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c60dd9-709f-4fa4-a956-e6f66902aef6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e192f873-41d5-4c2c-8cc4-672880a7e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_info(features, masks, labels):\n",
    "    print(f\"Shape of features: {features.size()}\")\n",
    "    print(f\"Shape of masks: {masks.size()}\")\n",
    "    print(f\"Shape of labels: {labels.size()}\")\n",
    "    \n",
    "    print(f\"Labels (format: [(label, count)]: {list(zip(*torch.unique(labels, return_counts = True)))}\")\n",
    "\n",
    "    assert len(features) == len(labels)\n",
    "    assert len(masks) == len(labels)\n",
    "    assert features.size() == masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f80d9d4c-33ac-46f3-ae23-9dddc2e4c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features: torch.Size([15591, 48, 104])\n",
      "Shape of masks: torch.Size([15591, 48, 104])\n",
      "Shape of labels: torch.Size([15591])\n",
      "Labels (format: [(label, count)]: [(tensor(0.), tensor(14241)), (tensor(1.), tensor(1350))]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/lvl2_train.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f).sort_values(['subject_id', 'hadm_id', 'icustay_id'])\n",
    "\n",
    "with open(\"data/Ys_train.pkl\", \"rb\") as f:\n",
    "    labels = pickle.load(f).sort_values(['subject_id', 'hadm_id', 'icustay_id'])\n",
    "    \n",
    "data = data.reset_index(drop=True).droplevel(level=\"LEVEL2\", axis=1)\n",
    "features = data[\"mean\"].to_numpy()\n",
    "masks = data[\"mask\"].to_numpy()\n",
    "\n",
    "features = np.split(features, [48 * i for i in range(1, len(features) // 48)])\n",
    "masks = np.split(masks, [48 * i for i in range(1, len(masks) // 48)])\n",
    "\n",
    "features = torch.tensor(features, dtype=torch.float).to(device)\n",
    "masks = torch.tensor(masks, dtype=torch.float).to(device)\n",
    "labels = torch.squeeze(torch.tensor(labels.to_numpy(), dtype=torch.float).to(device))\n",
    "\n",
    "print_data_info(features, masks, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb0252-35ed-440a-a512-2f3d84042959",
   "metadata": {},
   "source": [
    "## Create Training, Validation and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a5e98b4-65c7-4f7a-a2f5-86292822b211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset\n",
      "Shape of features: torch.Size([12627, 48, 104])\n",
      "Shape of masks: torch.Size([12627, 48, 104])\n",
      "Shape of labels: torch.Size([12627])\n",
      "Labels (format: [(label, count)]: [(tensor(0.), tensor(11531)), (tensor(1.), tensor(1096))]\n",
      "\n",
      "Test dataset\n",
      "Shape of features: torch.Size([1560, 48, 104])\n",
      "Shape of masks: torch.Size([1560, 48, 104])\n",
      "Shape of labels: torch.Size([1560])\n",
      "Labels (format: [(label, count)]: [(tensor(0.), tensor(1431)), (tensor(1.), tensor(129))]\n",
      "\n",
      "Validation dataset\n",
      "Shape of features: torch.Size([1404, 48, 104])\n",
      "Shape of masks: torch.Size([1404, 48, 104])\n",
      "Shape of labels: torch.Size([1404])\n",
      "Labels (format: [(label, count)]: [(tensor(0.), tensor(1279)), (tensor(1.), tensor(125))]\n"
     ]
    }
   ],
   "source": [
    "train_features, test_features, train_masks, test_masks, train_labels, test_labels = train_test_split(\n",
    "    features, masks, labels, test_size=0.1, random_state=42)\n",
    "train_features, val_features, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    train_features, train_masks, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training dataset\")\n",
    "print_data_info(train_features, train_masks, train_labels)\n",
    "print(\"\\nTest dataset\")\n",
    "print_data_info(test_features, test_masks, test_labels)\n",
    "print(\"\\nValidation dataset\")\n",
    "print_data_info(val_features, val_masks, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642e3bc-652b-4a4b-bf5e-eba848b8811e",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "\n",
    "Perform oversampling to ensure a balanced distribution of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "088d06bd-af90-4032-980d-06b4c4eb6f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iandre3/.conda/envs/mimic-extract-2/lib/python3.9/site-packages/sklearn/utils/validation.py:746: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n",
      "/home/iandre3/.conda/envs/mimic-extract-2/lib/python3.9/site-packages/sklearn/utils/validation.py:746: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features: torch.Size([23062, 48, 104])\n",
      "Shape of masks: torch.Size([23062, 48, 104])\n",
      "Shape of labels: torch.Size([23062])\n",
      "Labels (format: [(label, count)]: [(tensor(0.), tensor(11531)), (tensor(1.), tensor(11531))]\n"
     ]
    }
   ],
   "source": [
    "def oversample_data(features, masks, labels):\n",
    "    X = list(zip(features, masks))\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    X_resampled, labels_resampled = ros.fit_resample(X, labels)\n",
    "    features_resampled, masks_resampled = list(zip(*X_resampled))\n",
    "\n",
    "    features_resampled = np.array([np.array(xi) for xi in features_resampled])\n",
    "    masks_resampled = np.array([np.array(xi) for xi in masks_resampled])\n",
    "\n",
    "    features = torch.tensor(features_resampled, dtype=torch.float).to(device)\n",
    "    masks = torch.tensor(masks_resampled, dtype=torch.float).to(device)\n",
    "    labels = torch.tensor(labels_resampled, dtype=torch.float).to(device)\n",
    "\n",
    "    return features, masks, labels\n",
    "\n",
    "train_features, train_masks, train_labels = oversample_data(train_features, train_masks, train_labels)\n",
    "print_data_info(train_features, train_masks, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d747a-a518-4f92-8dc9-f6063accd022",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363adfb-a6a1-4a16-a257-3ede3b6f3992",
   "metadata": {},
   "source": [
    "### Train Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b01308c8-e8cd-4789-8e96-ce67834fbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_easy_tasks(model, x, mask, y, N, K, epoch):\n",
    "    criterion = nn.BCELoss(reduction='none')\n",
    "    model.train(False)\n",
    "    y_hat = model(x, mask)\n",
    "\n",
    "    # Pick tasks in the batch for which the loss is less than 1 / N.\n",
    "    loss = torch.add((1 / GAMMA) * criterion(y_hat, y), -1 / N) if USE_MODIFIED_LOSS_FUNCTION else torch.add(criterion(y_hat, y), -1 / N)\n",
    "    easy_indices = list((loss < 0).nonzero()) if epoch >= K else list(loss.nonzero())\n",
    "\n",
    "    easy_timeseries = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_masks = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_labels = torch.empty((len(easy_indices),))\n",
    "\n",
    "    for i, index in enumerate(easy_indices):\n",
    "        index = index.item()\n",
    "        easy_timeseries[i] = x[index]\n",
    "        easy_masks[i] = mask[index]\n",
    "        easy_labels[i] = y[index]\n",
    "\n",
    "    print(f\"samples picked: {len(easy_indices)}, positive samples picked: {sum(easy_labels)}, K: {K}, epoch: {epoch}\")\n",
    "\n",
    "    model.train(True)\n",
    "    return torch.Tensor(easy_timeseries).to(device), torch.Tensor(easy_masks).to(device), torch.Tensor(easy_labels).to(device)\n",
    "\n",
    "def create_easy_train_dataloader(model, N, K, epoch):\n",
    "    easy_timeseries, easy_masks, easy_labels = pick_easy_tasks(model, train_features, train_masks, train_labels, N, K, epoch)\n",
    "    easy_train_dataset = TensorDataset(easy_timeseries, easy_masks, easy_labels)\n",
    "    if len(easy_train_dataset) == 0:\n",
    "        return None, len(train_features)\n",
    "    return DataLoader(easy_train_dataset, batch_size=32, shuffle=True), len(train_features)\n",
    "\n",
    "train_dataset = TensorDataset(train_features, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d68739-ab0f-458b-8a53-5a029540735f",
   "metadata": {},
   "source": [
    "### Test and Validation Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0292051f-8b86-4fb8-9b6c-fcb9726101ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(test_features, test_masks, test_labels)\n",
    "val_dataset = TensorDataset(val_features, val_masks, val_labels)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1a9c4-56e0-4da3-872d-08f85083d512",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06d52678-4129-434a-824a-b7c10d5837f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F1 score constant 1: 0.1635055591890124\n",
      "Baseline F1 score uniform: 0.16047904191616766\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "constant_classifier = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "uniform_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "inputs, labels = None, None\n",
    "for inp, mask, lab in val_dataloader:\n",
    "    inputs, labels = inp, lab\n",
    "    labels = labels.clone().cpu().detach()\n",
    "\n",
    "constant_classifier.fit([0] * len(labels), labels)\n",
    "uniform_classifier.fit([0] * len(labels), labels)\n",
    "y_constant = constant_classifier.predict([0] * len(labels))\n",
    "y_uniform = uniform_classifier.predict([0] * len(labels))\n",
    "\n",
    "print(f\"Baseline F1 score constant 1: {f1_score(labels, y_constant)}\")\n",
    "print(f\"Baseline F1 score uniform: {f1_score(labels, y_uniform)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64cb3f-67e7-42e3-8a34-2a5fca799c70",
   "metadata": {},
   "source": [
    "## PACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833cdbf-e076-45ad-a602-204d36d7f594",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "73f76a59-a975-4e9b-b5f7-fc1cfd85ee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyModel(\n",
      "  (rnn): GRU(104, 32, batch_first=True)\n",
      "  (fc1): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=104, hidden_size=32, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        try:\n",
    "            x = x * mask\n",
    "        except:\n",
    "            print(x, mask)\n",
    "        _, h_n = self.rnn(x)\n",
    "        fc1_out = self.fc1(torch.squeeze(h_n))\n",
    "        r = fc1_out\n",
    "        \n",
    "        # Multiply prediction value before sigmoid activation by gamma.\n",
    "        # This is part of the micro framework.\n",
    "        if USE_MODIFIED_LOSS_FUNCTION:\n",
    "            r = torch.mul(fc1_out, GAMMA)\n",
    "            \n",
    "        res = torch.sigmoid(r)\n",
    "        return torch.squeeze(res)\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "tinymodel.to(device)\n",
    "\n",
    "print(tinymodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162135f6-9d68-47a9-a6d7-6600ed8b0565",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8df6ef20-818a-480b-8b02-e4fe68516dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(tinymodel.parameters(), LR)\n",
    "\n",
    "def loss_function(y_hat, y, model):\n",
    "    # Add a dimension when there is only one item in the batch.     \n",
    "    if len(y_hat.size()) == 0:\n",
    "        y_hat = torch.unsqueeze(y_hat, dim=0)\n",
    "    \n",
    "    l1 = OMEGA * sum(p.abs().sum() for p in model.parameters())\n",
    "    cr = criterion(y_hat, y)\n",
    "    \n",
    "    return l1 + cr\n",
    "\n",
    "def modified_loss_function(y_hat, y, model):\n",
    "    # Add a dimension when there is only one item in the batch.     \n",
    "    if len(y_hat.size()) == 0:\n",
    "        y_hat = torch.unsqueeze(y_hat, dim=0)\n",
    "\n",
    "    l1 = OMEGA * sum(p.abs().sum() for p in model.parameters())\n",
    "    cr = (1 / GAMMA) * criterion(y_hat, y)\n",
    "    \n",
    "    # Calculate the loss. The regularization is just an l1 regularization with a coefficient.\n",
    "    # The criterion is the binary cross-entropy multiplied by 1 / GAMMA, a part of the micro framework.\n",
    "    return l1 + cr\n",
    "\n",
    "    # The paper also says to subtract batch_size / N, but this value is different for different batches depending\n",
    "    # on the number of easy tasks in the batch, making it hard to compare loss across batches. Crucially, this \n",
    "    # is confusing when you look at the loss on training data (non-full batches) vs loss on testing data (full-size batches).\n",
    "    # This makes me think that actually the paper meant that you first pick easy tasks across the entire testing set,\n",
    "    # and then split it into batches rather than split into batches first and pick easy tasks after.\n",
    "    # batch_size = y_hat.size()[0]    \n",
    "    # neg = batch_size / N\n",
    "    # return l1 + cr - neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4c85b-d054-4f6d-adaf-bb72e6593e26",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a819e10-4fc0-4c74-8193-4d37277affcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "N is set to 16\n",
      "Epoch: [1, training batch   100] running training loss: 1.340\n",
      "Epoch: [1, training batch   200] running training loss: 0.717\n",
      "Epoch: [1, training batch   300] running training loss: 0.592\n",
      "Epoch: [1, training batch   400] running training loss: 0.586\n",
      "Epoch: [1, training batch   500] running training loss: 0.556\n",
      "Epoch: [1, training batch   600] running training loss: 0.550\n",
      "Epoch: [1, training batch   700] running training loss: 0.537\n",
      "In epoch 1, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.588\n",
      "\n",
      "\n",
      "N is set to 16\n",
      "Epoch: [2, training batch   100] running training loss: 0.531\n",
      "Epoch: [2, training batch   200] running training loss: 0.526\n",
      "Epoch: [2, training batch   300] running training loss: 0.545\n",
      "Epoch: [2, training batch   400] running training loss: 0.534\n",
      "Epoch: [2, training batch   500] running training loss: 0.535\n",
      "Epoch: [2, training batch   600] running training loss: 0.509\n",
      "Epoch: [2, training batch   700] running training loss: 0.524\n",
      "In epoch 2, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.400\n",
      "\n",
      "\n",
      "N is set to 12.307692307692307\n",
      "Epoch: [3, training batch   100] running training loss: 0.516\n",
      "Epoch: [3, training batch   200] running training loss: 0.514\n",
      "Epoch: [3, training batch   300] running training loss: 0.519\n",
      "Epoch: [3, training batch   400] running training loss: 0.503\n",
      "Epoch: [3, training batch   500] running training loss: 0.505\n",
      "Epoch: [3, training batch   600] running training loss: 0.506\n",
      "Epoch: [3, training batch   700] running training loss: 0.495\n",
      "In epoch 3, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.819\n",
      "\n",
      "\n",
      "N is set to 9.467455621301774\n",
      "Epoch: [4, training batch   100] running training loss: 0.496\n",
      "Epoch: [4, training batch   200] running training loss: 0.513\n",
      "Epoch: [4, training batch   300] running training loss: 0.493\n",
      "Epoch: [4, training batch   400] running training loss: 0.483\n",
      "Epoch: [4, training batch   500] running training loss: 0.499\n",
      "Epoch: [4, training batch   600] running training loss: 0.490\n",
      "Epoch: [4, training batch   700] running training loss: 0.483\n",
      "In epoch 4, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.430\n",
      "\n",
      "\n",
      "N is set to 7.282658170232134\n",
      "Epoch: [5, training batch   100] running training loss: 0.485\n",
      "Epoch: [5, training batch   200] running training loss: 0.484\n",
      "Epoch: [7, training batch   200] running training loss: 0.487\n",
      "Epoch: [7, training batch   300] running training loss: 0.457\n",
      "Epoch: [7, training batch   400] running training loss: 0.476\n",
      "Epoch: [7, training batch   500] running training loss: 0.466\n",
      "Epoch: [7, training batch   600] running training loss: 0.463\n",
      "Epoch: [7, training batch   700] running training loss: 0.493\n",
      "In epoch 7, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.863\n",
      "\n",
      "\n",
      "N is set to 3.314819376528053\n",
      "Epoch: [8, training batch   100] running training loss: 0.468\n",
      "Epoch: [8, training batch   200] running training loss: 0.474\n",
      "Epoch: [8, training batch   300] running training loss: 0.468\n",
      "Epoch: [8, training batch   400] running training loss: 0.470\n",
      "Epoch: [8, training batch   500] running training loss: 0.464\n",
      "Epoch: [8, training batch   600] running training loss: 0.473\n",
      "Epoch: [8, training batch   700] running training loss: 0.453\n",
      "In epoch 8, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.480\n",
      "\n",
      "\n",
      "N is set to 2.5498610588677333\n",
      "Epoch: [9, training batch   100] running training loss: 0.475\n",
      "Epoch: [9, training batch   200] running training loss: 0.467\n",
      "Epoch: [9, training batch   300] running training loss: 0.462\n",
      "Epoch: [9, training batch   400] running training loss: 0.464\n",
      "Epoch: [9, training batch   500] running training loss: 0.447\n",
      "Epoch: [9, training batch   600] running training loss: 0.471\n",
      "Epoch: [9, training batch   700] running training loss: 0.474\n",
      "In epoch 9, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.645\n",
      "\n",
      "\n",
      "N is set to 1.96143158374441\n",
      "Epoch: [10, training batch   100] running training loss: 0.456\n",
      "Epoch: [10, training batch   200] running training loss: 0.460\n",
      "Epoch: [10, training batch   300] running training loss: 0.442\n",
      "Epoch: [10, training batch   400] running training loss: 0.475\n",
      "Epoch: [10, training batch   500] running training loss: 0.452\n",
      "Epoch: [10, training batch   600] running training loss: 0.475\n",
      "Epoch: [10, training batch   700] running training loss: 0.463\n",
      "In epoch 10, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.602\n",
      "\n",
      "\n",
      "N is set to 1.5087935259572385\n",
      "Epoch: [11, training batch   100] running training loss: 0.477\n",
      "Epoch: [11, training batch   200] running training loss: 0.473\n",
      "Epoch: [11, training batch   300] running training loss: 0.453\n",
      "Epoch: [11, training batch   400] running training loss: 0.449\n",
      "Epoch: [11, training batch   500] running training loss: 0.459\n",
      "Epoch: [11, training batch   600] running training loss: 0.455\n",
      "Epoch: [11, training batch   700] running training loss: 0.453\n",
      "In epoch 11, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.437\n",
      "\n",
      "\n",
      "N is set to 1.1606104045824912\n",
      "Epoch: [12, training batch   100] running training loss: 0.455\n",
      "Epoch: [12, training batch   200] running training loss: 0.448\n",
      "Epoch: [12, training batch   300] running training loss: 0.455\n",
      "Epoch: [12, training batch   400] running training loss: 0.465\n",
      "Epoch: [12, training batch   500] running training loss: 0.467\n",
      "Epoch: [12, training batch   600] running training loss: 0.464\n",
      "Epoch: [12, training batch   700] running training loss: 0.444\n",
      "In epoch 12, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.403\n",
      "\n",
      "\n",
      "N is set to 0.892777234294224\n",
      "Epoch: [13, training batch   100] running training loss: 0.451\n",
      "Epoch: [13, training batch   200] running training loss: 0.440\n",
      "Epoch: [13, training batch   300] running training loss: 0.447\n",
      "Epoch: [13, training batch   400] running training loss: 0.471\n",
      "Epoch: [13, training batch   500] running training loss: 0.457\n",
      "Epoch: [13, training batch   600] running training loss: 0.455\n",
      "Epoch: [13, training batch   700] running training loss: 0.464\n",
      "In epoch 13, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.359\n",
      "\n",
      "\n",
      "N is set to 0.6867517186878646\n",
      "Epoch: [14, training batch   100] running training loss: 0.451\n",
      "Epoch: [14, training batch   200] running training loss: 0.441\n",
      "Epoch: [14, training batch   300] running training loss: 0.453\n",
      "Epoch: [14, training batch   400] running training loss: 0.446\n",
      "Epoch: [14, training batch   500] running training loss: 0.459\n",
      "Epoch: [14, training batch   600] running training loss: 0.448\n",
      "Epoch: [14, training batch   700] running training loss: 0.465\n",
      "In epoch 14, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.415\n",
      "\n",
      "\n",
      "N is set to 0.5282705528368189\n",
      "Epoch: [15, training batch   100] running training loss: 0.444\n",
      "Epoch: [15, training batch   200] running training loss: 0.430\n",
      "Epoch: [15, training batch   300] running training loss: 0.471\n",
      "Epoch: [15, training batch   400] running training loss: 0.439\n",
      "Epoch: [15, training batch   500] running training loss: 0.448\n",
      "Epoch: [15, training batch   600] running training loss: 0.453\n",
      "Epoch: [15, training batch   700] running training loss: 0.455\n",
      "In epoch 15, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.451\n",
      "\n",
      "\n",
      "N is set to 0.4063619637206299\n",
      "Epoch: [16, training batch   100] running training loss: 0.457\n",
      "Epoch: [16, training batch   200] running training loss: 0.449\n",
      "Epoch: [16, training batch   300] running training loss: 0.440\n",
      "Epoch: [16, training batch   400] running training loss: 0.449\n",
      "Epoch: [16, training batch   500] running training loss: 0.450\n",
      "Epoch: [16, training batch   600] running training loss: 0.445\n",
      "Epoch: [16, training batch   700] running training loss: 0.454\n",
      "In epoch 16, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.507\n",
      "\n",
      "\n",
      "N is set to 0.312586125938946\n",
      "Epoch: [17, training batch   100] running training loss: 0.444\n",
      "Epoch: [17, training batch   200] running training loss: 0.450\n",
      "Epoch: [17, training batch   300] running training loss: 0.441\n",
      "Epoch: [17, training batch   400] running training loss: 0.445\n",
      "Epoch: [17, training batch   500] running training loss: 0.438\n",
      "Epoch: [17, training batch   600] running training loss: 0.443\n",
      "Epoch: [17, training batch   700] running training loss: 0.459\n",
      "In epoch 17, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.542\n",
      "\n",
      "\n",
      "N is set to 0.24045086610688154\n",
      "Epoch: [18, training batch   100] running training loss: 0.431\n",
      "Epoch: [18, training batch   200] running training loss: 0.456\n",
      "Epoch: [18, training batch   300] running training loss: 0.442\n",
      "Epoch: [18, training batch   400] running training loss: 0.459\n",
      "Epoch: [18, training batch   500] running training loss: 0.460\n",
      "Epoch: [18, training batch   600] running training loss: 0.434\n",
      "Epoch: [18, training batch   700] running training loss: 0.432\n",
      "In epoch 18, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.350\n",
      "\n",
      "\n",
      "N is set to 0.1849622046976012\n",
      "Epoch: [19, training batch   100] running training loss: 0.449\n",
      "Epoch: [19, training batch   200] running training loss: 0.444\n",
      "Epoch: [19, training batch   300] running training loss: 0.442\n",
      "Epoch: [19, training batch   400] running training loss: 0.443\n",
      "Epoch: [19, training batch   500] running training loss: 0.437\n",
      "Epoch: [19, training batch   600] running training loss: 0.443\n",
      "Epoch: [19, training batch   700] running training loss: 0.456\n",
      "In epoch 19, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.543\n",
      "\n",
      "\n",
      "N is set to 0.14227861899815475\n",
      "Epoch: [20, training batch   100] running training loss: 0.429\n",
      "Epoch: [20, training batch   200] running training loss: 0.448\n",
      "Epoch: [20, training batch   300] running training loss: 0.450\n",
      "Epoch: [20, training batch   400] running training loss: 0.449\n",
      "Epoch: [20, training batch   500] running training loss: 0.436\n",
      "Epoch: [20, training batch   600] running training loss: 0.447\n",
      "Epoch: [20, training batch   700] running training loss: 0.444\n",
      "In epoch 20, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.375\n",
      "\n",
      "\n",
      "N is set to 0.10944509153704211\n",
      "Epoch: [21, training batch   100] running training loss: 0.441\n",
      "Epoch: [21, training batch   200] running training loss: 0.454\n",
      "Epoch: [21, training batch   300] running training loss: 0.428\n",
      "Epoch: [21, training batch   400] running training loss: 0.452\n",
      "Epoch: [21, training batch   500] running training loss: 0.438\n",
      "Epoch: [21, training batch   600] running training loss: 0.436\n",
      "Epoch: [21, training batch   700] running training loss: 0.442\n",
      "In epoch 21, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.684\n",
      "\n",
      "\n",
      "N is set to 0.08418853195157085\n",
      "Epoch: [22, training batch   100] running training loss: 0.434\n",
      "Epoch: [22, training batch   200] running training loss: 0.421\n",
      "Epoch: [22, training batch   300] running training loss: 0.445\n",
      "Epoch: [22, training batch   400] running training loss: 0.449\n",
      "Epoch: [22, training batch   500] running training loss: 0.439\n",
      "Epoch: [22, training batch   600] running training loss: 0.444\n",
      "Epoch: [22, training batch   700] running training loss: 0.450\n",
      "In epoch 22, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.356\n",
      "\n",
      "\n",
      "N is set to 0.06476040919351603\n",
      "Epoch: [23, training batch   100] running training loss: 0.442\n",
      "Epoch: [23, training batch   200] running training loss: 0.437\n",
      "Epoch: [23, training batch   300] running training loss: 0.454\n",
      "Epoch: [23, training batch   400] running training loss: 0.442\n",
      "Epoch: [23, training batch   500] running training loss: 0.449\n",
      "Epoch: [23, training batch   600] running training loss: 0.425\n",
      "Epoch: [23, training batch   700] running training loss: 0.448\n",
      "In epoch 23, number of examples trained on is 23062 out of 23062\n",
      "Validation loss: 0.438\n",
      "\n",
      "\n",
      "N is set to 0.04981569937962771\n",
      "Epoch: [24, training batch   100] running training loss: 0.454\n",
      "Epoch: [24, training batch   200] running training loss: 0.436\n",
      "Epoch: [24, training batch   300] running training loss: 0.435\n",
      "Epoch: [24, training batch   400] running training loss: 0.443\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m output \u001b[38;5;241m=\u001b[39m tinymodel(features, masks)\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m modified_loss_function(output, labels, tinymodel) \u001b[38;5;28;01mif\u001b[39;00m USE_MODIFIED_LOSS_FUNCTION \u001b[38;5;28;01melse\u001b[39;00m loss_function(output, labels, tinymodel)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/mimic-extract-2/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mimic-extract-2/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "previous_loss = 1e10\n",
    "previous_state_dict = None\n",
    "optimistic_iters = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"\\n\\nN is set to {N}\")\n",
    "    running_loss = 0.0\n",
    "    number_examples_used = 0\n",
    "    dataloader, max_num_examples = create_easy_train_dataloader(tinymodel, N, K, epoch) if USE_SELF_PACED_LEARNING else (train_dataloader, len(train_dataset))\n",
    "    \n",
    "    if dataloader is not None:\n",
    "        for i, (features, masks, labels) in enumerate(dataloader):\n",
    "            tinymodel.train(True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Train on easy sequences.\n",
    "            output = tinymodel(features, masks)\n",
    "            loss = modified_loss_function(output, labels, tinymodel) if USE_MODIFIED_LOSS_FUNCTION else loss_function(output, labels, tinymodel)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            number_examples_used += len(features)\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch: [{epoch + 1}, training batch {i + 1:5d}] running training loss: {(running_loss / 100):.3f}')\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    tinymodel.train(False)\n",
    "    print(f\"In epoch {epoch + 1}, number of examples trained on is {number_examples_used} out of {max_num_examples}\")\n",
    "\n",
    "    running_val_loss = 0.0\n",
    "    for i, (val_inputs, val_masks, val_labels) in enumerate(val_dataloader):\n",
    "        val_outputs = tinymodel(val_inputs, val_masks)\n",
    "        val_loss = modified_loss_function(output, labels, tinymodel) if USE_MODIFIED_LOSS_FUNCTION else loss_function(output, labels, tinymodel)\n",
    "        running_val_loss += val_loss.item()\n",
    "    \n",
    "    normalized_running_val_loss = running_val_loss / len(val_dataloader)\n",
    "    print(f\"Validation loss: {normalized_running_val_loss:.3f}\")\n",
    "    \n",
    "    if previous_loss - normalized_running_val_loss < TOLERANCE and epoch > 25 and previous_state_dict is not None:\n",
    "        if optimistic_iters < PATIENCE:\n",
    "            print(\"Being patient\")\n",
    "            optimistic_iters += 1\n",
    "        else:\n",
    "            print(f\"Early stopping, as {previous_loss - normalized_running_val_loss} < {TOLERANCE}\")\n",
    "            tinymodel.load_state_dict(previous_state_dict)\n",
    "            break\n",
    "    else:\n",
    "        previous_loss = normalized_running_val_loss\n",
    "        previous_state_dict = tinymodel.state_dict()\n",
    "        optimistic_iters = 0\n",
    "    \n",
    "    # Once we have passed K warm-up epochs, decrease N by a factor of lambd\n",
    "    # and as such increase 1 / N, so increase the loss threshold that defines\n",
    "    # what tasks are considrered easy.\n",
    "    if epoch >= K:\n",
    "        N = N / LAMBDA\n",
    "\n",
    "print(\"\\n\\nFinished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b72798-671b-4794-aeaf-789940adf2cd",
   "metadata": {},
   "source": [
    "### Save Trained Model to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18207808-1174-467b-aa04-f93a76168ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled_features = []\n",
    "if USE_MODIFIED_LOSS_FUNCTION:\n",
    "    enabled_features.append(\"modified_loss\")\n",
    "if USE_SELF_PACED_LEARNING:\n",
    "    enabled_features.append(\"spl\")\n",
    "\n",
    "features_string = '_and_'.join(enabled_features)\n",
    "\n",
    "model_name = f\"model_with_{features_string}.pt\" if len(features_string) > 0 else \"base_model.pt\"\n",
    "\n",
    "torch.save(tinymodel.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554a18c-b725-4dc4-874f-adf77c04ea6c",
   "metadata": {},
   "source": [
    "## PACE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "70f8f3fd-834f-4f0c-8f40-3134b29499d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.40772532188841204\n",
      "ROC AUC: 0.7836607998959907\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the metric-coverage plot: 0.9523113756362374\n"
     ]
    }
   ],
   "source": [
    "tinymodel.train(False)\n",
    "\n",
    "out = None\n",
    "lab = None\n",
    "for inputs, masks, labels in test_dataloader:\n",
    "    outputs = tinymodel(inputs, masks)\n",
    "    out = outputs\n",
    "    outputs = outputs > 0.5\n",
    "    lab = labels\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    print(f\"F1 score: {f1_score(labels, outputs)}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(labels, outputs)}\")\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "coverages = []\n",
    "calculated_metrics = []\n",
    "    \n",
    "AUC = 0\n",
    "confidences = 0.5 + abs(out - 0.5)\n",
    "ordered_indices = torch.argsort(confidences, descending=True)\n",
    "ordered_outputs = torch.index_select(out, 0, ordered_indices)\n",
    "ordered_labels = torch.index_select(lab, 0, ordered_indices)\n",
    "\n",
    "for confidence_threshold in np.arange(0.99, 0.01, -0.01):\n",
    "    above_confidence_threshold_indices = torch.nonzero(ordered_outputs[ordered_outputs > confidence_threshold]).squeeze()\n",
    "    above_confidence_outputs = torch.index_select(ordered_outputs, 0, above_confidence_threshold_indices)\n",
    "    above_confidence_labels = torch.index_select(ordered_labels, 0, above_confidence_threshold_indices)\n",
    "    \n",
    "    if torch.sum(above_confidence_labels) == 0 or torch.sum(above_confidence_outputs) == 0:\n",
    "        continue\n",
    "    if torch.sum(above_confidence_labels) == len(above_confidence_labels) or torch.sum(above_confidence_outputs) == len(above_confidence_outputs):\n",
    "        continue\n",
    "\n",
    "    metric = roc_auc_score(above_confidence_labels.cpu().detach().numpy(), above_confidence_outputs.cpu().detach().numpy())\n",
    "    calculated_metrics.append(metric)\n",
    "    coverage = len(above_confidence_outputs) / len(ordered_outputs)\n",
    "    coverages.append(coverage)\n",
    "    \n",
    "    AUC += metric * 0.01\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "# ax.set_linewidths([3])\n",
    "ax.plot(coverages, calculated_metrics, linewidth=3)\n",
    "ax.set_title('Metric-Coverage')\n",
    "ax.set_xlabel('Coverage')\n",
    "ax.set_ylabel('AUC')\n",
    "plt.show()\n",
    "print(f\"Area under the metric-coverage plot: {AUC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac4354-277d-43a8-b34c-3279625d362e",
   "metadata": {},
   "source": [
    "## Loading the state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51d883-0368-4527-bb1a-fd40d0a48ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load('model.pt')\n",
    "new_model = TinyModel()\n",
    "new_model.load_state_dict(model_state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mimic-extract-2]",
   "language": "python",
   "name": "conda-env-.conda-mimic-extract-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
