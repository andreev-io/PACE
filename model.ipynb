{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bfb9fe-a383-473b-9d49-6f9561ab932e",
   "metadata": {},
   "source": [
    "# PACE Model Replication Pipeline\n",
    "\n",
    "**Required packages:**\n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `pytorch`\n",
    "- `imbalanced-learn`\n",
    "- `scikit-learn`\n",
    "- `matplotlib`\n",
    "\n",
    "**Optional packages:**\n",
    "- `ipython`\n",
    "- `ipykernel`\n",
    "\n",
    "**Required python version:** 3.9.x\n",
    "We cannot use a newer version of python until pytorch adds support for it: https://github.com/pytorch/pytorch/issues/66424\n",
    "\n",
    "**Installation commands:**\n",
    "\n",
    "- `/path/to/conda create --name=dl4h-39 python=3.9.12`\n",
    "- `conda activate dl4h-39`\n",
    "- `conda install pandas pytables scipy numpy ipython ipykernel pytorch scikit-learn matplotlib`\n",
    "- `conda install -c conda-forge imbalanced-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb27d3c-0696-46c9-a4b5-6dbcb92a50df",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279f1fb-953e-4b4e-8553-2d55baebc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.5\n",
    "N = 16\n",
    "LAMBDA = 1.3\n",
    "LR = 0.001\n",
    "OMEGA = 0.001\n",
    "K = 1\n",
    "TOLERANCE = 0.01\n",
    "PATIENCE = 2\n",
    "\n",
    "USE_MODIFIED_LOSS_FUNCTION = True\n",
    "USE_SELF_PACED_LEARNING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004fd1a-a7be-4d86-8f2d-e891dcb035f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79433514-990a-42ba-8197-3836049dbc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30443d-dedd-44c1-ae88-f97e32e5b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c60dd9-709f-4fa4-a956-e6f66902aef6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e192f873-41d5-4c2c-8cc4-672880a7e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_info(features, masks, labels):\n",
    "    print(f\"Shape of features: {features.size()}\")\n",
    "    print(f\"Shape of masks: {masks.size()}\")\n",
    "    print(f\"Shape of labels: {labels.size()}\")\n",
    "    \n",
    "    print(f\"Labels (format: [(label, count)]: {list(zip(*torch.unique(labels, return_counts = True)))}\")\n",
    "\n",
    "    assert len(features) == len(labels)\n",
    "    assert len(masks) == len(labels)\n",
    "    assert features.size() == masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d9d4c-33ac-46f3-ae23-9dddc2e4c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train_features.pkl\", \"rb\") as f:\n",
    "    train_features = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/train_masks.pkl\", \"rb\") as f:\n",
    "    train_masks = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/train_labels.pkl\", \"rb\") as f:\n",
    "    train_labels = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/test_features.pkl\", \"rb\") as f:\n",
    "    test_features = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/test_masks.pkl\", \"rb\") as f:\n",
    "    test_masks = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/test_labels.pkl\", \"rb\") as f:\n",
    "    test_labels = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/val_features.pkl\", \"rb\") as f:\n",
    "    val_features = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/val_masks.pkl\", \"rb\") as f:\n",
    "    val_masks = pickle.load(f).to(device)\n",
    "\n",
    "with open(\"data/val_labels.pkl\", \"rb\") as f:\n",
    "    val_labels = pickle.load(f).to(device)\n",
    "    \n",
    "print(\"Training dataset\")\n",
    "print_data_info(train_features, train_masks, train_labels)\n",
    "\n",
    "print(\"\\nTest dataset\")\n",
    "print_data_info(test_features, test_masks, test_labels)\n",
    "\n",
    "print(\"\\nValidation dataset\")\n",
    "print_data_info(val_features, val_masks, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d747a-a518-4f92-8dc9-f6063accd022",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363adfb-a6a1-4a16-a257-3ede3b6f3992",
   "metadata": {},
   "source": [
    "### Train Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01308c8-e8cd-4789-8e96-ce67834fbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_easy_tasks(model, x, mask, y, N, K, epoch):\n",
    "    criterion = nn.BCELoss(reduction='none')\n",
    "    model.train(False)\n",
    "    y_hat = model(x, mask)\n",
    "\n",
    "    # Pick tasks in the batch for which the loss is less than 1 / N.\n",
    "    loss = torch.add((1 / GAMMA) * criterion(y_hat, y), -1 / N) if USE_MODIFIED_LOSS_FUNCTION else torch.add(criterion(y_hat, y), -1 / N)\n",
    "    easy_indices = list((loss < 0).nonzero()) if epoch >= K else list(loss.nonzero())\n",
    "\n",
    "    easy_timeseries = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_masks = torch.empty((len(easy_indices), x.size()[1], x.size()[2]))\n",
    "    easy_labels = torch.empty((len(easy_indices),))\n",
    "\n",
    "    for i, index in enumerate(easy_indices):\n",
    "        index = index.item()\n",
    "        easy_timeseries[i] = x[index]\n",
    "        easy_masks[i] = mask[index]\n",
    "        easy_labels[i] = y[index]\n",
    "\n",
    "    print(f\"samples picked: {len(easy_indices)}, positive samples picked: {sum(easy_labels)}, K: {K}, epoch: {epoch}\")\n",
    "\n",
    "    model.train(True)\n",
    "    return torch.Tensor(easy_timeseries).to(device), torch.Tensor(easy_masks).to(device), torch.Tensor(easy_labels).to(device)\n",
    "\n",
    "def create_easy_train_dataloader(model, N, K, epoch):\n",
    "    easy_timeseries, easy_masks, easy_labels = pick_easy_tasks(model, train_features, train_masks, train_labels, N, K, epoch)\n",
    "    easy_train_dataset = TensorDataset(easy_timeseries, easy_masks, easy_labels)\n",
    "    if len(easy_train_dataset) == 0:\n",
    "        return None, len(train_features)\n",
    "    return DataLoader(easy_train_dataset, batch_size=32, shuffle=True), len(train_features)\n",
    "\n",
    "train_dataset = TensorDataset(train_features, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d68739-ab0f-458b-8a53-5a029540735f",
   "metadata": {},
   "source": [
    "### Test and Validation Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292051f-8b86-4fb8-9b6c-fcb9726101ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(test_features, test_masks, test_labels)\n",
    "val_dataset = TensorDataset(val_features, val_masks, val_labels)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1a9c4-56e0-4da3-872d-08f85083d512",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d52678-4129-434a-824a-b7c10d5837f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "constant_classifier = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "uniform_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "inputs, labels = None, None\n",
    "for inp, mask, lab in val_dataloader:\n",
    "    inputs, labels = inp, lab\n",
    "    labels = labels.clone().cpu().detach()\n",
    "\n",
    "constant_classifier.fit([0] * len(labels), labels)\n",
    "uniform_classifier.fit([0] * len(labels), labels)\n",
    "y_constant = constant_classifier.predict([0] * len(labels))\n",
    "y_uniform = uniform_classifier.predict([0] * len(labels))\n",
    "\n",
    "print(f\"Baseline F1 score constant 1: {f1_score(labels, y_constant)}\")\n",
    "print(f\"Baseline F1 score uniform: {f1_score(labels, y_uniform)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64cb3f-67e7-42e3-8a34-2a5fca799c70",
   "metadata": {},
   "source": [
    "## PACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833cdbf-e076-45ad-a602-204d36d7f594",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f76a59-a975-4e9b-b5f7-fc1cfd85ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=104, hidden_size=32, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        try:\n",
    "            x = x * mask\n",
    "        except:\n",
    "            print(x, mask)\n",
    "        _, h_n = self.rnn(x)\n",
    "        fc1_out = self.fc1(torch.squeeze(h_n))\n",
    "        r = fc1_out\n",
    "        \n",
    "        # Multiply prediction value before sigmoid activation by gamma.\n",
    "        # This is part of the micro framework.\n",
    "        if USE_MODIFIED_LOSS_FUNCTION:\n",
    "            r = torch.mul(fc1_out, GAMMA)\n",
    "            \n",
    "        res = torch.sigmoid(r)\n",
    "        return torch.squeeze(res)\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "tinymodel.to(device)\n",
    "\n",
    "print(tinymodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162135f6-9d68-47a9-a6d7-6600ed8b0565",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6ef20-818a-480b-8b02-e4fe68516dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(tinymodel.parameters(), LR)\n",
    "\n",
    "def loss_function(y_hat, y, model):\n",
    "    # Add a dimension when there is only one item in the batch.     \n",
    "    if len(y_hat.size()) == 0:\n",
    "        y_hat = torch.unsqueeze(y_hat, dim=0)\n",
    "    \n",
    "    l1 = OMEGA * sum(p.abs().sum() for p in model.parameters())\n",
    "    cr = criterion(y_hat, y)\n",
    "    \n",
    "    return l1 + cr\n",
    "\n",
    "def modified_loss_function(y_hat, y, model):\n",
    "    # Add a dimension when there is only one item in the batch.     \n",
    "    if len(y_hat.size()) == 0:\n",
    "        y_hat = torch.unsqueeze(y_hat, dim=0)\n",
    "\n",
    "    l1 = OMEGA * sum(p.abs().sum() for p in model.parameters())\n",
    "    cr = (1 / GAMMA) * criterion(y_hat, y)\n",
    "    \n",
    "    # Calculate the loss. The regularization is just an l1 regularization with a coefficient.\n",
    "    # The criterion is the binary cross-entropy multiplied by 1 / GAMMA, a part of the micro framework.\n",
    "    return l1 + cr\n",
    "\n",
    "    # The paper also says to subtract batch_size / N, but this value is different for different batches depending\n",
    "    # on the number of easy tasks in the batch, making it hard to compare loss across batches. Crucially, this \n",
    "    # is confusing when you look at the loss on training data (non-full batches) vs loss on testing data (full-size batches).\n",
    "    # This makes me think that actually the paper meant that you first pick easy tasks across the entire testing set,\n",
    "    # and then split it into batches rather than split into batches first and pick easy tasks after.\n",
    "    # batch_size = y_hat.size()[0]    \n",
    "    # neg = batch_size / N\n",
    "    # return l1 + cr - neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4c85b-d054-4f6d-adaf-bb72e6593e26",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a819e10-4fc0-4c74-8193-4d37277affcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_loss = 1e10\n",
    "previous_state_dict = None\n",
    "optimistic_iters = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"\\n\\nN is set to {N}\")\n",
    "    running_loss = 0.0\n",
    "    number_examples_used = 0\n",
    "    dataloader, max_num_examples = create_easy_train_dataloader(tinymodel, N, K, epoch) if USE_SELF_PACED_LEARNING else (train_dataloader, len(train_dataset))\n",
    "    \n",
    "    if dataloader is not None:\n",
    "        for i, (features, masks, labels) in enumerate(dataloader):\n",
    "            tinymodel.train(True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Train on easy sequences.\n",
    "            output = tinymodel(features, masks)\n",
    "            loss = modified_loss_function(output, labels, tinymodel) if USE_MODIFIED_LOSS_FUNCTION else loss_function(output, labels, tinymodel)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            number_examples_used += len(features)\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch: [{epoch + 1}, training batch {i + 1:5d}] running training loss: {(running_loss / 100):.3f}')\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    tinymodel.train(False)\n",
    "    print(f\"In epoch {epoch + 1}, number of examples trained on is {number_examples_used} out of {max_num_examples}\")\n",
    "\n",
    "    running_val_loss = 0.0\n",
    "    for i, (val_inputs, val_masks, val_labels) in enumerate(val_dataloader):\n",
    "        val_outputs = tinymodel(val_inputs, val_masks)\n",
    "        val_loss = modified_loss_function(output, labels, tinymodel) if USE_MODIFIED_LOSS_FUNCTION else loss_function(output, labels, tinymodel)\n",
    "        running_val_loss += val_loss.item()\n",
    "    \n",
    "    normalized_running_val_loss = running_val_loss / len(val_dataloader)\n",
    "    print(f\"Validation loss: {normalized_running_val_loss:.3f}\")\n",
    "    \n",
    "    if previous_loss - normalized_running_val_loss < TOLERANCE and epoch > 25 and previous_state_dict is not None:\n",
    "        if optimistic_iters < PATIENCE:\n",
    "            print(\"Being patient\")\n",
    "            optimistic_iters += 1\n",
    "        else:\n",
    "            print(f\"Early stopping, as {previous_loss - normalized_running_val_loss} < {TOLERANCE}\")\n",
    "            tinymodel.load_state_dict(previous_state_dict)\n",
    "            break\n",
    "    else:\n",
    "        previous_loss = normalized_running_val_loss\n",
    "        previous_state_dict = tinymodel.state_dict()\n",
    "        optimistic_iters = 0\n",
    "    \n",
    "    # Once we have passed K warm-up epochs, decrease N by a factor of lambd\n",
    "    # and as such increase 1 / N, so increase the loss threshold that defines\n",
    "    # what tasks are considrered easy.\n",
    "    if epoch >= K:\n",
    "        N = N / LAMBDA\n",
    "\n",
    "print(\"\\n\\nFinished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b72798-671b-4794-aeaf-789940adf2cd",
   "metadata": {},
   "source": [
    "### Save Trained Model to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18207808-1174-467b-aa04-f93a76168ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled_features = []\n",
    "if USE_MODIFIED_LOSS_FUNCTION:\n",
    "    enabled_features.append(\"modified_loss\")\n",
    "if USE_SELF_PACED_LEARNING:\n",
    "    enabled_features.append(\"spl\")\n",
    "\n",
    "features_string = '_and_'.join(enabled_features)\n",
    "\n",
    "model_name = f\"model_with_{features_string}.pt\" if len(features_string) > 0 else \"base_model.pt\"\n",
    "\n",
    "torch.save(tinymodel.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554a18c-b725-4dc4-874f-adf77c04ea6c",
   "metadata": {},
   "source": [
    "## PACE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8f3fd-834f-4f0c-8f40-3134b29499d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinymodel.train(False)\n",
    "\n",
    "out = None\n",
    "lab = None\n",
    "for inputs, masks, labels in test_dataloader:\n",
    "    outputs = tinymodel(inputs, masks)\n",
    "    out = outputs\n",
    "    outputs = outputs > 0.5\n",
    "    lab = labels\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    print(f\"F1 score: {f1_score(labels, outputs)}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(labels, outputs)}\")\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "coverages = []\n",
    "calculated_metrics = []\n",
    "    \n",
    "AUC = 0\n",
    "confidences = 0.5 + abs(out - 0.5)\n",
    "ordered_indices = torch.argsort(confidences, descending=True)\n",
    "ordered_outputs = torch.index_select(out, 0, ordered_indices)\n",
    "ordered_labels = torch.index_select(lab, 0, ordered_indices)\n",
    "\n",
    "for confidence_threshold in np.arange(0.99, 0.01, -0.01):\n",
    "    above_confidence_threshold_indices = torch.nonzero(ordered_outputs[ordered_outputs > confidence_threshold]).squeeze()\n",
    "    above_confidence_outputs = torch.index_select(ordered_outputs, 0, above_confidence_threshold_indices)\n",
    "    above_confidence_labels = torch.index_select(ordered_labels, 0, above_confidence_threshold_indices)\n",
    "    \n",
    "    if torch.sum(above_confidence_labels) == 0 or torch.sum(above_confidence_outputs) == 0:\n",
    "        continue\n",
    "    if torch.sum(above_confidence_labels) == len(above_confidence_labels) or torch.sum(above_confidence_outputs) == len(above_confidence_outputs):\n",
    "        continue\n",
    "\n",
    "    metric = roc_auc_score(above_confidence_labels.cpu().detach().numpy(), above_confidence_outputs.cpu().detach().numpy())\n",
    "    calculated_metrics.append(metric)\n",
    "    coverage = len(above_confidence_outputs) / len(ordered_outputs)\n",
    "    coverages.append(coverage)\n",
    "    \n",
    "    AUC += metric * 0.01\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "# ax.set_linewidths([3])\n",
    "ax.plot(coverages, calculated_metrics, linewidth=3)\n",
    "ax.set_title('Metric-Coverage')\n",
    "ax.set_xlabel('Coverage')\n",
    "ax.set_ylabel('AUC')\n",
    "plt.show()\n",
    "print(f\"Area under the metric-coverage plot: {AUC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac4354-277d-43a8-b34c-3279625d362e",
   "metadata": {},
   "source": [
    "## Loading the state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51d883-0368-4527-bb1a-fd40d0a48ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load('model.pt')\n",
    "new_model = TinyModel()\n",
    "new_model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02f08b-2f03-4846-99da-df342b7c84bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.5.1]",
   "language": "python",
   "name": "conda-env-opence-v1.5.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
