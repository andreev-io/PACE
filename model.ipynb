{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7635ebcf-72d0-4401-a8f1-30c209378284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63172a0b-e8d6-4334-a41d-7d49155b9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icu  measurements  preprocessed\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80cac9b9-7b0a-41b1-8216-b0e4e852d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/preprocessed\"        \n",
    "\n",
    "class MortalityDataset(Dataset):    \n",
    "    def __init__(self):\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        \n",
    "        sequences = []\n",
    "        directory = os.fsencode(f\"{PATH}\")\n",
    "        fs = os.listdir(directory)\n",
    "        for idx, file in enumerate(fs):\n",
    "            filename = os.fsdecode(file)\n",
    "            if not filename.endswith(\".csv\"): \n",
    "                continue\n",
    "                \n",
    "            df = pd.read_csv(f\"{PATH}/{filename}\")\n",
    "            y_true = df[\"y_true\"][0]\n",
    "            del df[\"y_true\"]\n",
    "            \n",
    "            data = torch.from_numpy(df.values)\n",
    "            self.sequences.append(data)\n",
    "            self.labels.append(y_true)\n",
    "            \n",
    "            if idx % 500 == 0:\n",
    "                print(f\"loaded {idx+1} out of {len(fs)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c185c05-b215-48be-9c62-c845016f4fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1 out of 20030\n",
      "loaded 501 out of 20030\n",
      "loaded 1001 out of 20030\n",
      "loaded 1501 out of 20030\n",
      "loaded 2001 out of 20030\n",
      "loaded 2501 out of 20030\n",
      "loaded 3001 out of 20030\n",
      "loaded 3501 out of 20030\n",
      "loaded 4001 out of 20030\n",
      "loaded 4501 out of 20030\n",
      "loaded 5001 out of 20030\n",
      "loaded 5501 out of 20030\n",
      "loaded 6001 out of 20030\n",
      "loaded 6501 out of 20030\n",
      "loaded 7001 out of 20030\n",
      "loaded 7501 out of 20030\n",
      "loaded 8001 out of 20030\n",
      "loaded 8501 out of 20030\n",
      "loaded 9001 out of 20030\n",
      "loaded 9501 out of 20030\n",
      "loaded 10001 out of 20030\n",
      "loaded 10501 out of 20030\n",
      "loaded 11001 out of 20030\n",
      "loaded 11501 out of 20030\n",
      "loaded 12001 out of 20030\n",
      "loaded 12501 out of 20030\n",
      "loaded 13001 out of 20030\n",
      "loaded 13501 out of 20030\n",
      "loaded 14001 out of 20030\n",
      "loaded 14501 out of 20030\n",
      "loaded 15001 out of 20030\n",
      "loaded 15501 out of 20030\n",
      "loaded 16001 out of 20030\n",
      "loaded 16501 out of 20030\n",
      "loaded 17001 out of 20030\n",
      "loaded 17501 out of 20030\n",
      "loaded 18001 out of 20030\n",
      "loaded 18501 out of 20030\n",
      "loaded 19001 out of 20030\n",
      "loaded 19501 out of 20030\n",
      "loaded 20001 out of 20030\n",
      "(tensor([[46,  0,  0,  ...,  0, 65,  0],\n",
      "        [46,  0,  0,  ...,  0, 65,  0],\n",
      "        [46,  0,  0,  ...,  0, 65,  0],\n",
      "        ...,\n",
      "        [46,  0,  0,  ..., 37, 65,  0],\n",
      "        [46,  0,  0,  ...,  0, 65,  0],\n",
      "        [46,  0,  0,  ...,  0, 65,  0]]), 0)\n",
      "(tensor([[59,  0,  0,  ...,  0, 89,  0],\n",
      "        [59,  0,  0,  ...,  0, 89,  0],\n",
      "        [59,  0,  0,  ...,  0, 89,  0],\n",
      "        ...,\n",
      "        [59,  0,  0,  ...,  0, 89,  0],\n",
      "        [59,  0,  0,  ...,  0, 89,  0],\n",
      "        [59,  0,  0,  ...,  0, 89,  0]]), 0)\n"
     ]
    }
   ],
   "source": [
    "m = MortalityDataset()\n",
    "print(m[0])\n",
    "print(m[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c0c971-ab20-4857-8684-86ea2e952c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bcfb8b2-423c-4615-8df5-1dd6859d083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "def create_batch(inp):\n",
    "    features = [i[0].type(torch.float).to(device) for i in inp]\n",
    "    labels = [i[1] for i in inp]\n",
    "    packed_seq = pack_sequence(features, enforce_sorted=False)\n",
    "    labels = torch.tensor(labels, dtype=torch.float).to(device)\n",
    "    return packed_seq, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13a4048b-a691-4ea6-8bfd-8507961d33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "train_data, test_data, validation_data = random_split(m, [int(0.8 * len(m)), int(0.1 * len(m)), int(0.1 * len(m))], torch.Generator())\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=create_batch, num_workers=0)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True, collate_fn=create_batch, num_workers=0)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=len(validation_data), shuffle=True, collate_fn=create_batch, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aacdfe67-c25f-4bf9-9f0f-6b6137173db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F1 score constant 1: 0.19630796938316075\n",
      "Baseline F1 score uniform: 0.2095857026807474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "constant_classifier = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "uniform_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "inputs, labels = None, None\n",
    "for inp, lab in validation_dataloader:\n",
    "    inputs, labels = inp, lab\n",
    "    labels = labels.clone().cpu().detach()\n",
    "\n",
    "constant_classifier.fit([0] * len(labels), labels)\n",
    "uniform_classifier.fit([0] * len(labels), labels)\n",
    "y_constant = constant_classifier.predict([0] * len(labels))\n",
    "y_uniform = uniform_classifier.predict([0] * len(labels))\n",
    "\n",
    "print(f\"Baseline F1 score constant 1: {f1_score(labels, y_constant)}\")\n",
    "print(f\"Baseline F1 score uniform: {f1_score(labels, y_uniform)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b48d7e17-b9bb-41f0-b4f4-73e8dda0a38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (rnn): GRU(149, 32)\n",
      "  (fc1): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        \n",
    "#       Bidirectional (experimenting)\n",
    "#         self.rnn = nn.GRU(input_size=149, hidden_size=32, num_layers=1, bidirectional=True)\n",
    "#         self.fc1 = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=149, hidden_size=32, num_layers=1, bidirectional=False)\n",
    "        self.fc1 = nn.Linear(in_features=32, out_features=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        _, h_n = self.rnn(x)\n",
    "#         print(h_n.size())\n",
    "#         h_n = torch.cat((h_n[0], h_n[1]), dim=1)\n",
    "#         print(h_n.size())\n",
    "        fc1_out = self.fc1(torch.squeeze(h_n))\n",
    "        res = torch.sigmoid(fc1_out)\n",
    "        \n",
    "        return torch.squeeze(res)\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "tinymodel.to(device)\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae80b486-c86a-41e7-947c-87afbf624ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(tinymodel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f1ea2c4-ff8c-4aec-a221-2a32c9f9b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1, training sample    50] running loss: 0.440\n",
      "Epoch: [1, training sample   100] running loss: 0.389\n",
      "Epoch: [1, training sample   150] running loss: 0.371\n",
      "Epoch: [1, training sample   200] running loss: 0.377\n",
      "Epoch: [1, training sample   250] running loss: 0.356\n",
      "Epoch: [1, training sample   300] running loss: 0.389\n",
      "Epoch: [1, training sample   350] running loss: 0.392\n",
      "Epoch: [1, training sample   400] running loss: 0.385\n",
      "Epoch: [1, training sample   450] running loss: 0.330\n",
      "Epoch: [1, training sample   500] running loss: 0.358\n",
      "Test loss: 0.3742932081222534\n",
      "Epoch: [2, training sample    50] running loss: 0.332\n",
      "Epoch: [2, training sample   100] running loss: 0.366\n",
      "Epoch: [2, training sample   150] running loss: 0.350\n",
      "Epoch: [2, training sample   200] running loss: 0.341\n",
      "Epoch: [2, training sample   250] running loss: 0.369\n",
      "Epoch: [2, training sample   300] running loss: 0.350\n",
      "Epoch: [2, training sample   350] running loss: 0.355\n",
      "Epoch: [2, training sample   400] running loss: 0.373\n",
      "Epoch: [2, training sample   450] running loss: 0.328\n",
      "Epoch: [2, training sample   500] running loss: 0.360\n",
      "Test loss: 0.3598814010620117\n",
      "Epoch: [3, training sample    50] running loss: 0.357\n",
      "Epoch: [3, training sample   100] running loss: 0.312\n",
      "Epoch: [3, training sample   150] running loss: 0.337\n",
      "Epoch: [3, training sample   200] running loss: 0.349\n",
      "Epoch: [3, training sample   250] running loss: 0.347\n",
      "Epoch: [3, training sample   300] running loss: 0.316\n",
      "Epoch: [3, training sample   350] running loss: 0.349\n",
      "Epoch: [3, training sample   400] running loss: 0.340\n",
      "Epoch: [3, training sample   450] running loss: 0.336\n",
      "Epoch: [3, training sample   500] running loss: 0.339\n",
      "Test loss: 0.34110331535339355\n",
      "Epoch: [4, training sample    50] running loss: 0.329\n",
      "Epoch: [4, training sample   100] running loss: 0.329\n",
      "Epoch: [4, training sample   150] running loss: 0.346\n",
      "Epoch: [4, training sample   200] running loss: 0.321\n",
      "Epoch: [4, training sample   250] running loss: 0.289\n",
      "Epoch: [4, training sample   300] running loss: 0.335\n",
      "Epoch: [4, training sample   350] running loss: 0.341\n",
      "Epoch: [4, training sample   400] running loss: 0.315\n",
      "Epoch: [4, training sample   450] running loss: 0.302\n",
      "Epoch: [4, training sample   500] running loss: 0.316\n",
      "Test loss: 0.3331142067909241\n",
      "Epoch: [5, training sample    50] running loss: 0.305\n",
      "Epoch: [5, training sample   100] running loss: 0.330\n",
      "Epoch: [5, training sample   150] running loss: 0.290\n",
      "Epoch: [5, training sample   200] running loss: 0.337\n",
      "Epoch: [5, training sample   250] running loss: 0.302\n",
      "Epoch: [5, training sample   300] running loss: 0.327\n",
      "Epoch: [5, training sample   350] running loss: 0.324\n",
      "Epoch: [5, training sample   400] running loss: 0.307\n",
      "Epoch: [5, training sample   450] running loss: 0.283\n",
      "Epoch: [5, training sample   500] running loss: 0.289\n",
      "Test loss: 0.32366496324539185\n",
      "Epoch: [6, training sample    50] running loss: 0.298\n",
      "Epoch: [6, training sample   100] running loss: 0.293\n",
      "Epoch: [6, training sample   150] running loss: 0.301\n",
      "Epoch: [6, training sample   200] running loss: 0.303\n",
      "Epoch: [6, training sample   250] running loss: 0.311\n",
      "Epoch: [6, training sample   300] running loss: 0.291\n",
      "Epoch: [6, training sample   350] running loss: 0.299\n",
      "Epoch: [6, training sample   400] running loss: 0.297\n",
      "Epoch: [6, training sample   450] running loss: 0.289\n",
      "Epoch: [6, training sample   500] running loss: 0.300\n",
      "Test loss: 0.3198893964290619\n",
      "Epoch: [7, training sample    50] running loss: 0.300\n",
      "Epoch: [7, training sample   100] running loss: 0.253\n",
      "Epoch: [7, training sample   150] running loss: 0.296\n",
      "Epoch: [7, training sample   200] running loss: 0.303\n",
      "Epoch: [7, training sample   250] running loss: 0.265\n",
      "Epoch: [7, training sample   300] running loss: 0.291\n",
      "Epoch: [7, training sample   350] running loss: 0.319\n",
      "Epoch: [7, training sample   400] running loss: 0.281\n",
      "Epoch: [7, training sample   450] running loss: 0.288\n",
      "Epoch: [7, training sample   500] running loss: 0.285\n",
      "Test loss: 0.3068130314350128\n",
      "Epoch: [8, training sample    50] running loss: 0.294\n",
      "Epoch: [8, training sample   100] running loss: 0.280\n",
      "Epoch: [8, training sample   150] running loss: 0.245\n",
      "Epoch: [8, training sample   200] running loss: 0.261\n",
      "Epoch: [8, training sample   250] running loss: 0.299\n",
      "Epoch: [8, training sample   300] running loss: 0.326\n",
      "Epoch: [8, training sample   350] running loss: 0.282\n",
      "Epoch: [8, training sample   400] running loss: 0.270\n",
      "Epoch: [8, training sample   450] running loss: 0.302\n",
      "Epoch: [8, training sample   500] running loss: 0.285\n",
      "Test loss: 0.31765449047088623\n",
      "Epoch: [9, training sample    50] running loss: 0.268\n",
      "Epoch: [9, training sample   100] running loss: 0.280\n",
      "Epoch: [9, training sample   150] running loss: 0.273\n",
      "Epoch: [9, training sample   200] running loss: 0.295\n",
      "Epoch: [9, training sample   250] running loss: 0.268\n",
      "Epoch: [9, training sample   300] running loss: 0.279\n",
      "Epoch: [9, training sample   350] running loss: 0.275\n",
      "Epoch: [9, training sample   400] running loss: 0.269\n",
      "Epoch: [9, training sample   450] running loss: 0.273\n",
      "Epoch: [9, training sample   500] running loss: 0.282\n",
      "Test loss: 0.2959160804748535\n",
      "Epoch: [10, training sample    50] running loss: 0.270\n",
      "Epoch: [10, training sample   100] running loss: 0.283\n",
      "Epoch: [10, training sample   150] running loss: 0.281\n",
      "Epoch: [10, training sample   200] running loss: 0.273\n",
      "Epoch: [10, training sample   250] running loss: 0.285\n",
      "Epoch: [10, training sample   300] running loss: 0.266\n",
      "Epoch: [10, training sample   350] running loss: 0.270\n",
      "Epoch: [10, training sample   400] running loss: 0.272\n",
      "Epoch: [10, training sample   450] running loss: 0.275\n",
      "Epoch: [10, training sample   500] running loss: 0.273\n",
      "Test loss: 0.3017615079879761\n",
      "Epoch: [11, training sample    50] running loss: 0.275\n",
      "Epoch: [11, training sample   100] running loss: 0.272\n",
      "Epoch: [11, training sample   150] running loss: 0.281\n",
      "Epoch: [11, training sample   200] running loss: 0.252\n",
      "Epoch: [11, training sample   250] running loss: 0.283\n",
      "Epoch: [11, training sample   300] running loss: 0.261\n",
      "Epoch: [11, training sample   350] running loss: 0.270\n",
      "Epoch: [11, training sample   400] running loss: 0.270\n",
      "Epoch: [11, training sample   450] running loss: 0.263\n",
      "Epoch: [11, training sample   500] running loss: 0.274\n",
      "Test loss: 0.3069281280040741\n",
      "Epoch: [12, training sample    50] running loss: 0.270\n",
      "Epoch: [12, training sample   100] running loss: 0.253\n",
      "Epoch: [12, training sample   150] running loss: 0.268\n",
      "Epoch: [12, training sample   200] running loss: 0.255\n",
      "Epoch: [12, training sample   250] running loss: 0.275\n",
      "Epoch: [12, training sample   300] running loss: 0.250\n",
      "Epoch: [12, training sample   350] running loss: 0.261\n",
      "Epoch: [12, training sample   400] running loss: 0.273\n",
      "Epoch: [12, training sample   450] running loss: 0.251\n",
      "Epoch: [12, training sample   500] running loss: 0.281\n",
      "Test loss: 0.2912203073501587\n",
      "Epoch: [13, training sample    50] running loss: 0.256\n",
      "Epoch: [13, training sample   100] running loss: 0.274\n",
      "Epoch: [13, training sample   150] running loss: 0.271\n",
      "Epoch: [13, training sample   200] running loss: 0.268\n",
      "Epoch: [13, training sample   250] running loss: 0.242\n",
      "Epoch: [13, training sample   300] running loss: 0.239\n",
      "Epoch: [13, training sample   350] running loss: 0.260\n",
      "Epoch: [13, training sample   400] running loss: 0.254\n",
      "Epoch: [13, training sample   450] running loss: 0.261\n",
      "Epoch: [13, training sample   500] running loss: 0.281\n",
      "Test loss: 0.29404035210609436\n",
      "Epoch: [14, training sample    50] running loss: 0.259\n",
      "Epoch: [14, training sample   100] running loss: 0.260\n",
      "Epoch: [14, training sample   150] running loss: 0.232\n",
      "Epoch: [14, training sample   200] running loss: 0.251\n",
      "Epoch: [14, training sample   250] running loss: 0.244\n",
      "Epoch: [14, training sample   300] running loss: 0.263\n",
      "Epoch: [14, training sample   350] running loss: 0.267\n",
      "Epoch: [14, training sample   400] running loss: 0.251\n",
      "Epoch: [14, training sample   450] running loss: 0.270\n",
      "Epoch: [14, training sample   500] running loss: 0.280\n",
      "Test loss: 0.28235405683517456\n",
      "Epoch: [15, training sample    50] running loss: 0.263\n",
      "Epoch: [15, training sample   100] running loss: 0.259\n",
      "Epoch: [15, training sample   150] running loss: 0.254\n",
      "Epoch: [15, training sample   200] running loss: 0.238\n",
      "Epoch: [15, training sample   250] running loss: 0.272\n",
      "Epoch: [15, training sample   300] running loss: 0.258\n",
      "Epoch: [15, training sample   350] running loss: 0.289\n",
      "Epoch: [15, training sample   400] running loss: 0.218\n",
      "Epoch: [15, training sample   450] running loss: 0.269\n",
      "Epoch: [15, training sample   500] running loss: 0.246\n",
      "Test loss: 0.2843448221683502\n",
      "Epoch: [16, training sample    50] running loss: 0.236\n",
      "Epoch: [16, training sample   100] running loss: 0.253\n",
      "Epoch: [16, training sample   150] running loss: 0.253\n",
      "Epoch: [16, training sample   200] running loss: 0.250\n",
      "Epoch: [16, training sample   250] running loss: 0.264\n",
      "Epoch: [16, training sample   300] running loss: 0.250\n",
      "Epoch: [16, training sample   350] running loss: 0.273\n",
      "Epoch: [16, training sample   400] running loss: 0.241\n",
      "Epoch: [16, training sample   450] running loss: 0.264\n",
      "Epoch: [16, training sample   500] running loss: 0.281\n",
      "Test loss: 0.27740153670310974\n",
      "Epoch: [17, training sample    50] running loss: 0.262\n",
      "Epoch: [17, training sample   100] running loss: 0.240\n",
      "Epoch: [17, training sample   150] running loss: 0.264\n",
      "Epoch: [17, training sample   200] running loss: 0.242\n",
      "Epoch: [17, training sample   250] running loss: 0.269\n",
      "Epoch: [17, training sample   300] running loss: 0.251\n",
      "Epoch: [17, training sample   350] running loss: 0.232\n",
      "Epoch: [17, training sample   400] running loss: 0.262\n",
      "Epoch: [17, training sample   450] running loss: 0.242\n",
      "Epoch: [17, training sample   500] running loss: 0.255\n",
      "Test loss: 0.2808874845504761\n",
      "Epoch: [18, training sample    50] running loss: 0.246\n",
      "Epoch: [18, training sample   100] running loss: 0.241\n",
      "Epoch: [18, training sample   150] running loss: 0.269\n",
      "Epoch: [18, training sample   200] running loss: 0.249\n",
      "Epoch: [18, training sample   250] running loss: 0.236\n",
      "Epoch: [18, training sample   300] running loss: 0.234\n",
      "Epoch: [18, training sample   350] running loss: 0.263\n",
      "Epoch: [18, training sample   400] running loss: 0.258\n",
      "Epoch: [18, training sample   450] running loss: 0.258\n",
      "Epoch: [18, training sample   500] running loss: 0.253\n",
      "Test loss: 0.2860235571861267\n",
      "Epoch: [19, training sample    50] running loss: 0.251\n",
      "Epoch: [19, training sample   100] running loss: 0.231\n",
      "Epoch: [19, training sample   150] running loss: 0.234\n",
      "Epoch: [19, training sample   200] running loss: 0.243\n",
      "Epoch: [19, training sample   250] running loss: 0.252\n",
      "Epoch: [19, training sample   300] running loss: 0.229\n",
      "Epoch: [19, training sample   350] running loss: 0.245\n",
      "Epoch: [19, training sample   400] running loss: 0.261\n",
      "Epoch: [19, training sample   450] running loss: 0.264\n",
      "Epoch: [19, training sample   500] running loss: 0.265\n",
      "Test loss: 0.28549495339393616\n",
      "Epoch: [20, training sample    50] running loss: 0.251\n",
      "Epoch: [20, training sample   100] running loss: 0.242\n",
      "Epoch: [20, training sample   150] running loss: 0.246\n",
      "Epoch: [20, training sample   200] running loss: 0.261\n",
      "Epoch: [20, training sample   250] running loss: 0.232\n",
      "Epoch: [20, training sample   300] running loss: 0.247\n",
      "Epoch: [20, training sample   350] running loss: 0.229\n",
      "Epoch: [20, training sample   400] running loss: 0.236\n",
      "Epoch: [20, training sample   450] running loss: 0.246\n",
      "Epoch: [20, training sample   500] running loss: 0.247\n",
      "Test loss: 0.2819160223007202\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        tinymodel.train(True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = tinymodel(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'Epoch: [{epoch + 1}, training sample {i + 1:5d}] running loss: {running_loss / 50:.3f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    running_vloss = 0.0\n",
    "    tinymodel.train(False)\n",
    "    for i, (vinputs, vlabels) in enumerate(test_dataloader):\n",
    "        voutputs = tinymodel(vinputs)\n",
    "        vloss = criterion(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "    \n",
    "    print(f\"Test loss: {running_vloss / len(test_dataloader)}\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6592c29c-748d-4424-8949-8fb5c0df3a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.4883720930232558\n",
      "ROC AUC: 0.7108536992778763\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "tinymodel.train(False)\n",
    "for inputs, labels in validation_dataloader:\n",
    "    outputs = tinymodel(inputs)\n",
    "    outputs = outputs > 0.5\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    print(f\"F1 score: {f1_score(labels, outputs)}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(labels, outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e6675-004d-44e8-9554-4a838774891b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61cfcea-38cd-430b-b2f4-dcafd2d95176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.5.1]",
   "language": "python",
   "name": "conda-env-opence-v1.5.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
